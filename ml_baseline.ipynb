{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    balanced_accuracy_score, brier_score_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except Exception:\n",
    "    xgb_available = False\n",
    "\n",
    "RND = 1706\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def bootstrap_ci_classification(y_true, y_pred, y_proba, metric=\"f1_macro\", n_boot=2000, stratified=True):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    idx_pos = np.where(y_true == 1)[0]\n",
    "    idx_neg = np.where(y_true == 0)[0]\n",
    "    scores = []\n",
    "    for _ in range(n_boot):\n",
    "        if stratified:\n",
    "            samp_pos = rng.choice(idx_pos, size=len(idx_pos), replace=True)\n",
    "            samp_neg = rng.choice(idx_neg, size=len(idx_neg), replace=True)\n",
    "            idx = np.concatenate([samp_pos, samp_neg])\n",
    "        else:\n",
    "            idx = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
    "        yt, yp, pr = y_true[idx], y_pred[idx], y_proba[idx]\n",
    "        if metric == \"f1_macro\":\n",
    "            s = f1_score(yt, yp, average=\"macro\")\n",
    "        elif metric == \"auc\":\n",
    "            if len(np.unique(yt)) < 2:\n",
    "                continue\n",
    "            s = roc_auc_score(yt, pr)\n",
    "        else:\n",
    "            raise ValueError(\"metric must be 'f1_macro' or 'auc'\")\n",
    "        scores.append(s)\n",
    "    lo, hi = np.percentile(scores, [2.5, 97.5])\n",
    "    return float(np.mean(scores)), float(lo), float(hi)\n",
    "\n",
    "def icc2_1_with_ci(y_probs1, y_probs2, n_boot=2000):\n",
    "    if np.allclose(y_probs1, y_probs1[0]) or np.allclose(y_probs2, y_probs2[0]):\n",
    "        print(\"Warning: One of the probability vectors is constant. ICC(2,1) is undefined (NaN).\")\n",
    "        return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        import pingouin as pg\n",
    "        n = len(y_probs1)\n",
    "        df_long = pd.DataFrame({\n",
    "            \"subject_id\": np.repeat(np.arange(n), 2),\n",
    "            \"method\": np.tile([0, 1], n),\n",
    "            \"value\": np.concatenate([y_probs1, y_probs2])\n",
    "        })\n",
    "        icc_tbl = pg.intraclass_corr(data=df_long, targets=\"subject_id\", raters=\"method\", ratings=\"value\")\n",
    "        row = icc_tbl.loc[icc_tbl[\"Type\"] == \"ICC2\"]\n",
    "        icc = float(row[\"ICC\"].values[0])\n",
    "        ci = row[\"CI95%\"].values[0]\n",
    "        if isinstance(ci, str):\n",
    "            lo, hi = [float(x) for x in ci.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")]\n",
    "        else:\n",
    "            lo, hi = float(ci[0]), float(ci[1])\n",
    "        return icc, lo, hi\n",
    "    except Exception as e:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "def run_baseline_classification_pipeline(\n",
    "    dataset_path,\n",
    "    target_col=\"Depression_label\",\n",
    "    drop_cols=None,\n",
    "    train_value=\"train\",\n",
    "    test_value=\"test\"\n",
    "):\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [\"PTSD_severity\", \"PTSD_label\", \"split\", \"Participant\", \"semantic_perplexity\"]\n",
    "\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    split_lower = df[\"split\"].astype(str).str.lower().str.strip()\n",
    "    is_train = split_lower == train_value\n",
    "    is_test = split_lower == test_value\n",
    "\n",
    "    X_train = df.loc[is_train].drop(columns=drop_cols + [target_col], errors=\"ignore\")\n",
    "    y_train = df.loc[is_train, target_col].astype(int)\n",
    "    X_test = df.loc[is_test].drop(columns=drop_cols + [target_col], errors=\"ignore\")\n",
    "    y_test = df.loc[is_test, target_col].astype(int)\n",
    "\n",
    "    categorical_cols = [c for c in X_train.columns if X_train[c].dtype.name in [\"object\", \"category\"]]\n",
    "    numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"enc\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "    pre = ColumnTransformer([\n",
    "        (\"cat\", cat_pipe, categorical_cols),\n",
    "        (\"num\", num_pipe, numeric_cols),\n",
    "    ], remainder=\"drop\")\n",
    "\n",
    "    def metrics_from_preds(y_true, y_pred, y_proba=None, y_proba_other=None):\n",
    "        out = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"balanced_acc\": balanced_accuracy_score(y_true, y_pred),\n",
    "            \"roc_auc\": np.nan,\n",
    "            \"pr_auc\": np.nan,\n",
    "            \"brier\": np.nan,\n",
    "            \"f1_lo\": np.nan,\n",
    "            \"f1_hi\": np.nan,\n",
    "            \"auc_lo\": np.nan,\n",
    "            \"auc_hi\": np.nan,\n",
    "            \"icc2_1\": np.nan,\n",
    "            \"icc_lo\": np.nan,\n",
    "            \"icc_hi\": np.nan,\n",
    "        }\n",
    "        if y_proba is not None and len(np.unique(y_true)) == 2:\n",
    "            try: out[\"roc_auc\"] = roc_auc_score(y_true, y_proba)\n",
    "            except: pass\n",
    "            try: out[\"pr_auc\"] = average_precision_score(y_true, y_proba)\n",
    "            except: pass\n",
    "            try: out[\"brier\"] = brier_score_loss(y_true, y_proba)\n",
    "            except: pass\n",
    "            try:\n",
    "                f1_mean, f1_lo, f1_hi = bootstrap_ci_classification(y_true, y_pred, y_proba, metric=\"f1_macro\")\n",
    "                auc_mean, auc_lo, auc_hi = bootstrap_ci_classification(y_true, y_pred, y_proba, metric=\"auc\")\n",
    "                out[\"f1_lo\"], out[\"f1_hi\"] = f1_lo, f1_hi\n",
    "                out[\"auc_lo\"], out[\"auc_hi\"] = auc_lo, auc_hi\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        if y_proba is not None and y_proba_other is not None:\n",
    "            icc, lo, hi = icc2_1_with_ci(y_proba, y_proba_other)\n",
    "            out[\"icc2_1\"], out[\"icc_lo\"], out[\"icc_hi\"] = icc, lo, hi\n",
    "        return out\n",
    "    \n",
    "    def explain_model(model, model_name, X_test, y_test, feature_names):\n",
    "        clf = model.named_steps[\"clf\"]\n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            imp = clf.feature_importances_\n",
    "            imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": imp}) \\\n",
    "                .sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "            print(f\"\\n[{model_name}] Top-10 impurity importances:\")\n",
    "            print(imp_df.head(10))\n",
    "            top_n = min(15, imp_df.shape[0])\n",
    "            plt.figure(figsize=(8, max(5, 0.3 * top_n)))\n",
    "            plt.barh(imp_df.loc[:top_n - 1, \"feature\"][::-1], imp_df.loc[:top_n - 1, \"importance\"][::-1])\n",
    "            plt.title(f\"Impurity Importances (top-15) — {model_name}\")\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.ylabel(\"Feature\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        try:\n",
    "            try:\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                scoring = \"roc_auc\"\n",
    "            except Exception:\n",
    "                scoring = \"balanced_accuracy\"\n",
    "            perm = permutation_importance(model, X_test, y_test,\n",
    "                                          n_repeats=10, random_state=RND, scoring=scoring)\n",
    "            perm_df = pd.DataFrame({\n",
    "                \"feature\": feature_names,\n",
    "                \"importance_mean\": perm.importances_mean,\n",
    "                \"importance_std\": perm.importances_std\n",
    "            }).sort_values(\"importance_mean\", ascending=False).reset_index(drop=True)\n",
    "            print(f\"\\n[{model_name}] Top-10 permutation importances:\")\n",
    "            print(perm_df.head(10))\n",
    "            top_n = min(15, perm_df.shape[0])\n",
    "            plt.figure(figsize=(8, max(5, 0.3 * top_n)))\n",
    "            plt.barh(perm_df.loc[:top_n - 1, \"feature\"][::-1], perm_df.loc[:top_n - 1, \"importance_mean\"][::-1])\n",
    "            plt.title(f\"Permutation Importances (top-15) — {model_name}\")\n",
    "            plt.xlabel(\"Mean importance (test)\")\n",
    "            plt.ylabel(\"Feature\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[{model_name}] Permutation importance failed: {e}\")\n",
    "\n",
    "    def fit_eval(model_name, estimator, y_proba_other=None):\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"clf\", estimator)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        feature_names = []\n",
    "        try:\n",
    "            encoder = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"enc\"]\n",
    "            cat_names = encoder.get_feature_names_out(categorical_cols)\n",
    "            feature_names = list(cat_names) + numeric_cols\n",
    "        except Exception:\n",
    "            feature_names = categorical_cols + numeric_cols\n",
    "\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        try:\n",
    "            y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            try:\n",
    "                from scipy.special import expit\n",
    "                y_proba = expit(pipe.decision_function(X_test))\n",
    "            except Exception:\n",
    "                y_proba = None\n",
    "\n",
    "        uniq, counts = np.unique(y_proba, return_counts=True) if y_proba is not None else ([], [])\n",
    "        print(f\"[{model_name}] unique predicted probabilities: {dict(zip(uniq, counts))}\")\n",
    "\n",
    "        mets = metrics_from_preds(y_test, y_pred, y_proba, y_proba_other)\n",
    "        print(f\"\\n=== {model_name} ===\")\n",
    "        print(\"Metrics:\",\n",
    "              {k: round(v, 4) if isinstance(v, (int, float)) and not np.isnan(v) else v for k, v in mets.items()})\n",
    "        print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        explain_model(pipe, model_name, X_test, y_test, feature_names)\n",
    "        return {\"model\": model_name, **mets, \"y_proba\": y_proba}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(\n",
    "        random_state=RND,\n",
    "        class_weight=\"balanced\",\n",
    "        max_depth=10,\n",
    "        min_samples_leaf=4,\n",
    "    )\n",
    "    res_dt = fit_eval(\"DecisionTree\", dt)\n",
    "\n",
    "    # Random Forest (ICC vs DT)\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=RND,\n",
    "        max_depth=10,\n",
    "        min_samples_leaf=4,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=1\n",
    "    )\n",
    "    res_rf = fit_eval(\"RandomForest\", rf, y_proba_other=res_dt[\"y_proba\"])\n",
    "\n",
    "    # XGBoost (ICC vs RF)\n",
    "    if xgb_available:\n",
    "        pos = int((y_train == 1).sum())\n",
    "        neg = int((y_train == 0).sum())\n",
    "        spw = float(neg / max(pos, 1)) if pos > 0 else 1.0\n",
    "\n",
    "        xgb = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RND,\n",
    "            n_estimators=300,\n",
    "            max_depth=10,\n",
    "            # learning_rate=0.08,\n",
    "            # subsample=0.8,\n",
    "            # colsample_bytree=0.8,\n",
    "            # min_child_weight=4,\n",
    "            # reg_lambda=1.0,\n",
    "            scale_pos_weight=spw\n",
    "        )\n",
    "        res_xgb = fit_eval(\"XGBoost\", xgb, y_proba_other=res_rf[\"y_proba\"])\n",
    "        results.append(res_xgb)\n",
    "        # Дополнительно: DT vs XGB\n",
    "        res_dt_xgb = fit_eval(\"DecisionTree vs XGBoost\", dt, y_proba_other=res_xgb[\"y_proba\"])\n",
    "        results.append(res_dt_xgb)\n",
    "    else:\n",
    "        print(\"⚠️ XGBoost is not installed — skipping this model.\")\n",
    "\n",
    "    # Добавляем все модели в результирующую таблицу\n",
    "    results.insert(0, res_dt)\n",
    "    results.insert(1, res_rf)\n",
    "\n",
    "    res_df = pd.DataFrame(results)[[\n",
    "        \"model\", \"accuracy\", \"f1_macro\", \"f1_lo\", \"f1_hi\",\n",
    "        \"balanced_acc\", \"roc_auc\", \"auc_lo\", \"auc_hi\",\n",
    "        \"pr_auc\", \"brier\", \"icc2_1\", \"icc_lo\", \"icc_hi\"\n",
    "    ]]\n",
    "    print(\"\\n=== Summary metrics on TEST ===\")\n",
    "    print(res_df.round(4).to_string(index=False))\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = run_baseline_classification_pipeline(\"/Users/pelmeshek1706/Desktop/projects/airest_notebooks/data/full_dataset_eng_gemma.csv\", target_col=\"Depression_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb446b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = run_baseline_classification_pipeline(\"/Users/pelmeshek1706/Desktop/projects/airest_notebooks/data/full_dataset_eng.csv\", target_col=\"Depression_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = run_baseline_classification_pipeline(\"/Users/pelmeshek1706/Desktop/projects/airest_notebooks/data/full_dataset_ukr_gemma.csv\", target_col=\"Depression_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = run_baseline_classification_pipeline(\"/Users/pelmeshek1706/Desktop/projects/airest_notebooks/data/full_dataset_ukr_bert.csv\", target_col=\"Depression_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4279a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
