{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "TAG_DICT_T = {\n",
    "    \"ua\" : {\n",
    "    \"CCONJ\": \"Conjunction\",\n",
    "    \"PRON\": \"Pronoun\",\n",
    "    \"NOUN\": \"Noun\",\n",
    "    \"ADJ\": \"Adjective\",\n",
    "    \"PUNCT\": \"Punctuation\",\n",
    "    \"VERB\": \"Verb\",\n",
    "    \"AUX\": \"Auxiliary\",\n",
    "    \"ADV\": \"Adverb\",\n",
    "    \"ADP\": \"Adposition\",\n",
    "    \"SCONJ\": \"Subordinating Conjunction\",\n",
    "    \"NUM\": \"Numeral\",\n",
    "    \"PROPN\": \"Proper Noun\"\n",
    "},\n",
    "    \"uk\" : {\n",
    "    \"CCONJ\": \"Conjunction\",\n",
    "    \"PRON\": \"Pronoun\",\n",
    "    \"NOUN\": \"Noun\",\n",
    "    \"ADJ\": \"Adjective\",\n",
    "    \"PUNCT\": \"Punctuation\",\n",
    "    \"VERB\": \"Verb\",\n",
    "    \"AUX\": \"Auxiliary\",\n",
    "    \"ADV\": \"Adverb\",\n",
    "    \"ADP\": \"Adposition\",\n",
    "    \"SCONJ\": \"Subordinating Conjunction\",\n",
    "    \"NUM\": \"Numeral\",\n",
    "    \"PROPN\": \"Proper Noun\"\n",
    "},\n",
    "    'en' : {\n",
    "    \"PRP\": \"Pronoun\",\n",
    "    \"PRP$\": \"Pronoun\",\n",
    "    \"VB\": \"Verb\",\n",
    "    \"VBD\": \"Verb\",\n",
    "    \"VBG\": \"Verb\",\n",
    "    \"VBN\": \"Verb\",\n",
    "    \"VBP\": \"Verb\",\n",
    "    \"VBZ\": \"Verb\",\n",
    "    \"JJ\": \"Adjective\",\n",
    "    \"JJR\": \"Adjective\",\n",
    "    \"JJS\": \"Adjective\",\n",
    "    \"NN\": \"Noun\",\n",
    "    \"NNP\": \"Noun\",\n",
    "    \"NNS\": \"Noun\",\n",
    "    \"RB\": \"Adverb\",\n",
    "    \"RBR\": \"Adverb\",\n",
    "    \"RBS\": \"Adverb\",\n",
    "    \"DT\": \"Determiner\"}\n",
    "}\n",
    "\n",
    "FIRST_PERSON_PRONOUNS = [\"I\", \"me\", \"my\", \"mine\", \"myself\"]\n",
    "FIRST_PERSON_PRONOUNS_T = {'en' : {\"I\", \"me\", \"my\", \"mine\", \"myself\"},\n",
    "                           'ua' : {\"я\", \"мене\", \"мені\", \"мною\", \"мій\", \"моя\", \"мої\", \"моє\"},\n",
    "                           'uk' : {\"я\", \"мене\", \"мені\", \"мною\", \"мій\", \"моя\", \"мої\", \"моє\"},}\n",
    "PRESENT = [\"VBP\", \"VBZ\"]\n",
    "PAST = [\"VBD\", \"VBN\"]\n",
    "def get_tag_l(full_text, lang='en'):\n",
    "    nlp = spacy.load(\"uk_core_news_sm\") if (lang in ['uk', 'ua']) else spacy.load(\"en_core_web_sm\")\n",
    "    if type(full_text) == list:\n",
    "        full_text = \" \".join(full_text)\n",
    "    doc = nlp(full_text)\n",
    "    \n",
    "    # Get the original tags and map them using our dictionary if available\n",
    "    pos_tags = [(token.text, TAG_DICT_T[lang].get(token.tag_, token.tag_)) for token in doc]\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1006)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Загрузка необходимых ресурсов\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Пример текста\n",
    "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Токенизация текста\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Определение частей речи\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('еней', 'Noun'),\n",
       " ('був', 'Auxiliary'),\n",
       " ('парубок', 'Noun'),\n",
       " ('моторний', 'Adjective'),\n",
       " ('і', 'Conjunction'),\n",
       " ('хлопець', 'Noun'),\n",
       " ('хоч', 'PART'),\n",
       " ('куди', 'Adverb'),\n",
       " ('козак', 'Noun'),\n",
       " ('удався', 'Verb'),\n",
       " ('все', 'Adverb'),\n",
       " ('зле', 'Adverb'),\n",
       " ('проворний', 'Adjective'),\n",
       " ('закінченню', 'Noun'),\n",
       " ('завзятіший', 'Adjective'),\n",
       " ('всіх', 'DET'),\n",
       " ('бурлак', 'Noun'),\n",
       " ('греки', 'Noun'),\n",
       " ('як', 'Subordinating Conjunction'),\n",
       " ('спаливши', 'Verb'),\n",
       " ('троє', 'Numeral'),\n",
       " ('зробили', 'Verb'),\n",
       " ('знижку', 'Noun'),\n",
       " ('карикатурною', 'Noun'),\n",
       " ('він', 'Pronoun'),\n",
       " ('взявши', 'Verb'),\n",
       " ('торбу', 'Noun'),\n",
       " ('життя', 'Noun'),\n",
       " ('подав', 'Verb'),\n",
       " ('забравши', 'Verb'),\n",
       " ('деяких', 'DET'),\n",
       " ('троянців', 'Noun'),\n",
       " ('осмалених', 'Adjective'),\n",
       " ('як', 'Subordinating Conjunction'),\n",
       " ('гиря', 'Noun'),\n",
       " ('уряд', 'Noun'),\n",
       " ('квантів', 'Noun'),\n",
       " (\"п'ятами\", 'Noun'),\n",
       " ('строї', 'Noun'),\n",
       " ('накрила', 'Verb')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = ['еней', 'був', 'парубок', 'моторний', 'і', 'хлопець', 'хоч', 'куди', 'козак', 'удався', 'все', 'зле', 'проворний', 'закінченню', 'завзятіший', 'всіх', 'бурлак', 'греки', 'як', 'спаливши', 'троє', 'зробили', 'знижку', 'карикатурною', 'він', 'взявши', 'торбу', 'життя', 'подав', 'забравши', 'деяких', 'троянців', 'осмалених', 'як', 'гиря', 'уряд', 'квантів', \"п'ятами\", 'строї', 'накрила']\n",
    "word_list_en = ['once', 'a', 'young', 'agile', 'and', 'lad', 'though', 'wherever', 'a', 'cossack', 'succeeded', 'everything', 'wrong', 'nimble', 'ending', 'most', 'persistent', 'all', 'haidamak', 'like', 'having', 'burned', 'three', 'made', 'discount', 'caricature', 'he', 'taking', 'bag', 'life', 'gave', 'taking', 'some', 'trojans', 'singed', 'like', 'weight', 'government', 'quanta', 'heels', 'ranks', 'covered']\n",
    "lang = \"ua\" # \"en\"\n",
    "tag_list = get_tag_l(word_list, lang=lang)\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list_pos = [TAG_DICT_T[lang][tag[1]] if tag[1] in TAG_DICT_T[lang].keys() else \"Other\" for tag in tag_list] # change TAG_LIST\n",
    "tag_list_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_person = [word in FIRST_PERSON_PRONOUNS_T[lang] for word in word_list]\n",
    "first_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other',\n",
       " 'Other']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list_verb = [\"Present\" if tag[1] in PRESENT else \"Past\" if tag[1] in PAST else \"Other\" for tag in tag_list]\n",
    "tag_list_verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tag list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('еней', 'Noun', []),\n",
       " ('був', 'Auxiliary', ['Past']),\n",
       " ('парубок', 'Noun', []),\n",
       " ('моторний', 'Adjective', []),\n",
       " ('і', 'Conjunction', []),\n",
       " ('хлопець', 'Noun', []),\n",
       " ('хоч', 'PART', []),\n",
       " ('куди', 'Adverb', []),\n",
       " ('козак', 'Noun', []),\n",
       " ('удався', 'Verb', ['Past']),\n",
       " ('все', 'Adverb', []),\n",
       " ('зле', 'Adverb', []),\n",
       " ('проворний', 'Adjective', []),\n",
       " ('закінченню', 'Noun', []),\n",
       " ('завзятіший', 'Adjective', []),\n",
       " ('всіх', 'DET', []),\n",
       " ('бурлак', 'Noun', []),\n",
       " ('греки', 'Noun', []),\n",
       " ('як', 'Subordinating Conjunction', []),\n",
       " ('спаливши', 'Verb', ['Past']),\n",
       " ('троє', 'Numeral', []),\n",
       " ('зробили', 'Verb', ['Past']),\n",
       " ('знижку', 'Noun', []),\n",
       " ('карикатурною', 'Noun', []),\n",
       " ('він', 'Pronoun', []),\n",
       " ('взявши', 'Verb', ['Past']),\n",
       " ('торбу', 'Noun', []),\n",
       " ('життя', 'Noun', []),\n",
       " ('подав', 'Verb', ['Past']),\n",
       " ('забравши', 'Verb', ['Past']),\n",
       " ('деяких', 'DET', []),\n",
       " ('троянців', 'Noun', []),\n",
       " ('осмалених', 'Adjective', []),\n",
       " ('як', 'Subordinating Conjunction', []),\n",
       " ('гиря', 'Noun', []),\n",
       " ('уряд', 'Noun', []),\n",
       " ('квантів', 'Noun', []),\n",
       " (\"п'ятами\", 'Noun', []),\n",
       " ('строї', 'Noun', []),\n",
       " ('накрила', 'Verb', ['Past'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"uk_core_news_sm\") if lang in ['ua', 'uk'] else spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" \".join(word_list))\n",
    "\n",
    "# Получаем список: (слово, сопоставленный тег, список значений морф. признака \"Tense\")\n",
    "tag_list = [(token.text, TAG_DICT_T[lang].get(token.tag_, token.tag_), token.morph.get(\"Tense\"))\n",
    "            for token in doc]\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_l_test(full_text, lang='en'):\n",
    "    if lang in ['ua', 'uk']:\n",
    "        nlp = spacy.load(\"uk_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    if isinstance(full_text, list):\n",
    "        full_text = \" \".join(full_text)\n",
    "        \n",
    "    doc = nlp(full_text)\n",
    "    tags = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if lang in ['ua', 'uk']:\n",
    "            pos = TAG_DICT_T[lang].get(token.pos_, token.pos_)\n",
    "            if token.pos_ in {\"VERB\", \"AUX\"}:\n",
    "                tense_vals = token.morph.get(\"Tense\")\n",
    "                if tense_vals:\n",
    "                    if \"Past\" in tense_vals:\n",
    "                        verb_tense = \"Past\"\n",
    "                    elif \"Pres\" in tense_vals:\n",
    "                        verb_tense = \"Present\"\n",
    "                    else:\n",
    "                        verb_tense = \"Other\"\n",
    "                else:\n",
    "                    verb_tense = \"Other\"\n",
    "            else:\n",
    "                verb_tense = None \n",
    "        else:\n",
    "            pos = TAG_DICT_T[lang].get(token.tag_, token.tag_)\n",
    "            if token.tag_ in PRESENT:\n",
    "                verb_tense = \"Present\"\n",
    "            elif token.tag_ in PAST:\n",
    "                verb_tense = \"Past\"\n",
    "            else:\n",
    "                verb_tense = \"Other\"\n",
    "        tags.append((token.text, pos, verb_tense))\n",
    "        \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мій', 'DET', None),\n",
       " ('еней', 'Noun', None),\n",
       " ('являється', 'Verb', 'Present'),\n",
       " ('парубок', 'Noun', None),\n",
       " ('моторний', 'Adjective', None),\n",
       " ('і', 'Conjunction', None),\n",
       " ('хлопець', 'Noun', None),\n",
       " ('хоч', 'PART', None),\n",
       " ('куди', 'Adverb', None),\n",
       " ('козак', 'Noun', None),\n",
       " ('удався', 'Verb', 'Past'),\n",
       " ('все', 'Adverb', None),\n",
       " ('зле', 'Adverb', None),\n",
       " ('проворний', 'Adjective', None),\n",
       " ('закінченню', 'Noun', None),\n",
       " ('завзятіший', 'Adjective', None),\n",
       " ('всіх', 'DET', None),\n",
       " ('бурлак', 'Noun', None),\n",
       " ('греки', 'Noun', None),\n",
       " ('як', 'Subordinating Conjunction', None),\n",
       " ('спаливши', 'Verb', 'Past'),\n",
       " ('троє', 'Numeral', None),\n",
       " ('зробили', 'Verb', 'Past'),\n",
       " ('знижку', 'Noun', None),\n",
       " ('карикатурною', 'Noun', None),\n",
       " ('він', 'Pronoun', None),\n",
       " ('взявши', 'Verb', 'Past'),\n",
       " ('торбу', 'Noun', None),\n",
       " ('життя', 'Noun', None),\n",
       " ('подав', 'Verb', 'Past'),\n",
       " ('забравши', 'Verb', 'Past'),\n",
       " ('деяких', 'DET', None),\n",
       " ('троянців', 'Noun', None),\n",
       " ('осмалених', 'Adjective', None),\n",
       " ('як', 'Subordinating Conjunction', None),\n",
       " ('гиря', 'Noun', None),\n",
       " ('уряд', 'Noun', None),\n",
       " ('квантів', 'Noun', None),\n",
       " (\"п'ятами\", 'Noun', None),\n",
       " ('строї', 'Noun', None),\n",
       " ('накрила', 'Verb', 'Past')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = ['мій','еней', 'являється', 'парубок', 'моторний', 'і', 'хлопець', 'хоч', 'куди', 'козак', 'удався', 'все', 'зле', 'проворний', 'закінченню', 'завзятіший', 'всіх', 'бурлак', 'греки', 'як', 'спаливши', 'троє', 'зробили', 'знижку', 'карикатурною', 'він', 'взявши', 'торбу', 'життя', 'подав', 'забравши', 'деяких', 'троянців', 'осмалених', 'як', 'гиря', 'уряд', 'квантів', \"п'ятами\", 'строї', 'накрила']\n",
    "word_list_en = ['once', 'a', 'my', 'young', 'agile', 'and', 'lad', 'though', 'wherever', 'a', 'cossack', 'succeeded', 'everything', 'wrong', 'nimble', 'ending', 'most', 'persistent', 'all', 'haidamak', 'like', 'having', 'burned', 'three', 'made', 'discount', 'caricature', 'he', 'taking', 'bag', 'life', 'gave', 'taking', 'some', 'trojans', 'singed', 'like', 'weight', 'government', 'quanta', 'heels', 'ranks', 'covered']\n",
    "lang = \"ua\" # \"en\"\n",
    "tag_list = get_tag_l_test(word_list, lang='ua')\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'Noun',\n",
       " 'Verb',\n",
       " 'Noun',\n",
       " 'Adjective',\n",
       " 'Conjunction',\n",
       " 'Noun',\n",
       " 'PART',\n",
       " 'Adverb',\n",
       " 'Noun',\n",
       " 'Verb',\n",
       " 'Adverb',\n",
       " 'Adverb',\n",
       " 'Adjective',\n",
       " 'Noun',\n",
       " 'Adjective',\n",
       " 'DET',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Subordinating Conjunction',\n",
       " 'Verb',\n",
       " 'Numeral',\n",
       " 'Verb',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Pronoun',\n",
       " 'Verb',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Verb',\n",
       " 'Verb',\n",
       " 'DET',\n",
       " 'Noun',\n",
       " 'Adjective',\n",
       " 'Subordinating Conjunction',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Noun',\n",
       " 'Verb']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list_pos_test = [tag[1] for tag in tag_list]\n",
    "tag_list_pos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "first_person_flags = []\n",
    "lang = 'ua'\n",
    "first_person_flags = [True if word.lower() in FIRST_PERSON_PRONOUNS_T[lang] else np.nan \n",
    "                        for word, pos, _ in tag_list]\n",
    "first_person_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " 'Present',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Past',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Past',\n",
       " nan,\n",
       " 'Past',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Past',\n",
       " nan,\n",
       " nan,\n",
       " 'Past',\n",
       " 'Past',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'Past']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_tense_list = []\n",
    "for (_, pos, verb_tense) in tag_list:\n",
    "    if pos == \"Verb\":\n",
    "        verb_tense_list.append(verb_tense)\n",
    "    else:\n",
    "        verb_tense_list.append(np.nan)\n",
    "verb_tense_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4390243902439024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek/Desktop/projects/own/own_venv/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3255813953488373\n"
     ]
    }
   ],
   "source": [
    "def calculate_first_person_percentage(text, lang='en'):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of first person pronouns in the input text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to be analyzed.\n",
    "        lang (str): Language code ('en' for English, 'ua' or 'uk' for Ukrainian).\n",
    "    \n",
    "    Returns:\n",
    "        float: The percentage of first person pronouns in the text, or np.nan if no tokens.\n",
    "    \"\"\"\n",
    "    if lang in ['ua', 'uk']:\n",
    "        nlp = spacy.load(\"uk_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Обработка текста\n",
    "    doc = nlp(text)\n",
    "    total_tokens = len(doc)\n",
    "    if total_tokens == 0:\n",
    "        return np.nan\n",
    "\n",
    "    first_person_count = sum(1 for token in doc if token.text.lower() in FIRST_PERSON_PRONOUNS_T[lang])\n",
    "    \n",
    "    return (first_person_count / total_tokens) * 100\n",
    "    \n",
    "text = \"мій еней був парубок моторний і хлопець хоч куди козак удався все зле проворний закінченню завзятіший всіх бурлак греки як спаливши троє зробили знижку карикатурною він взявши торбу життя подав забравши деяких троянців осмалених як гиря уряд квантів п'ятами строї накрила\"\n",
    "print(calculate_first_person_percentage(text, lang='ua'))\n",
    "text_en = \" \".join(word_list_en)\n",
    "print(calculate_first_person_percentage(text_en, lang='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'once a my young agile and lad though wherever a cossack succeeded everything wrong nimble ending most persistent all haidamak like having burned three made discount caricature he taking bag life gave taking some trojans singed like weight government quanta heels ranks covered'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['first_person']\n",
    "words['word_coherence']\n",
    "words['word_coherence_variability_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мені', 'Pronoun', None),\n",
       " ('моє', 'DET', None),\n",
       " ('моя', 'DET', None),\n",
       " ('мій', 'DET', None),\n",
       " ('мною', 'Pronoun', None),\n",
       " ('я', 'Pronoun', None),\n",
       " ('мене', 'Pronoun', None),\n",
       " ('мої', 'DET', None)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['мені', 'моє', 'моя', 'мій', 'мною', 'я', 'мене', 'мої']\n",
    "tag_list = get_tag_l_test(test, lang='ua')\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns['syllables_per_min'] -> \\/\n",
    "turns['first_person_sentiment_negative'] # ?\n",
    "turns['word_repeat_percentage'] # ? \n",
    "turns['phrase_repeat_percentage'] -> phrases_texts\n",
    "turns['first_order_sentence_tangeniality'] -> phrases_texts\n",
    "turns['second_order_sentence_tangeniality'] -> phrases_texts\n",
    "turns['turn_to_turn_tangeniality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stay here....\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openwillis.face'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 939\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStay here....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# print(openwillis.speech.__file__)\u001b[39;00m\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenwillis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspeech\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mows\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenwillis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mowt\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/final_airest_voice/airest/openwillis/openwillis/src/openwillis/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     facial_expressivity,\n\u001b[1;32m      3\u001b[0m     emotional_expressivity,\n\u001b[1;32m      4\u001b[0m     eye_blink_rate,\n\u001b[1;32m      5\u001b[0m     preprocess_face_video,\n\u001b[1;32m      6\u001b[0m     create_cropped_video\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvoice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     vocal_acoustics,\n\u001b[1;32m     10\u001b[0m     audio_preprocess,\n\u001b[1;32m     11\u001b[0m     phonation_acoustics,\n\u001b[1;32m     12\u001b[0m     to_audio\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspeech\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     speech_characteristics,\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openwillis.face'"
     ]
    }
   ],
   "source": [
    "transcript_json_ua_3 = {'text': \" Мій еней був парабок моторний, і хлопець, хоч куди козак, удався на все злепроворний, завзятявши й от всіх бурлак. Но греки, як спаливши трою, зробили з неї скирту гною. Він, взявши торбу, тягу дав, забравши деяких троянців, осмалених як гиря ланців, пам'ятами п'ятами з трої накидав. Він, швидко проробивши човни, на синє море попускав троянців, насажавши повні і куди очі, почурхав. Но зла юнона, суча дочка, розкрутку дахтилась, як квочка. Еней не любила страх, давно вже вона хотіла, щоб його душка полетіла к чортам, і щоб і дух не пах. Еней був тяжко не по серцю юноні, все її гнівив. Здавався, гірчіншій їй відперцю, ні в чим юноні не просив. Но гірзати їй не любився. Дякую. Дякую. Дякую. Дякую. Дякую.\",\n",
    " 'segments': [{'id': 0,\n",
    "   'seek': 0,\n",
    "   'start': 0.8400000000000003,\n",
    "   'end': 8.48,\n",
    "   'text': ' Мій еней був парабок моторний, і хлопець, хоч куди козак, удався на все злепроворний, завзятявши й от всіх бурлак.',\n",
    "   'tokens': [50365,\n",
    "    3493,\n",
    "    16471,\n",
    "    1997,\n",
    "    14418,\n",
    "    1268,\n",
    "    9748,\n",
    "    11813,\n",
    "    3771,\n",
    "    2637,\n",
    "    9971,\n",
    "    2542,\n",
    "    15199,\n",
    "    11,\n",
    "    8934,\n",
    "    45566,\n",
    "    3762,\n",
    "    10935,\n",
    "    678,\n",
    "    11,\n",
    "    13057,\n",
    "    981,\n",
    "    45462,\n",
    "    981,\n",
    "    3434,\n",
    "    1272,\n",
    "    11,\n",
    "    11927,\n",
    "    1828,\n",
    "    1679,\n",
    "    1470,\n",
    "    4640,\n",
    "    1423,\n",
    "    693,\n",
    "    5018,\n",
    "    6680,\n",
    "    1717,\n",
    "    15199,\n",
    "    11,\n",
    "    13388,\n",
    "    1544,\n",
    "    4558,\n",
    "    12998,\n",
    "    1198,\n",
    "    435,\n",
    "    24540,\n",
    "    2943,\n",
    "    2852,\n",
    "    1827,\n",
    "    1157,\n",
    "    1268,\n",
    "    7489,\n",
    "    693,\n",
    "    1272,\n",
    "    13,\n",
    "    50789],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.09687371865296975,\n",
    "   'compression_ratio': 1.997093023255814,\n",
    "   'no_speech_prob': 8.072216506338492e-11,\n",
    "   'words': [{'word': ' Мій',\n",
    "     'start': 0.8400000000000003,\n",
    "     'end': 1.52,\n",
    "     'probability': 0.6422673165798187},\n",
    "    {'word': ' еней',\n",
    "     'start': 1.52,\n",
    "     'end': 1.92,\n",
    "     'probability': 0.7646919190883636},\n",
    "    {'word': ' був',\n",
    "     'start': 1.92,\n",
    "     'end': 2.22,\n",
    "     'probability': 0.9951876401901245},\n",
    "    {'word': ' парабок',\n",
    "     'start': 2.22,\n",
    "     'end': 2.74,\n",
    "     'probability': 0.7586871186892191},\n",
    "    {'word': ' моторний,',\n",
    "     'start': 2.74,\n",
    "     'end': 3.3,\n",
    "     'probability': 0.9474562009175619},\n",
    "    {'word': ' і',\n",
    "     'start': 3.38,\n",
    "     'end': 3.46,\n",
    "     'probability': 0.9635983109474182},\n",
    "    {'word': ' хлопець,',\n",
    "     'start': 3.46,\n",
    "     'end': 3.96,\n",
    "     'probability': 0.9997376501560211},\n",
    "    {'word': ' хоч',\n",
    "     'start': 3.96,\n",
    "     'end': 4.14,\n",
    "     'probability': 0.9908085465431213},\n",
    "    {'word': ' куди',\n",
    "     'start': 4.14,\n",
    "     'end': 4.44,\n",
    "     'probability': 0.9795309007167816},\n",
    "    {'word': ' козак,',\n",
    "     'start': 4.44,\n",
    "     'end': 4.86,\n",
    "     'probability': 0.9975240429242452},\n",
    "    {'word': ' удався',\n",
    "     'start': 4.96,\n",
    "     'end': 5.38,\n",
    "     'probability': 0.9289791186650594},\n",
    "    {'word': ' на',\n",
    "     'start': 5.38,\n",
    "     'end': 5.54,\n",
    "     'probability': 0.9419454336166382},\n",
    "    {'word': ' все',\n",
    "     'start': 5.54,\n",
    "     'end': 5.76,\n",
    "     'probability': 0.9966894388198853},\n",
    "    {'word': ' злепроворний,',\n",
    "     'start': 5.76,\n",
    "     'end': 6.68,\n",
    "     'probability': 0.8388001720110575},\n",
    "    {'word': ' завзятявши',\n",
    "     'start': 6.84,\n",
    "     'end': 7.5,\n",
    "     'probability': 0.9592462380727133},\n",
    "    {'word': ' й',\n",
    "     'start': 7.5,\n",
    "     'end': 7.54,\n",
    "     'probability': 0.7431787848472595},\n",
    "    {'word': ' от',\n",
    "     'start': 7.54,\n",
    "     'end': 7.74,\n",
    "     'probability': 0.9727557897567749},\n",
    "    {'word': ' всіх',\n",
    "     'start': 7.74,\n",
    "     'end': 8.02,\n",
    "     'probability': 0.9985808332761129},\n",
    "    {'word': ' бурлак.',\n",
    "     'start': 8.02,\n",
    "     'end': 8.48,\n",
    "     'probability': 0.9941712617874146}]},\n",
    "  {'id': 1,\n",
    "   'seek': 0,\n",
    "   'start': 8.66,\n",
    "   'end': 21.34,\n",
    "   'text': \" Но греки, як спаливши трою, зробили з неї скирту гною. Він, взявши торбу, тягу дав, забравши деяких троянців, осмалених як гиря ланців, пам'ятами п'ятами з трої накидав.\",\n",
    "   'tokens': [50789,\n",
    "    7264,\n",
    "    11726,\n",
    "    2872,\n",
    "    435,\n",
    "    11,\n",
    "    14760,\n",
    "    5307,\n",
    "    1218,\n",
    "    2739,\n",
    "    1198,\n",
    "    435,\n",
    "    7550,\n",
    "    30815,\n",
    "    11,\n",
    "    1423,\n",
    "    17129,\n",
    "    5435,\n",
    "    1423,\n",
    "    1725,\n",
    "    8045,\n",
    "    776,\n",
    "    2241,\n",
    "    481,\n",
    "    13549,\n",
    "    2342,\n",
    "    1234,\n",
    "    1148,\n",
    "    13,\n",
    "    2348,\n",
    "    17113,\n",
    "    11,\n",
    "    11892,\n",
    "    12998,\n",
    "    1198,\n",
    "    435,\n",
    "    25594,\n",
    "    33567,\n",
    "    11,\n",
    "    1069,\n",
    "    25217,\n",
    "    585,\n",
    "    12472,\n",
    "    11,\n",
    "    13890,\n",
    "    5608,\n",
    "    1198,\n",
    "    435,\n",
    "    36397,\n",
    "    681,\n",
    "    11855,\n",
    "    7550,\n",
    "    354,\n",
    "    15737,\n",
    "    1814,\n",
    "    9718,\n",
    "    11,\n",
    "    8940,\n",
    "    919,\n",
    "    1218,\n",
    "    2495,\n",
    "    1157,\n",
    "    14760,\n",
    "    2342,\n",
    "    4490,\n",
    "    681,\n",
    "    2344,\n",
    "    1416,\n",
    "    1814,\n",
    "    9718,\n",
    "    11,\n",
    "    39164,\n",
    "    6,\n",
    "    4558,\n",
    "    5150,\n",
    "    713,\n",
    "    6,\n",
    "    4558,\n",
    "    5150,\n",
    "    1423,\n",
    "    7550,\n",
    "    49065,\n",
    "    20955,\n",
    "    3586,\n",
    "    1828,\n",
    "    13,\n",
    "    51432],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.09687371865296975,\n",
    "   'compression_ratio': 1.997093023255814,\n",
    "   'no_speech_prob': 8.072216506338492e-11,\n",
    "   'words': [{'word': ' Но',\n",
    "     'start': 8.66,\n",
    "     'end': 8.92,\n",
    "     'probability': 0.6577830910682678},\n",
    "    {'word': ' греки,',\n",
    "     'start': 8.92,\n",
    "     'end': 9.32,\n",
    "     'probability': 0.9954709410667419},\n",
    "    {'word': ' як',\n",
    "     'start': 9.38,\n",
    "     'end': 9.48,\n",
    "     'probability': 0.9997451901435852},\n",
    "    {'word': ' спаливши',\n",
    "     'start': 9.48,\n",
    "     'end': 10.06,\n",
    "     'probability': 0.9991959929466248},\n",
    "    {'word': ' трою,',\n",
    "     'start': 10.06,\n",
    "     'end': 10.38,\n",
    "     'probability': 0.9777328670024872},\n",
    "    {'word': ' зробили',\n",
    "     'start': 10.5,\n",
    "     'end': 10.94,\n",
    "     'probability': 0.9996898174285889},\n",
    "    {'word': ' з',\n",
    "     'start': 10.94,\n",
    "     'end': 11.06,\n",
    "     'probability': 0.9992071986198425},\n",
    "    {'word': ' неї',\n",
    "     'start': 11.06,\n",
    "     'end': 11.28,\n",
    "     'probability': 0.9586704671382904},\n",
    "    {'word': ' скирту',\n",
    "     'start': 11.28,\n",
    "     'end': 11.8,\n",
    "     'probability': 0.9101131856441498},\n",
    "    {'word': ' гною.',\n",
    "     'start': 11.8,\n",
    "     'end': 12.3,\n",
    "     'probability': 0.9980077544848124},\n",
    "    {'word': ' Він,',\n",
    "     'start': 12.46,\n",
    "     'end': 12.8,\n",
    "     'probability': 0.9989625513553619},\n",
    "    {'word': ' взявши',\n",
    "     'start': 12.9,\n",
    "     'end': 13.46,\n",
    "     'probability': 0.9994816482067108},\n",
    "    {'word': ' торбу,',\n",
    "     'start': 13.46,\n",
    "     'end': 13.94,\n",
    "     'probability': 0.9922364056110382},\n",
    "    {'word': ' тягу',\n",
    "     'start': 14.2,\n",
    "     'end': 14.52,\n",
    "     'probability': 0.9912166396776835},\n",
    "    {'word': ' дав,',\n",
    "     'start': 14.52,\n",
    "     'end': 14.7,\n",
    "     'probability': 0.9983481168746948},\n",
    "    {'word': ' забравши',\n",
    "     'start': 14.92,\n",
    "     'end': 15.5,\n",
    "     'probability': 0.9997718185186386},\n",
    "    {'word': ' деяких',\n",
    "     'start': 15.5,\n",
    "     'end': 15.94,\n",
    "     'probability': 0.999702533086141},\n",
    "    {'word': ' троянців,',\n",
    "     'start': 15.94,\n",
    "     'end': 16.54,\n",
    "     'probability': 0.9982081174850463},\n",
    "    {'word': ' осмалених',\n",
    "     'start': 16.7,\n",
    "     'end': 17.32,\n",
    "     'probability': 0.9201163172721862},\n",
    "    {'word': ' як',\n",
    "     'start': 17.32,\n",
    "     'end': 17.54,\n",
    "     'probability': 0.5358445048332214},\n",
    "    {'word': ' гиря',\n",
    "     'start': 17.54,\n",
    "     'end': 18.02,\n",
    "     'probability': 0.9868524074554443},\n",
    "    {'word': ' ланців,',\n",
    "     'start': 18.02,\n",
    "     'end': 18.5,\n",
    "     'probability': 0.994851291179657},\n",
    "    {'word': ' пам',\n",
    "     'start': 18.84,\n",
    "     'end': 19.06,\n",
    "     'probability': 0.9989840388298035},\n",
    "    {'word': \"'ятами\",\n",
    "     'start': 19.06,\n",
    "     'end': 19.48,\n",
    "     'probability': 0.9953069090843201},\n",
    "    {'word': ' п',\n",
    "     'start': 19.48,\n",
    "     'end': 19.96,\n",
    "     'probability': 0.7916982173919678},\n",
    "    {'word': \"'ятами\",\n",
    "     'start': 19.96,\n",
    "     'end': 20.34,\n",
    "     'probability': 0.9915903210639954},\n",
    "    {'word': ' з',\n",
    "     'start': 20.34,\n",
    "     'end': 20.58,\n",
    "     'probability': 0.6393404006958008},\n",
    "    {'word': ' трої',\n",
    "     'start': 20.58,\n",
    "     'end': 20.78,\n",
    "     'probability': 0.7992312014102936},\n",
    "    {'word': ' накидав.',\n",
    "     'start': 20.78,\n",
    "     'end': 21.34,\n",
    "     'probability': 0.9917691151301066}]},\n",
    "  {'id': 2,\n",
    "   'seek': 0,\n",
    "   'start': 21.52,\n",
    "   'end': 29.3,\n",
    "   'text': ' Він, швидко проробивши човни, на синє море попускав троянців, насажавши повні і куди очі, почурхав.',\n",
    "   'tokens': [51464,\n",
    "    2348,\n",
    "    17113,\n",
    "    11,\n",
    "    5941,\n",
    "    42172,\n",
    "    3752,\n",
    "    1285,\n",
    "    1717,\n",
    "    2061,\n",
    "    2739,\n",
    "    1198,\n",
    "    435,\n",
    "    1358,\n",
    "    1055,\n",
    "    1903,\n",
    "    11,\n",
    "    1470,\n",
    "    47079,\n",
    "    8403,\n",
    "    24127,\n",
    "    387,\n",
    "    10694,\n",
    "    21303,\n",
    "    1828,\n",
    "    7550,\n",
    "    354,\n",
    "    15737,\n",
    "    1814,\n",
    "    9718,\n",
    "    11,\n",
    "    6519,\n",
    "    3234,\n",
    "    1828,\n",
    "    1198,\n",
    "    435,\n",
    "    10499,\n",
    "    10793,\n",
    "    8934,\n",
    "    981,\n",
    "    45462,\n",
    "    5875,\n",
    "    1827,\n",
    "    11,\n",
    "    12079,\n",
    "    7489,\n",
    "    1157,\n",
    "    1828,\n",
    "    13,\n",
    "    51831],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.09687371865296975,\n",
    "   'compression_ratio': 1.997093023255814,\n",
    "   'no_speech_prob': 8.072216506338492e-11,\n",
    "   'words': [{'word': ' Він,',\n",
    "     'start': 21.52,\n",
    "     'end': 22.2,\n",
    "     'probability': 0.9995375871658325},\n",
    "    {'word': ' швидко',\n",
    "     'start': 22.22,\n",
    "     'end': 22.64,\n",
    "     'probability': 0.9994813601175944},\n",
    "    {'word': ' проробивши',\n",
    "     'start': 22.64,\n",
    "     'end': 23.28,\n",
    "     'probability': 0.9968960483868917},\n",
    "    {'word': ' човни,',\n",
    "     'start': 23.28,\n",
    "     'end': 23.7,\n",
    "     'probability': 0.9618228276570638},\n",
    "    {'word': ' на',\n",
    "     'start': 23.84,\n",
    "     'end': 24.06,\n",
    "     'probability': 0.9345859289169312},\n",
    "    {'word': ' синє',\n",
    "     'start': 24.06,\n",
    "     'end': 24.5,\n",
    "     'probability': 0.7359660416841507},\n",
    "    {'word': ' море',\n",
    "     'start': 24.5,\n",
    "     'end': 24.9,\n",
    "     'probability': 0.9987072944641113},\n",
    "    {'word': ' попускав',\n",
    "     'start': 24.9,\n",
    "     'end': 25.54,\n",
    "     'probability': 0.9966495037078857},\n",
    "    {'word': ' троянців,',\n",
    "     'start': 25.54,\n",
    "     'end': 26.2,\n",
    "     'probability': 0.9261908054351806},\n",
    "    {'word': ' насажавши',\n",
    "     'start': 26.24,\n",
    "     'end': 26.92,\n",
    "     'probability': 0.9588270425796509},\n",
    "    {'word': ' повні',\n",
    "     'start': 26.92,\n",
    "     'end': 27.32,\n",
    "     'probability': 0.998679518699646},\n",
    "    {'word': ' і',\n",
    "     'start': 27.32,\n",
    "     'end': 27.54,\n",
    "     'probability': 0.6929349303245544},\n",
    "    {'word': ' куди',\n",
    "     'start': 27.54,\n",
    "     'end': 28.24,\n",
    "     'probability': 0.9374934434890747},\n",
    "    {'word': ' очі,',\n",
    "     'start': 28.24,\n",
    "     'end': 28.66,\n",
    "     'probability': 0.9977817237377167},\n",
    "    {'word': ' почурхав.',\n",
    "     'start': 28.78,\n",
    "     'end': 29.3,\n",
    "     'probability': 0.9702638387680054}]},\n",
    "  {'id': 3,\n",
    "   'seek': 2930,\n",
    "   'start': 29.3,\n",
    "   'end': 40.88,\n",
    "   'text': ' Но зла юнона, суча дочка, розкрутку дахтилась, як квочка. Еней не любила страх, давно вже вона хотіла, щоб його душка полетіла к чортам, і щоб і дух не пах.',\n",
    "   'tokens': [50365,\n",
    "    7264,\n",
    "    1423,\n",
    "    4989,\n",
    "    29488,\n",
    "    489,\n",
    "    14411,\n",
    "    11,\n",
    "    776,\n",
    "    4187,\n",
    "    386,\n",
    "    1070,\n",
    "    29576,\n",
    "    11,\n",
    "    20681,\n",
    "    755,\n",
    "    47220,\n",
    "    4401,\n",
    "    1070,\n",
    "    7004,\n",
    "    403,\n",
    "    19522,\n",
    "    11,\n",
    "    14760,\n",
    "    35350,\n",
    "    29576,\n",
    "    13,\n",
    "    6538,\n",
    "    14418,\n",
    "    1725,\n",
    "    9875,\n",
    "    10578,\n",
    "    50190,\n",
    "    11,\n",
    "    40086,\n",
    "    40738,\n",
    "    740,\n",
    "    14411,\n",
    "    11515,\n",
    "    1827,\n",
    "    4989,\n",
    "    11,\n",
    "    42899,\n",
    "    44123,\n",
    "    1070,\n",
    "    34187,\n",
    "    4692,\n",
    "    1094,\n",
    "    1827,\n",
    "    4989,\n",
    "    981,\n",
    "    1358,\n",
    "    20040,\n",
    "    1685,\n",
    "    11,\n",
    "    8934,\n",
    "    42899,\n",
    "    8934,\n",
    "    35535,\n",
    "    1725,\n",
    "    713,\n",
    "    7004,\n",
    "    13,\n",
    "    50945],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.14866265190972222,\n",
    "   'compression_ratio': 1.8929889298892988,\n",
    "   'no_speech_prob': 1.6906970268237842e-10,\n",
    "   'words': [{'word': ' Но',\n",
    "     'start': 29.3,\n",
    "     'end': 29.86,\n",
    "     'probability': 0.623090922832489},\n",
    "    {'word': ' зла',\n",
    "     'start': 29.86,\n",
    "     'end': 30.1,\n",
    "     'probability': 0.9888802170753479},\n",
    "    {'word': ' юнона,',\n",
    "     'start': 30.1,\n",
    "     'end': 30.58,\n",
    "     'probability': 0.755329449971517},\n",
    "    {'word': ' суча',\n",
    "     'start': 30.7,\n",
    "     'end': 31.02,\n",
    "     'probability': 0.9929297963778178},\n",
    "    {'word': ' дочка,',\n",
    "     'start': 31.02,\n",
    "     'end': 31.38,\n",
    "     'probability': 0.9979160726070404},\n",
    "    {'word': ' розкрутку',\n",
    "     'start': 31.44,\n",
    "     'end': 31.94,\n",
    "     'probability': 0.761854350566864},\n",
    "    {'word': ' дахтилась,',\n",
    "     'start': 31.94,\n",
    "     'end': 32.42,\n",
    "     'probability': 0.8515902012586594},\n",
    "    {'word': ' як',\n",
    "     'start': 32.52,\n",
    "     'end': 32.64,\n",
    "     'probability': 0.9911714196205139},\n",
    "    {'word': ' квочка.',\n",
    "     'start': 32.64,\n",
    "     'end': 33.2,\n",
    "     'probability': 0.9285567700862885},\n",
    "    {'word': ' Еней',\n",
    "     'start': 34.26,\n",
    "     'end': 34.5,\n",
    "     'probability': 0.378426730632782},\n",
    "    {'word': ' не',\n",
    "     'start': 34.5,\n",
    "     'end': 34.78,\n",
    "     'probability': 0.9615265130996704},\n",
    "    {'word': ' любила',\n",
    "     'start': 34.78,\n",
    "     'end': 35.16,\n",
    "     'probability': 0.9993954002857208},\n",
    "    {'word': ' страх,',\n",
    "     'start': 35.16,\n",
    "     'end': 35.54,\n",
    "     'probability': 0.9968419075012207},\n",
    "    {'word': ' давно',\n",
    "     'start': 35.72,\n",
    "     'end': 35.98,\n",
    "     'probability': 0.835843563079834},\n",
    "    {'word': ' вже',\n",
    "     'start': 35.98,\n",
    "     'end': 36.26,\n",
    "     'probability': 0.4714806377887726},\n",
    "    {'word': ' вона',\n",
    "     'start': 36.26,\n",
    "     'end': 36.52,\n",
    "     'probability': 0.9988734126091003},\n",
    "    {'word': ' хотіла,',\n",
    "     'start': 36.52,\n",
    "     'end': 37.0,\n",
    "     'probability': 0.9996220866839091},\n",
    "    {'word': ' щоб',\n",
    "     'start': 37.1,\n",
    "     'end': 37.28,\n",
    "     'probability': 0.9821822047233582},\n",
    "    {'word': ' його',\n",
    "     'start': 37.28,\n",
    "     'end': 37.62,\n",
    "     'probability': 0.9992345571517944},\n",
    "    {'word': ' душка',\n",
    "     'start': 37.62,\n",
    "     'end': 38.1,\n",
    "     'probability': 0.6397128254175186},\n",
    "    {'word': ' полетіла',\n",
    "     'start': 38.1,\n",
    "     'end': 38.72,\n",
    "     'probability': 0.9991815090179443},\n",
    "    {'word': ' к',\n",
    "     'start': 38.72,\n",
    "     'end': 38.84,\n",
    "     'probability': 0.9559592008590698},\n",
    "    {'word': ' чортам,',\n",
    "     'start': 38.84,\n",
    "     'end': 39.38,\n",
    "     'probability': 0.969536026318868},\n",
    "    {'word': ' і',\n",
    "     'start': 39.56,\n",
    "     'end': 39.7,\n",
    "     'probability': 0.8777403235435486},\n",
    "    {'word': ' щоб',\n",
    "     'start': 39.7,\n",
    "     'end': 39.88,\n",
    "     'probability': 0.9940900206565857},\n",
    "    {'word': ' і',\n",
    "     'start': 39.88,\n",
    "     'end': 40.1,\n",
    "     'probability': 0.9966331124305725},\n",
    "    {'word': ' дух',\n",
    "     'start': 40.1,\n",
    "     'end': 40.38,\n",
    "     'probability': 0.9925909042358398},\n",
    "    {'word': ' не',\n",
    "     'start': 40.38,\n",
    "     'end': 40.54,\n",
    "     'probability': 0.9974307417869568},\n",
    "    {'word': ' пах.',\n",
    "     'start': 40.54,\n",
    "     'end': 40.88,\n",
    "     'probability': 0.9964443445205688}]},\n",
    "  {'id': 4,\n",
    "   'seek': 2930,\n",
    "   'start': 42.34,\n",
    "   'end': 52.62,\n",
    "   'text': ' Еней був тяжко не по серцю юноні, все її гнівив. Здавався, гірчіншій їй відперцю, ні в чим юноні не просив.',\n",
    "   'tokens': [51030,\n",
    "    6538,\n",
    "    14418,\n",
    "    1268,\n",
    "    9748,\n",
    "    34641,\n",
    "    3752,\n",
    "    1725,\n",
    "    2801,\n",
    "    14490,\n",
    "    1814,\n",
    "    1148,\n",
    "    29488,\n",
    "    489,\n",
    "    1784,\n",
    "    1827,\n",
    "    11,\n",
    "    4640,\n",
    "    27902,\n",
    "    8045,\n",
    "    2342,\n",
    "    489,\n",
    "    9718,\n",
    "    2739,\n",
    "    13,\n",
    "    17613,\n",
    "    1828,\n",
    "    1828,\n",
    "    1679,\n",
    "    11,\n",
    "    2342,\n",
    "    36154,\n",
    "    753,\n",
    "    17113,\n",
    "    1198,\n",
    "    16471,\n",
    "    27902,\n",
    "    1644,\n",
    "    16947,\n",
    "    14566,\n",
    "    1814,\n",
    "    1148,\n",
    "    11,\n",
    "    46645,\n",
    "    740,\n",
    "    1358,\n",
    "    2165,\n",
    "    29488,\n",
    "    489,\n",
    "    1784,\n",
    "    1827,\n",
    "    1725,\n",
    "    21109,\n",
    "    2739,\n",
    "    13,\n",
    "    51531],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.14866265190972222,\n",
    "   'compression_ratio': 1.8929889298892988,\n",
    "   'no_speech_prob': 1.6906970268237842e-10,\n",
    "   'words': [{'word': ' Еней',\n",
    "     'start': 42.34,\n",
    "     'end': 42.9,\n",
    "     'probability': 0.990341305732727},\n",
    "    {'word': ' був',\n",
    "     'start': 42.9,\n",
    "     'end': 43.1,\n",
    "     'probability': 0.9993610978126526},\n",
    "    {'word': ' тяжко',\n",
    "     'start': 43.1,\n",
    "     'end': 43.5,\n",
    "     'probability': 0.996361494064331},\n",
    "    {'word': ' не',\n",
    "     'start': 43.5,\n",
    "     'end': 43.64,\n",
    "     'probability': 0.9879442453384399},\n",
    "    {'word': ' по',\n",
    "     'start': 43.64,\n",
    "     'end': 43.84,\n",
    "     'probability': 0.9994526505470276},\n",
    "    {'word': ' серцю',\n",
    "     'start': 43.84,\n",
    "     'end': 44.24,\n",
    "     'probability': 0.9581860105196635},\n",
    "    {'word': ' юноні,',\n",
    "     'start': 44.24,\n",
    "     'end': 44.86,\n",
    "     'probability': 0.9107691049575806},\n",
    "    {'word': ' все',\n",
    "     'start': 44.9,\n",
    "     'end': 45.12,\n",
    "     'probability': 0.9964738488197327},\n",
    "    {'word': ' її',\n",
    "     'start': 45.12,\n",
    "     'end': 45.4,\n",
    "     'probability': 0.9885966777801514},\n",
    "    {'word': ' гнівив.',\n",
    "     'start': 45.4,\n",
    "     'end': 45.9,\n",
    "     'probability': 0.9962864071130753},\n",
    "    {'word': ' Здавався,',\n",
    "     'start': 46.6,\n",
    "     'end': 47.16,\n",
    "     'probability': 0.995691329240799},\n",
    "    {'word': ' гірчіншій',\n",
    "     'start': 47.28,\n",
    "     'end': 49.5,\n",
    "     'probability': 0.7558990021546682},\n",
    "    {'word': ' їй',\n",
    "     'start': 49.5,\n",
    "     'end': 50.06,\n",
    "     'probability': 0.9615082740783691},\n",
    "    {'word': ' відперцю,',\n",
    "     'start': 50.06,\n",
    "     'end': 50.82,\n",
    "     'probability': 0.8438445925712585},\n",
    "    {'word': ' ні',\n",
    "     'start': 50.92,\n",
    "     'end': 51.12,\n",
    "     'probability': 0.9655731320381165},\n",
    "    {'word': ' в',\n",
    "     'start': 51.12,\n",
    "     'end': 51.28,\n",
    "     'probability': 0.9971381425857544},\n",
    "    {'word': ' чим',\n",
    "     'start': 51.28,\n",
    "     'end': 51.52,\n",
    "     'probability': 0.9903046190738678},\n",
    "    {'word': ' юноні',\n",
    "     'start': 51.52,\n",
    "     'end': 51.98,\n",
    "     'probability': 0.9986999481916428},\n",
    "    {'word': ' не',\n",
    "     'start': 51.98,\n",
    "     'end': 52.1,\n",
    "     'probability': 0.999115526676178},\n",
    "    {'word': ' просив.',\n",
    "     'start': 52.1,\n",
    "     'end': 52.62,\n",
    "     'probability': 0.9998294413089752}]},\n",
    "  {'id': 5,\n",
    "   'seek': 2930,\n",
    "   'start': 53.78,\n",
    "   'end': 55.8,\n",
    "   'text': ' Но гірзати їй не любився.',\n",
    "   'tokens': [51581,\n",
    "    7264,\n",
    "    2342,\n",
    "    36154,\n",
    "    1544,\n",
    "    9585,\n",
    "    27902,\n",
    "    1644,\n",
    "    1725,\n",
    "    9875,\n",
    "    2739,\n",
    "    1679,\n",
    "    13,\n",
    "    51692],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.14866265190972222,\n",
    "   'compression_ratio': 1.8929889298892988,\n",
    "   'no_speech_prob': 1.6906970268237842e-10,\n",
    "   'words': [{'word': ' Но',\n",
    "     'start': 53.78,\n",
    "     'end': 54.34,\n",
    "     'probability': 0.8985239863395691},\n",
    "    {'word': ' гірзати',\n",
    "     'start': 54.34,\n",
    "     'end': 54.88,\n",
    "     'probability': 0.931101843714714},\n",
    "    {'word': ' їй',\n",
    "     'start': 54.88,\n",
    "     'end': 55.1,\n",
    "     'probability': 0.9463007748126984},\n",
    "    {'word': ' не',\n",
    "     'start': 55.1,\n",
    "     'end': 55.24,\n",
    "     'probability': 0.9978616833686829},\n",
    "    {'word': ' любився.',\n",
    "     'start': 55.24,\n",
    "     'end': 55.8,\n",
    "     'probability': 0.9991166591644287}]},\n",
    "  {'id': 6,\n",
    "   'seek': 5930,\n",
    "   'start': 59.3,\n",
    "   'end': 60.66,\n",
    "   'text': ' Дякую.',\n",
    "   'tokens': [50365, 3401, 681, 35119, 13, 50465],\n",
    "   'temperature': 0.4,\n",
    "   'avg_logprob': -0.393883204460144,\n",
    "   'compression_ratio': 1.4583333333333333,\n",
    "   'no_speech_prob': 1.47235015290903e-08,\n",
    "   'words': [{'word': ' Дякую.',\n",
    "     'start': 59.3,\n",
    "     'end': 60.66,\n",
    "     'probability': 0.8371396462122599}]},\n",
    "  {'id': 7,\n",
    "   'seek': 5930,\n",
    "   'start': 61.3,\n",
    "   'end': 62.1,\n",
    "   'text': ' Дякую.',\n",
    "   'tokens': [50465, 3401, 681, 35119, 13, 50565],\n",
    "   'temperature': 0.4,\n",
    "   'avg_logprob': -0.393883204460144,\n",
    "   'compression_ratio': 1.4583333333333333,\n",
    "   'no_speech_prob': 1.47235015290903e-08,\n",
    "   'words': [{'word': ' Дякую.',\n",
    "     'start': 61.3,\n",
    "     'end': 62.1,\n",
    "     'probability': 0.8776981234550476}]},\n",
    "  {'id': 8,\n",
    "   'seek': 5930,\n",
    "   'start': 63.3,\n",
    "   'end': 63.34,\n",
    "   'text': ' Дякую.',\n",
    "   'tokens': [50565, 3401, 681, 35119, 13, 50665],\n",
    "   'temperature': 0.4,\n",
    "   'avg_logprob': -0.393883204460144,\n",
    "   'compression_ratio': 1.4583333333333333,\n",
    "   'no_speech_prob': 1.47235015290903e-08,\n",
    "   'words': [{'word': ' Дякую.',\n",
    "     'start': 62.64,\n",
    "     'end': 63.34,\n",
    "     'probability': 0.9730692108472189}]},\n",
    "  {'id': 9,\n",
    "   'seek': 6334,\n",
    "   'start': 63.34,\n",
    "   'end': 63.42,\n",
    "   'text': ' Дякую.',\n",
    "   'tokens': [50365, 3401, 681, 35119, 13, 50465],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.14914451326642716,\n",
    "   'compression_ratio': 1.0,\n",
    "   'no_speech_prob': 1.2345171995775672e-08,\n",
    "   'words': [{'word': ' Дякую.',\n",
    "     'start': 63.34,\n",
    "     'end': 63.42,\n",
    "     'probability': 0.861284613609314}]},\n",
    "  {'id': 10,\n",
    "   'seek': 6334,\n",
    "   'start': 64.66000000000001,\n",
    "   'end': 64.98,\n",
    "   'text': ' Дякую.',\n",
    "   'tokens': [50465, 3401, 681, 35119, 13, 50565],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.14914451326642716,\n",
    "   'compression_ratio': 1.0,\n",
    "   'no_speech_prob': 1.2345171995775672e-08,\n",
    "   'words': [{'word': ' Дякую.',\n",
    "     'start': 64.66000000000001,\n",
    "     'end': 64.98,\n",
    "     'probability': 0.8964025775591532}]},\n",
    "  {'id': 11,\n",
    "   'seek': 6498,\n",
    "   'start': 64.96,\n",
    "   'end': 64.96,\n",
    "   'text': '',\n",
    "   'tokens': [],\n",
    "   'temperature': 0.0,\n",
    "   'avg_logprob': -0.11033681460789271,\n",
    "   'compression_ratio': 0.55,\n",
    "   'no_speech_prob': 1.7829767884336434e-08,\n",
    "   'words': []}],\n",
    " 'language': 'uk'}\n",
    "\n",
    "# import openwillis.speech\n",
    "print()\n",
    "print(\"Stay here....\")\n",
    "# print(openwillis.speech.__file__)\n",
    "\n",
    "import openwillis.speech as ows\n",
    "import openwillis.transcribe as owt\n",
    "print(\"continue...\")\n",
    "\n",
    "\n",
    "import os\n",
    "import certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import traceback\n",
    "try:\n",
    "    words, turns, summary_sc = ows.speech_characteristics(json_conf = transcript_json_ua_3, option = 'coherence', language='ua', speaker_label = 'SPEAKER_A') # lang = 'ua'\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_word_pause</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>first_person</th>\n",
       "      <th>verb_tense</th>\n",
       "      <th>word_coherence</th>\n",
       "      <th>word_coherence_5</th>\n",
       "      <th>word_coherence_10</th>\n",
       "      <th>word_coherence_variability_2</th>\n",
       "      <th>word_coherence_variability_3</th>\n",
       "      <th>word_coherence_variability_4</th>\n",
       "      <th>word_coherence_variability_5</th>\n",
       "      <th>word_coherence_variability_6</th>\n",
       "      <th>word_coherence_variability_7</th>\n",
       "      <th>word_coherence_variability_8</th>\n",
       "      <th>word_coherence_variability_9</th>\n",
       "      <th>word_coherence_variability_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>DET</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.437936</td>\n",
       "      <td>0.425541</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.468544</td>\n",
       "      <td>0.456904</td>\n",
       "      <td>0.600032</td>\n",
       "      <td>0.449674</td>\n",
       "      <td>0.459455</td>\n",
       "      <td>0.485383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.619461</td>\n",
       "      <td>0.536148</td>\n",
       "      <td>0.438621</td>\n",
       "      <td>0.457433</td>\n",
       "      <td>0.568730</td>\n",
       "      <td>0.671084</td>\n",
       "      <td>0.576404</td>\n",
       "      <td>0.539414</td>\n",
       "      <td>0.480176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Verb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Past</td>\n",
       "      <td>0.486828</td>\n",
       "      <td>0.563554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.507464</td>\n",
       "      <td>0.603619</td>\n",
       "      <td>0.441070</td>\n",
       "      <td>0.503572</td>\n",
       "      <td>0.424893</td>\n",
       "      <td>0.481575</td>\n",
       "      <td>0.565722</td>\n",
       "      <td>0.498301</td>\n",
       "      <td>0.486386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.385541</td>\n",
       "      <td>0.589166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410622</td>\n",
       "      <td>0.460235</td>\n",
       "      <td>0.559640</td>\n",
       "      <td>0.695474</td>\n",
       "      <td>0.576089</td>\n",
       "      <td>0.566425</td>\n",
       "      <td>0.430694</td>\n",
       "      <td>0.407247</td>\n",
       "      <td>0.523750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530204</td>\n",
       "      <td>0.628629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.571466</td>\n",
       "      <td>0.575412</td>\n",
       "      <td>0.517631</td>\n",
       "      <td>0.592307</td>\n",
       "      <td>0.580790</td>\n",
       "      <td>0.416935</td>\n",
       "      <td>0.440372</td>\n",
       "      <td>0.658272</td>\n",
       "      <td>0.481447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "      <td>Verb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Present</td>\n",
       "      <td>0.518374</td>\n",
       "      <td>0.787403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.64</td>\n",
       "      <td>2</td>\n",
       "      <td>Verb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Present</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>Verb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Present</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Verb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Present</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.24</td>\n",
       "      <td>2</td>\n",
       "      <td>Verb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Present</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pre_word_pause  num_syllables part_of_speech first_person verb_tense  \\\n",
       "0               NaN              1            DET         True        NaN   \n",
       "1              0.00              2           Noun          NaN        NaN   \n",
       "2              0.00              1           Verb          NaN       Past   \n",
       "3              0.00              3           Noun          NaN        NaN   \n",
       "4              0.00              3      Adjective          NaN        NaN   \n",
       "..              ...            ...            ...          ...        ...   \n",
       "117            3.50              2           Verb          NaN    Present   \n",
       "118            0.64              2           Verb          NaN    Present   \n",
       "119            0.54              2           Verb          NaN    Present   \n",
       "120            0.00              2           Verb          NaN    Present   \n",
       "121            1.24              2           Verb          NaN    Present   \n",
       "\n",
       "     word_coherence  word_coherence_5  word_coherence_10  \\\n",
       "0               NaN               NaN                NaN   \n",
       "1          0.518550               NaN                NaN   \n",
       "2          0.486828          0.563554                NaN   \n",
       "3          0.385541          0.589166                NaN   \n",
       "4          0.530204          0.628629                NaN   \n",
       "..              ...               ...                ...   \n",
       "117        0.518374          0.787403                NaN   \n",
       "118        1.000000          0.903675                NaN   \n",
       "119        1.000000          1.000000                NaN   \n",
       "120        1.000000               NaN                NaN   \n",
       "121        1.000000               NaN                NaN   \n",
       "\n",
       "     word_coherence_variability_2  word_coherence_variability_3  \\\n",
       "0                        0.437936                      0.425541   \n",
       "1                        0.619461                      0.536148   \n",
       "2                        0.507464                      0.603619   \n",
       "3                        0.410622                      0.460235   \n",
       "4                        0.571466                      0.575412   \n",
       "..                            ...                           ...   \n",
       "117                      1.000000                      1.000000   \n",
       "118                      1.000000                      1.000000   \n",
       "119                      1.000000                           NaN   \n",
       "120                           NaN                           NaN   \n",
       "121                           NaN                           NaN   \n",
       "\n",
       "     word_coherence_variability_4  word_coherence_variability_5  \\\n",
       "0                        0.496991                      0.468544   \n",
       "1                        0.438621                      0.457433   \n",
       "2                        0.441070                      0.503572   \n",
       "3                        0.559640                      0.695474   \n",
       "4                        0.517631                      0.592307   \n",
       "..                            ...                           ...   \n",
       "117                      1.000000                           NaN   \n",
       "118                           NaN                           NaN   \n",
       "119                           NaN                           NaN   \n",
       "120                           NaN                           NaN   \n",
       "121                           NaN                           NaN   \n",
       "\n",
       "     word_coherence_variability_6  word_coherence_variability_7  \\\n",
       "0                        0.456904                      0.600032   \n",
       "1                        0.568730                      0.671084   \n",
       "2                        0.424893                      0.481575   \n",
       "3                        0.576089                      0.566425   \n",
       "4                        0.580790                      0.416935   \n",
       "..                            ...                           ...   \n",
       "117                           NaN                           NaN   \n",
       "118                           NaN                           NaN   \n",
       "119                           NaN                           NaN   \n",
       "120                           NaN                           NaN   \n",
       "121                           NaN                           NaN   \n",
       "\n",
       "     word_coherence_variability_8  word_coherence_variability_9  \\\n",
       "0                        0.449674                      0.459455   \n",
       "1                        0.576404                      0.539414   \n",
       "2                        0.565722                      0.498301   \n",
       "3                        0.430694                      0.407247   \n",
       "4                        0.440372                      0.658272   \n",
       "..                            ...                           ...   \n",
       "117                           NaN                           NaN   \n",
       "118                           NaN                           NaN   \n",
       "119                           NaN                           NaN   \n",
       "120                           NaN                           NaN   \n",
       "121                           NaN                           NaN   \n",
       "\n",
       "     word_coherence_variability_10  \n",
       "0                         0.485383  \n",
       "1                         0.480176  \n",
       "2                         0.486386  \n",
       "3                         0.523750  \n",
       "4                         0.481447  \n",
       "..                             ...  \n",
       "117                            NaN  \n",
       "118                            NaN  \n",
       "119                            NaN  \n",
       "120                            NaN  \n",
       "121                            NaN  \n",
       "\n",
       "[122 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_turn_pause</th>\n",
       "      <th>turn_length_minutes</th>\n",
       "      <th>turn_length_words</th>\n",
       "      <th>words_per_min</th>\n",
       "      <th>syllables_per_min</th>\n",
       "      <th>speech_percentage</th>\n",
       "      <th>mean_pause_length</th>\n",
       "      <th>pause_variability</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_overall</th>\n",
       "      <th>mattr_5</th>\n",
       "      <th>mattr_10</th>\n",
       "      <th>mattr_25</th>\n",
       "      <th>mattr_50</th>\n",
       "      <th>mattr_100</th>\n",
       "      <th>first_person_percentage</th>\n",
       "      <th>first_person_sentiment_positive</th>\n",
       "      <th>first_person_sentiment_negative</th>\n",
       "      <th>word_repeat_percentage</th>\n",
       "      <th>phrase_repeat_percentage</th>\n",
       "      <th>first_order_sentence_tangeniality</th>\n",
       "      <th>second_order_sentence_tangeniality</th>\n",
       "      <th>turn_to_turn_tangeniality</th>\n",
       "      <th>semantic_perplexity</th>\n",
       "      <th>semantic_perplexity_5</th>\n",
       "      <th>semantic_perplexity_11</th>\n",
       "      <th>semantic_perplexity_15</th>\n",
       "      <th>interrupt_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.069</td>\n",
       "      <td>122</td>\n",
       "      <td>114.125351</td>\n",
       "      <td>213.283442</td>\n",
       "      <td>78.57811</td>\n",
       "      <td>0.113554</td>\n",
       "      <td>0.153734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975862</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.927917</td>\n",
       "      <td>0.881408</td>\n",
       "      <td>0.801905</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.451327</td>\n",
       "      <td>25.925926</td>\n",
       "      <td>0.669445</td>\n",
       "      <td>0.520764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.832659</td>\n",
       "      <td>457.636971</td>\n",
       "      <td>103.513402</td>\n",
       "      <td>91.752403</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pre_turn_pause turn_length_minutes turn_length_words words_per_min  \\\n",
       "0             NaN               1.069               122    114.125351   \n",
       "\n",
       "  syllables_per_min speech_percentage mean_pause_length pause_variability  \\\n",
       "0        213.283442          78.57811          0.113554          0.153734   \n",
       "\n",
       "  sentiment_pos sentiment_neg sentiment_neu sentiment_overall   mattr_5  \\\n",
       "0           0.0           0.0           1.0               0.0  0.975862   \n",
       "\n",
       "   mattr_10  mattr_25  mattr_50 mattr_100  first_person_percentage  \\\n",
       "0  0.966667  0.927917  0.881408  0.801905                 0.595238   \n",
       "\n",
       "   first_person_sentiment_positive  first_person_sentiment_negative  \\\n",
       "0                              0.0                              0.0   \n",
       "\n",
       "  word_repeat_percentage phrase_repeat_percentage  \\\n",
       "0               3.451327                25.925926   \n",
       "\n",
       "   first_order_sentence_tangeniality  second_order_sentence_tangeniality  \\\n",
       "0                           0.669445                            0.520764   \n",
       "\n",
       "   turn_to_turn_tangeniality  semantic_perplexity  semantic_perplexity_5  \\\n",
       "0                        NaN            41.832659             457.636971   \n",
       "\n",
       "   semantic_perplexity_11  semantic_perplexity_15  interrupt_flag  \n",
       "0              103.513402               91.752403           False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns['syllables_per_min'] -> \\/\n",
    "turns['first_person_sentiment_negative'] # ?\n",
    "turns['word_repeat_percentage'] -> \\/\n",
    "turns['phrase_repeat_percentage'] -> \\/\n",
    "turns['first_order_sentence_tangeniality'] -> \\/\n",
    "turns['second_order_sentence_tangeniality'] -> \\/\n",
    "turns['turn_to_turn_tangeniality'] -> ? насколько текст первого спикера связан с вторым. , так как просто была аудиозапись то мы и поставили нан"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/pelmeshek1706/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Инициализируем анализатор один раз, чтобы не грузить модель при каждом вызове\n",
    "_vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text: str, lang: str = 'en') -> dict:\n",
    "    \"\"\"\n",
    "    Рассчитывает sentiment-счётчики для переданного текста и возвращает\n",
    "    только три метрики: positive, negative и neutral.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    text : str\n",
    "        Исходный текст для анализа.\n",
    "    lang : str, optional\n",
    "        Язык текста ('en' или 'uk'/'ua'). Для прочих значений\n",
    "        анализатор всё равно будет использовать английский VADER.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    dict\n",
    "        Словарь вида {'positive': float, 'negative': float, 'neutral': float}.\n",
    "    \"\"\"\n",
    "    # Если потребуется обрабатывать украинский иначе, можно добавить ветвление по lang\n",
    "    scores = _vader.polarity_scores(text)\n",
    "    print(scores)\n",
    "    return {\n",
    "        'positive': scores['pos'],\n",
    "        'negative': scores['neg'],\n",
    "        'neutral' : scores['neu'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positive': 0.0, 'negative': 0.0, 'neutral': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Ми посміхалися, дивлячись на захід сонця.\",\n",
    "    \"Цей жарт викликав у всіх напад сміху.\",\n",
    "    \"Вона обійняла сина після повернення.\",\n",
    "    \"Мене похвалили за старанну роботу.\",\n",
    "    \"Вона відчувала натхнення під час розмови.\",\n",
    "    \"Це рішення дало мені надію.\",\n",
    "    \"Голос у неї, як у сирени, чарівний і дзвінкий.\",\n",
    "    \"Це був прояв невимовної щедрості.\",\n",
    "    \"Він прийняв похвалу з гідністю.\",\n",
    "    \"Ми були захоплені живою вуличною музикою.\",\n",
    "    \"Наш успіх був результатом плідної праці.\",\n",
    "    \"Він відчув полегшення, отримавши довгоочікувану відповідь.\",\n",
    "    \"Цей вибух позитивних емоцій був викликаний її подарунком.\",\n",
    "    \"Мною отримана нагорода за видатні досягнення.\",\n",
    "    \"Щира вдячність читалася у її очах.\",\n",
    "    \"Друзі пригощалися соковитим гранатом.\"\n",
    "]\n",
    "\n",
    "texts_neutral = [\n",
    "    \"Він випив залпом майже весь захололий чай.\",\n",
    "    \"Колона серед музею привертала до себе увагу.\",\n",
    "    \"Ми намагалися підтримувати здоровий режим дня.\",\n",
    "    \"До підготовки випускного залучили усіх учнів старших класів.\",\n",
    "    \"Нам варто враховувати всі можливі нюанси.\",\n",
    "    \"Ця програма працює автоматично після оптимізації.\",\n",
    "    \"Ми вкрилися ковдрами, готуючись до сну.\",\n",
    "    \"Вони мають цілий арсенал ідей для цього проєкту.\",\n",
    "    \"Діти підготували танок до зимового свята.\",\n",
    "    \"База фундаментальних знань була отримана нею в університеті.\",\n",
    "    \"На ці сирники у їдальні стояла черга.\",\n",
    "    \"Вони сіли за стіл.\",\n",
    "    \"Їх команда взяла участь у спортивних змаганнях.\",\n",
    "    \"Його ціллю було отримання вищої освіти.\",\n",
    "    \"Наші сусіди чистили квартиру після ремонту.\",\n",
    "    \"Трава вже виднілася з-під шару снігу.\",\n",
    "    \"Він стояв на стартовій лінії та був готовий до бігу.\"\n",
    "]\n",
    "\n",
    "texts_negative = [\n",
    "    \"Його охопив жах від побаченого.\",\n",
    "    \"Вона дала своїй доньці ляпаса.\",\n",
    "    \"Після покарання він не виходив з кімнати.\",\n",
    "    \"Дівчина завмерла від пронизливого болю.\",\n",
    "    \"Раптовий вибух не давав йому оговтатись.\",\n",
    "    \"Галас вмить здійнявся серед людей.\",\n",
    "    \"Куля пролетіла поруч з моїм вухом.\",\n",
    "    \"Побачений мною будинок зруйнувався за секунди.\",\n",
    "    \"Ми  відчули загрозу у тиші.\",\n",
    "    \"Крик дитини змусив її зупинитися.\",\n",
    "    \"Він зник після останнього завдання.\"\n",
    "]\n",
    "\n",
    "get_sentiment(texts_negative[0], lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Инициализация нового пайплайна\n",
    "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    top_k=None  # возвращаем оценки для всех трёх классов\n",
    ")\n",
    "\n",
    "def hf_sentiment(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Возвращает словарь вида {'neg', 'neu', 'pos', 'compound'}\n",
    "    на основе мультиязычной модели cardiffnlp/twitter-xlm-roberta-base-sentiment.\n",
    "    Поддерживается любой текст — в том числе на английском и украинском.\n",
    "    \"\"\"\n",
    "    # Получаем список оценок для negative/neutral/positive\n",
    "    # sentiment(text) возвращает что-то вроде:\n",
    "    # [[{'label': 'negative', 'score': 0.53}, {'label': 'neutral', 'score': 0.37}, {'label': 'positive', 'score': 0.10}]]\n",
    "    results = sentiment(text)\n",
    "    labels_list = results[0]\n",
    "\n",
    "    # Собираем промежуточный словарь всех трёх метрик\n",
    "    tmp = {entry['label']: entry['score'] for entry in labels_list}\n",
    "\n",
    "    # Вычисляем compound аналогично VADER: pos − neg\n",
    "    compound = abs(tmp.get('positive', 0.0) - tmp.get('negative', 0.0))\n",
    "\n",
    "    # Возвращаем словарь с ключами neg, neu, pos и compound\n",
    "    return {\n",
    "        'neg':      tmp.get('negative', 0.0),\n",
    "        'neu':      tmp.get('neutral',  0.0),\n",
    "        'pos':      tmp.get('positive', 0.0),\n",
    "        'compound': compound\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Його охопив жах від побаченого.\n",
      "HuggingFace: {'neg': 0.8203920722007751, 'neu': 0.12689651548862457, 'pos': 0.05271144211292267, 'compound': 0.7676806300878525}\n",
      "\n",
      "Text: Вона дала своїй доньці ляпаса.\n",
      "HuggingFace: {'neg': 0.32253652811050415, 'neu': 0.5486360192298889, 'pos': 0.12882746756076813, 'compound': 0.19370906054973602}\n",
      "\n",
      "Text: Після покарання він не виходив з кімнати.\n",
      "HuggingFace: {'neg': 0.3906789720058441, 'neu': 0.5288834571838379, 'pos': 0.08043762296438217, 'compound': 0.31024134904146194}\n",
      "\n",
      "Text: Дівчина завмерла від пронизливого болю.\n",
      "HuggingFace: {'neg': 0.8358837962150574, 'neu': 0.12555889785289764, 'pos': 0.03855730593204498, 'compound': 0.7973264902830124}\n",
      "\n",
      "Text: Раптовий вибух не давав йому оговтатись.\n",
      "HuggingFace: {'neg': 0.7047736644744873, 'neu': 0.24816454946994781, 'pos': 0.047061819583177567, 'compound': 0.6577118448913097}\n",
      "\n",
      "Text: Галас вмить здійнявся серед людей.\n",
      "HuggingFace: {'neg': 0.3440284729003906, 'neu': 0.5282660126686096, 'pos': 0.12770548462867737, 'compound': 0.21632298827171326}\n",
      "\n",
      "Text: Куля пролетіла поруч з моїм вухом.\n",
      "HuggingFace: {'neg': 0.1842639446258545, 'neu': 0.4791916310787201, 'pos': 0.33654436469078064, 'compound': 0.15228042006492615}\n",
      "\n",
      "Text: Побачений мною будинок зруйнувався за секунди.\n",
      "HuggingFace: {'neg': 0.9020556807518005, 'neu': 0.078463114798069, 'pos': 0.01948121376335621, 'compound': 0.8825744669884443}\n",
      "\n",
      "Text: Ми  відчули загрозу у тиші.\n",
      "HuggingFace: {'neg': 0.748343288898468, 'neu': 0.20812416076660156, 'pos': 0.043532516807317734, 'compound': 0.7048107720911503}\n",
      "\n",
      "Text: Крик дитини змусив її зупинитися.\n",
      "HuggingFace: {'neg': 0.4893685579299927, 'neu': 0.434793084859848, 'pos': 0.0758383721113205, 'compound': 0.4135301858186722}\n",
      "\n",
      "Text: Він зник після останнього завдання.\n",
      "HuggingFace: {'neg': 0.5501631498336792, 'neu': 0.4110637903213501, 'pos': 0.03877309709787369, 'compound': 0.5113900527358055}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in texts_negative:\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"HuggingFace:\", hf_sentiment(text))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Ми посміхалися, дивлячись на захід сонця.', 'neg': 0.09388092160224915, 'neu': 0.4355852007865906, 'pos': 0.47053396701812744, 'compound': 0.3766530454158783}\n",
      "{'text': 'Цей жарт викликав у всіх напад сміху.', 'neg': 0.09819074720144272, 'neu': 0.18095670640468597, 'pos': 0.7208525538444519, 'compound': 0.6226618066430092}\n",
      "{'text': 'Вона обійняла сина після повернення.', 'neg': 0.5663132667541504, 'neu': 0.38876423239707947, 'pos': 0.044922515749931335, 'compound': 0.521390751004219}\n",
      "{'text': 'Мене похвалили за старанну роботу.', 'neg': 0.050080977380275726, 'neu': 0.11369993537664413, 'pos': 0.836219072341919, 'compound': 0.7861380949616432}\n",
      "{'text': 'Вона відчувала натхнення під час розмови.', 'neg': 0.05657082423567772, 'neu': 0.3774864375591278, 'pos': 0.5659427046775818, 'compound': 0.5093718804419041}\n",
      "{'text': 'Це рішення дало мені надію.', 'neg': 0.05295417457818985, 'neu': 0.1413368284702301, 'pos': 0.8057090044021606, 'compound': 0.7527548298239708}\n",
      "{'text': 'Голос у неї, як у сирени, чарівний і дзвінкий.', 'neg': 0.03551390394568443, 'neu': 0.2540420591831207, 'pos': 0.7104440331459045, 'compound': 0.6749301292002201}\n",
      "{'text': 'Це був прояв невимовної щедрості.', 'neg': 0.06763520836830139, 'neu': 0.09969892352819443, 'pos': 0.8326658606529236, 'compound': 0.7650306522846222}\n",
      "{'text': 'Він прийняв похвалу з гідністю.', 'neg': 0.08540236204862595, 'neu': 0.280328631401062, 'pos': 0.6342689990997314, 'compound': 0.5488666370511055}\n",
      "{'text': 'Ми були захоплені живою вуличною музикою.', 'neg': 0.0957057848572731, 'neu': 0.44258779287338257, 'pos': 0.46170637011528015, 'compound': 0.36600058525800705}\n",
      "{'text': 'Наш успіх був результатом плідної праці.', 'neg': 0.03326338529586792, 'neu': 0.2071879357099533, 'pos': 0.7595487236976624, 'compound': 0.7262853384017944}\n",
      "{'text': 'Він відчув полегшення, отримавши довгоочікувану відповідь.', 'neg': 0.05368032306432724, 'neu': 0.32069286704063416, 'pos': 0.625626802444458, 'compound': 0.5719464793801308}\n",
      "{'text': 'Цей вибух позитивних емоцій був викликаний її подарунком.', 'neg': 0.07040944695472717, 'neu': 0.3744974434375763, 'pos': 0.5550931096076965, 'compound': 0.48468366265296936}\n",
      "{'text': 'Мною отримана нагорода за видатні досягнення.', 'neg': 0.023058123886585236, 'neu': 0.0823260098695755, 'pos': 0.8946158289909363, 'compound': 0.871557705104351}\n",
      "{'text': 'Щира вдячність читалася у її очах.', 'neg': 0.06451552361249924, 'neu': 0.1257922500371933, 'pos': 0.8096922636032104, 'compound': 0.7451767399907112}\n",
      "{'text': 'Друзі пригощалися соковитим гранатом.', 'neg': 0.19188007712364197, 'neu': 0.4764914810657501, 'pos': 0.3316284418106079, 'compound': 0.13974836468696594}\n",
      "{'text': 'Він випив залпом майже весь захололий чай.', 'neg': 0.5307207107543945, 'neu': 0.37262558937072754, 'pos': 0.09665364772081375, 'compound': 0.4340670630335808}\n",
      "{'text': 'Колона серед музею привертала до себе увагу.', 'neg': 0.11034039407968521, 'neu': 0.3198225796222687, 'pos': 0.5698369741439819, 'compound': 0.4594965800642967}\n",
      "{'text': 'Ми намагалися підтримувати здоровий режим дня.', 'neg': 0.08898124098777771, 'neu': 0.456662654876709, 'pos': 0.4543560743331909, 'compound': 0.3653748333454132}\n",
      "{'text': 'До підготовки випускного залучили усіх учнів старших класів.', 'neg': 0.26688092947006226, 'neu': 0.6597327589988708, 'pos': 0.0733863115310669, 'compound': 0.19349461793899536}\n",
      "{'text': 'Нам варто враховувати всі можливі нюанси.', 'neg': 0.13475698232650757, 'neu': 0.6613204479217529, 'pos': 0.2039225697517395, 'compound': 0.06916558742523193}\n",
      "{'text': 'Ця програма працює автоматично після оптимізації.', 'neg': 0.027812503278255463, 'neu': 0.684842050075531, 'pos': 0.2873455286026001, 'compound': 0.25953302532434464}\n",
      "{'text': 'Ми вкрилися ковдрами, готуючись до сну.', 'neg': 0.3377595543861389, 'neu': 0.5035175085067749, 'pos': 0.15872295200824738, 'compound': 0.17903660237789154}\n",
      "{'text': 'Вони мають цілий арсенал ідей для цього проєкту.', 'neg': 0.11112281680107117, 'neu': 0.4732038676738739, 'pos': 0.4156733751296997, 'compound': 0.30455055832862854}\n",
      "{'text': 'Діти підготували танок до зимового свята.', 'neg': 0.033773161470890045, 'neu': 0.5326058864593506, 'pos': 0.43362095952033997, 'compound': 0.3998477980494499}\n",
      "{'text': 'База фундаментальних знань була отримана нею в університеті.', 'neg': 0.06759350001811981, 'neu': 0.7963716387748718, 'pos': 0.13603489100933075, 'compound': 0.06844139099121094}\n",
      "{'text': 'На ці сирники у їдальні стояла черга.', 'neg': 0.4916437864303589, 'neu': 0.4246642291545868, 'pos': 0.08369205892086029, 'compound': 0.4079517275094986}\n",
      "{'text': 'Вони сіли за стіл.', 'neg': 0.19479988515377045, 'neu': 0.7252053022384644, 'pos': 0.07999477535486221, 'compound': 0.11480510979890823}\n",
      "{'text': 'Їх команда взяла участь у спортивних змаганнях.', 'neg': 0.026645375415682793, 'neu': 0.6916965246200562, 'pos': 0.2816580533981323, 'compound': 0.25501267798244953}\n",
      "{'text': 'Його ціллю було отримання вищої освіти.', 'neg': 0.10253303498029709, 'neu': 0.6966164708137512, 'pos': 0.2008504718542099, 'compound': 0.09831743687391281}\n",
      "{'text': 'Наші сусіди чистили квартиру після ремонту.', 'neg': 0.043114639818668365, 'neu': 0.7297301292419434, 'pos': 0.22715528309345245, 'compound': 0.1840406432747841}\n",
      "{'text': 'Трава вже виднілася з-під шару снігу.', 'neg': 0.33070945739746094, 'neu': 0.5379816293716431, 'pos': 0.1313089281320572, 'compound': 0.19940052926540375}\n",
      "{'text': 'Він стояв на стартовій лінії та був готовий до бігу.', 'neg': 0.08332641422748566, 'neu': 0.776134192943573, 'pos': 0.14053936302661896, 'compound': 0.0572129487991333}\n",
      "{'text': 'Його охопив жах від побаченого.', 'neg': 0.8203920722007751, 'neu': 0.12689651548862457, 'pos': 0.05271144211292267, 'compound': 0.7676806300878525}\n",
      "{'text': 'Вона дала своїй доньці ляпаса.', 'neg': 0.32253652811050415, 'neu': 0.5486360192298889, 'pos': 0.12882746756076813, 'compound': 0.19370906054973602}\n",
      "{'text': 'Після покарання він не виходив з кімнати.', 'neg': 0.3906789720058441, 'neu': 0.5288834571838379, 'pos': 0.08043762296438217, 'compound': 0.31024134904146194}\n",
      "{'text': 'Дівчина завмерла від пронизливого болю.', 'neg': 0.8358837962150574, 'neu': 0.12555889785289764, 'pos': 0.03855730593204498, 'compound': 0.7973264902830124}\n",
      "{'text': 'Раптовий вибух не давав йому оговтатись.', 'neg': 0.7047736644744873, 'neu': 0.24816454946994781, 'pos': 0.047061819583177567, 'compound': 0.6577118448913097}\n",
      "{'text': 'Галас вмить здійнявся серед людей.', 'neg': 0.3440284729003906, 'neu': 0.5282660126686096, 'pos': 0.12770548462867737, 'compound': 0.21632298827171326}\n",
      "{'text': 'Куля пролетіла поруч з моїм вухом.', 'neg': 0.1842639446258545, 'neu': 0.4791916310787201, 'pos': 0.33654436469078064, 'compound': 0.15228042006492615}\n",
      "{'text': 'Побачений мною будинок зруйнувався за секунди.', 'neg': 0.9020556807518005, 'neu': 0.078463114798069, 'pos': 0.01948121376335621, 'compound': 0.8825744669884443}\n",
      "{'text': 'Ми  відчули загрозу у тиші.', 'neg': 0.748343288898468, 'neu': 0.20812416076660156, 'pos': 0.043532516807317734, 'compound': 0.7048107720911503}\n",
      "{'text': 'Крик дитини змусив її зупинитися.', 'neg': 0.4893685579299927, 'neu': 0.434793084859848, 'pos': 0.0758383721113205, 'compound': 0.4135301858186722}\n",
      "{'text': 'Він зник після останнього завдання.', 'neg': 0.5501631498336792, 'neu': 0.4110637903213501, 'pos': 0.03877309709787369, 'compound': 0.5113900527358055}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for text in texts+texts_neutral+texts_negative:\n",
    "    scores = hf_sentiment(text)\n",
    "    # создаём одну запись с полями: text, neg, neu, pos, compound\n",
    "    row = {'text': text, **scores}\n",
    "    # print(row)\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text       neg       neu  \\\n",
      "0           Ми посміхалися, дивлячись на захід сонця.  0.093881  0.435585   \n",
      "1               Цей жарт викликав у всіх напад сміху.  0.098191  0.180957   \n",
      "2                Вона обійняла сина після повернення.  0.566313  0.388764   \n",
      "3                  Мене похвалили за старанну роботу.  0.050081  0.113700   \n",
      "4           Вона відчувала натхнення під час розмови.  0.056571  0.377486   \n",
      "5                         Це рішення дало мені надію.  0.052954  0.141337   \n",
      "6      Голос у неї, як у сирени, чарівний і дзвінкий.  0.035514  0.254042   \n",
      "7                   Це був прояв невимовної щедрості.  0.067635  0.099699   \n",
      "8                     Він прийняв похвалу з гідністю.  0.085402  0.280329   \n",
      "9           Ми були захоплені живою вуличною музикою.  0.095706  0.442588   \n",
      "10           Наш успіх був результатом плідної праці.  0.033263  0.207188   \n",
      "11  Він відчув полегшення, отримавши довгоочікуван...  0.053680  0.320693   \n",
      "12  Цей вибух позитивних емоцій був викликаний її ...  0.070409  0.374497   \n",
      "13      Мною отримана нагорода за видатні досягнення.  0.023058  0.082326   \n",
      "14                 Щира вдячність читалася у її очах.  0.064516  0.125792   \n",
      "15              Друзі пригощалися соковитим гранатом.  0.191880  0.476491   \n",
      "16         Він випив залпом майже весь захололий чай.  0.530721  0.372626   \n",
      "17       Колона серед музею привертала до себе увагу.  0.110340  0.319823   \n",
      "18     Ми намагалися підтримувати здоровий режим дня.  0.088981  0.456663   \n",
      "19  До підготовки випускного залучили усіх учнів с...  0.266881  0.659733   \n",
      "20          Нам варто враховувати всі можливі нюанси.  0.134757  0.661320   \n",
      "21  Ця програма працює автоматично після оптимізації.  0.027813  0.684842   \n",
      "22            Ми вкрилися ковдрами, готуючись до сну.  0.337760  0.503518   \n",
      "23   Вони мають цілий арсенал ідей для цього проєкту.  0.111123  0.473204   \n",
      "24          Діти підготували танок до зимового свята.  0.033773  0.532606   \n",
      "25  База фундаментальних знань була отримана нею в...  0.067594  0.796372   \n",
      "26              На ці сирники у їдальні стояла черга.  0.491644  0.424664   \n",
      "27                                 Вони сіли за стіл.  0.194800  0.725205   \n",
      "28    Їх команда взяла участь у спортивних змаганнях.  0.026645  0.691697   \n",
      "29            Його ціллю було отримання вищої освіти.  0.102533  0.696616   \n",
      "30        Наші сусіди чистили квартиру після ремонту.  0.043115  0.729730   \n",
      "31              Трава вже виднілася з-під шару снігу.  0.330709  0.537982   \n",
      "32  Він стояв на стартовій лінії та був готовий до...  0.083326  0.776134   \n",
      "33                    Його охопив жах від побаченого.  0.820392  0.126897   \n",
      "34                     Вона дала своїй доньці ляпаса.  0.322537  0.548636   \n",
      "35          Після покарання він не виходив з кімнати.  0.390679  0.528883   \n",
      "36            Дівчина завмерла від пронизливого болю.  0.835884  0.125559   \n",
      "37           Раптовий вибух не давав йому оговтатись.  0.704774  0.248165   \n",
      "38                 Галас вмить здійнявся серед людей.  0.344028  0.528266   \n",
      "39                 Куля пролетіла поруч з моїм вухом.  0.184264  0.479192   \n",
      "40     Побачений мною будинок зруйнувався за секунди.  0.902056  0.078463   \n",
      "41                        Ми  відчули загрозу у тиші.  0.748343  0.208124   \n",
      "42                  Крик дитини змусив її зупинитися.  0.489369  0.434793   \n",
      "43                Він зник після останнього завдання.  0.550163  0.411064   \n",
      "\n",
      "         pos  compound  \n",
      "0   0.470534  0.376653  \n",
      "1   0.720853  0.622662  \n",
      "2   0.044923  0.521391  \n",
      "3   0.836219  0.786138  \n",
      "4   0.565943  0.509372  \n",
      "5   0.805709  0.752755  \n",
      "6   0.710444  0.674930  \n",
      "7   0.832666  0.765031  \n",
      "8   0.634269  0.548867  \n",
      "9   0.461706  0.366001  \n",
      "10  0.759549  0.726285  \n",
      "11  0.625627  0.571946  \n",
      "12  0.555093  0.484684  \n",
      "13  0.894616  0.871558  \n",
      "14  0.809692  0.745177  \n",
      "15  0.331628  0.139748  \n",
      "16  0.096654  0.434067  \n",
      "17  0.569837  0.459497  \n",
      "18  0.454356  0.365375  \n",
      "19  0.073386  0.193495  \n",
      "20  0.203923  0.069166  \n",
      "21  0.287346  0.259533  \n",
      "22  0.158723  0.179037  \n",
      "23  0.415673  0.304551  \n",
      "24  0.433621  0.399848  \n",
      "25  0.136035  0.068441  \n",
      "26  0.083692  0.407952  \n",
      "27  0.079995  0.114805  \n",
      "28  0.281658  0.255013  \n",
      "29  0.200850  0.098317  \n",
      "30  0.227155  0.184041  \n",
      "31  0.131309  0.199401  \n",
      "32  0.140539  0.057213  \n",
      "33  0.052711  0.767681  \n",
      "34  0.128827  0.193709  \n",
      "35  0.080438  0.310241  \n",
      "36  0.038557  0.797326  \n",
      "37  0.047062  0.657712  \n",
      "38  0.127705  0.216323  \n",
      "39  0.336544  0.152280  \n",
      "40  0.019481  0.882574  \n",
      "41  0.043533  0.704811  \n",
      "42  0.075838  0.413530  \n",
      "43  0.038773  0.511390  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Создаём DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Выводим результат\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.8203920722007751,\n",
       " 'neu': 0.12689651548862457,\n",
       " 'pos': 0.05271144211292267,\n",
       " 'compound': -0.7676806300878525}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_sentiment(texts_negative[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.5307207107543945,\n",
       " 'neu': 0.37262558937072754,\n",
       " 'pos': 0.09665364772081375,\n",
       " 'compound': -0.4340670630335808}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_sentiment(texts_neutral[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "words = pd.read_csv(\"/Users/pelmeshek1706/Desktop/projects/final_airest_voice/words_eney.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          NaN\n",
       "1          NaN\n",
       "2         Past\n",
       "3          NaN\n",
       "4          NaN\n",
       "        ...   \n",
       "117    Present\n",
       "118    Present\n",
       "119    Present\n",
       "120    Present\n",
       "121    Present\n",
       "Name: verb_tense, Length: 122, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['verb_tense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pandas in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Requirement already satisfied: packaging in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2024.12.0)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-macosx_12_0_arm64.whl (30.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.26.3)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "Successfully installed datasets-4.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-20.0.0 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosmus_eval_major_label.py\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------------------------------------------------- #\n",
    "# 1.  Load the COSMUS dataset (Telegram RU/UA posts)                   #\n",
    "# -------------------------------------------------------------------- #\n",
    "ds = load_dataset(\"YShynkarov/COSMUS\", split=\"train\")        # ~2.6 MB  [oai_citation:0‡huggingface.co](https://huggingface.co/datasets/YShynkarov/COSMUS/tree/main?utm_source=chatgpt.com) [oai_citation:1‡huggingface.co](https://huggingface.co/datasets/YShynkarov/COSMUS/viewer?utm_source=chatgpt.com)\n",
    "df = ds.to_pandas()[[\"document_content\", \"annotator_sentiment\", \"gpt_labels_v1\", \"language\"]]\n",
    "\n",
    "# Gold labels → integers\n",
    "label2id = {\"negative\": -1.0, \"neutral\": 0.0, \"positive\": 1.0}\n",
    "df[\"annotator_sentiment\"] = df[\"annotator_sentiment\"].map(label2id)\n",
    "df[\"gpt_labels_v1\"] = df[\"gpt_labels_v1\"].map(label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 2.  Sentiment model wrapper with major_label()                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Multilingual twitter-XLM-RoBERTa sentiment wrapper.\n",
    "    Provides polarity_scores() *and* major_label().\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        mdl = \"tabularisai/multilingual-sentiment-analysis\"#\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"       # 3-way (neg/neu/pos)  [oai_citation:2‡huggingface.co](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment?utm_source=chatgpt.com)\n",
    "        self._pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=mdl,\n",
    "            tokenizer=mdl,\n",
    "            truncation=True,        # <- key line: clamp to 514 tokens\n",
    "            padding=True,           # keeps batch shapes consistent\n",
    "            max_length=512,         # optional but explicit\n",
    "            top_k=3\n",
    "        )\n",
    "\n",
    "    # ---------- already supplied ----------\n",
    "    # def polarity_scores(self, text: str):\n",
    "    #     res   = self._pipe(text)              # list[list[dict(label,score)]]\n",
    "    #     scores = {d[\"label\"]: d[\"score\"] for d in res[0]}\n",
    "    #     compound = abs(scores.get(\"positive\", 0.0) - scores.get(\"negative\", 0.0))\n",
    "    #     return {\"neg\": scores.get(\"negative\", 0.0),\n",
    "    #             \"neu\": scores.get(\"neutral\",  0.0),\n",
    "    #             \"pos\": scores.get(\"positive\", 0.0),\n",
    "    #             \"compound\": compound}\n",
    "\n",
    "    # # ---------- new method ----------\n",
    "    # def major_label(self, text: str):\n",
    "    #     \"\"\"\n",
    "    #     Returns (text_label, int_label) where int_label ∈ {−1,0,1}.\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         sc   = self.polarity_scores(text)\n",
    "    #         best = max((\"neg\", \"neu\", \"pos\"), key=sc.get)              # pick highest  [oai_citation:5‡stackoverflow.com](https://stackoverflow.com/questions/70916877/cant-seem-to-iterate-over-a-column-to-assign-sentiment-values?utm_source=chatgpt.com)\n",
    "    #         text_label = {\"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"}[best]\n",
    "    #         return text_label, {\"negative\": -1.0, \"neutral\": 0.0, \"positive\": 1.0}[text_label]\n",
    "    #     except Exception as e: \n",
    "    #         print(\"Error in major_label() for text:\", text)\n",
    "    #         print(\"Exception:\", e)\n",
    "    def polarity_scores(self, text: str):\n",
    "        \"\"\"\n",
    "        Вызывает self._pipe(text), получает 5 меток:\n",
    "          Very Positive, Positive, Neutral, Very Negative, Negative\n",
    "        Группирует их в три категории:\n",
    "          - pos (Very Positive + Positive)\n",
    "          - neu (Neutral)\n",
    "          - neg (Very Negative + Negative)\n",
    "        Возвращает словарь с тремя суммарными скорорами и compound = |pos - neg|.\n",
    "        \"\"\"\n",
    "        # получаем список словарей [{'label':..., 'score':...}, ...]\n",
    "        raw = self._pipe(text)[0]\n",
    "        # print(\"raw:\", raw) \n",
    "        # отображение исходных меток на группы\n",
    "        label_map = {\n",
    "            \"Very Positive\": \"pos\",\n",
    "            \"Positive\":      \"pos\",\n",
    "            \"Neutral\":       \"neu\",\n",
    "            \"Very Negative\": \"neg\",\n",
    "            \"Negative\":      \"neg\"\n",
    "        }\n",
    "\n",
    "        # инициализируем суммы\n",
    "        agg = {\"pos\": 0.0, \"neu\": 0.0, \"neg\": 0.0}\n",
    "\n",
    "        # суммируем по группам\n",
    "        for entry in raw:\n",
    "            grp = label_map.get(entry[\"label\"])\n",
    "            if grp:\n",
    "                agg[grp] += entry[\"score\"]\n",
    "\n",
    "        # вычисляем compound\n",
    "        compound = abs(agg[\"pos\"] - agg[\"neg\"])\n",
    "\n",
    "        return {\n",
    "            \"neg\":      agg[\"neg\"],\n",
    "            \"neu\":      agg[\"neu\"],\n",
    "            \"pos\":      agg[\"pos\"],\n",
    "            \"compound\": compound\n",
    "        }\n",
    "\n",
    "    def major_label(self, text: str):\n",
    "        \"\"\"\n",
    "        Без изменений: берёт из polarity_scores наибольшую из трёх категорий\n",
    "        и возвращает (текстовую метку, числовую метку).\n",
    "        \"\"\"\n",
    "        sc = self.polarity_scores(text)\n",
    "        best = max((\"neg\", \"neu\", \"pos\"), key=lambda k: sc[k])\n",
    "        text_label = {\"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"}[best]\n",
    "        int_label  = {\"negative\": -1.0, \"neutral\": 0.0, \"positive\": 1.0}[text_label]\n",
    "        return text_label, int_label\n",
    "\n",
    "analyzer = SentimentAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'Я ненавижу эту погоду, она ужасная!'\n",
    "test_text = 'ты сделала мне очень приятно, я не ожидал такого'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('positive', 1.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.major_label(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Positive', 'score': 0.5239871740341187},\n",
       "  {'label': 'Very Positive', 'score': 0.40226301550865173},\n",
       "  {'label': 'Neutral', 'score': 0.041605394333601}]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer._pipe(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_content       ⚡️Українська делегація відправилася на перемов...\n",
       "annotator_sentiment                                                  0.0\n",
       "language                                                              ua\n",
       "sentiment_pred                                                       0.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: [{'label': 'Very Negative', 'score': 0.7365058064460754}, {'label': 'Negative', 'score': 0.12379347532987595}, {'label': 'Very Positive', 'score': 0.049464911222457886}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('negative', -1.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"#UA - Полонне LIFE https://t.me  |05/10/23|09:34CET| - - - - - - - Запустили послугу єВідновлення в застосунку Дія — подавайте заяву, щоб отримати кошти на ремонт пошкодженого житла. Світовий банк виділяє кошти, щоб українці змогли якомога скоріше відновити власні домівки, пошкоджені через повномасштабне вторгнення. На першому етапі послуги єВідновлення допомогу отримають люди, чиї будинки потребують косметичного ремонту. Наприклад, ваше житло придатне для життя, але треба замінити вікна, двері, відремонтувати дах тощо. Пріоритет мають учасники бойових дій, мобілізовані, сім’ї загиблих військових, люди з інвалідністю І та ІІ груп і багатодітні родини. Максимальна сума на відновлення одного об’єкта — 200 тисяч гривень. Далі масштабуємо програму. Щоб допомогу отримали люди, будинки яких були повністю знищені внаслідок російської агресії. Послуга в Дії працює за принципом, схожим до єПідтримки. Подаєте заяву на виплату за кілька кліків та відкриваєте спеціальну картку єВідновлення в одному з 5 банків-партнерів. Саме на цю картку потім надійдуть кошти, які витратите на будматеріали й ремонтні послуги. Суму компенсації визначить спеціальна комісія, яку скерує місцева влада. Комісія зафіксує ступінь руйнувань та зробить висновки. Рішення комісії отримаєте в Дії. Як подати заяву, які критерії, де відкрити картку та інше — зібрали для вас на зручному сайті. Послугу розробили разом з Міністерством розвитку громад, територій та інфраструктури за підтримки проєкту USAID / UK aid «Прозорість та підзвітність у державному управлінні та послугах/ TAPAS» та Світового банку. Полонне LIFE #ПроГоловне Запустили послугу єВідновлення в застосунку Дія — подавайте заяву, щоб отримати кошти на ремонт пошкодженого житла.  Світовий банк виділяє кошти, щоб українці змогли якомога скоріше відновити власні домівки, пошкоджені через повномасштабне вторгнення.   На першому етапі послуги єВідновлення допомогу отримають люди, чиї будинки потребують косметичного ремонту. Наприклад, ваше житло придатне для життя, але треба замінити вікна, двері, відремонтувати дах тощо. Пріоритет мають учасники бойових дій, мобілізовані, сім’ї загиблих військових, люди з інвалідністю І та ІІ груп і багатодітні родини. Максимальна сума на відновлення одного об’єкта — 200 тисяч гривень.   Далі масштабуємо програму. Щоб допомогу отримали люди, будинки яких були повністю знищені внаслідок російської агресії.   Послуга в Дії працює за принципом, схожим до єПідтримки. Подаєте заяву на виплату за кілька кліків та відкриваєте спеціальну картку єВідновлення в одному з 5 банків-партнерів. Саме на цю картку потім надійдуть кошти, які витратите на будматеріали й ремонтні послуги.   Суму компенсації визначить спеціальна комісія, яку скерує місцева влада. Комісія зафіксує ступінь руйнувань та зробить висновки. Рішення комісії отримаєте в Дії.   Як подати заяву, які критерії, де відкрити картку та інше — зібрали для вас на зручному сайті.  Послугу розробили разом з Міністерством розвитку громад, територій та інфраструктури за підтримки проєкту USAID / UK aid «Прозорість та підзвітність у державному управлінні та послугах/ TAPAS» та Світового банку. https://t.me\"\n",
    "analyzer.major_label(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 3.  Inference → sentiment_pred column                                #\n",
    "# -------------------------------------------------------------------- #\n",
    "from tqdm import tqdm      # или просто `from tqdm import tqdm`\n",
    "tqdm.pandas()\n",
    "\n",
    "df[\"sentiment_pred\"] = df[\"document_content\"].apply(\n",
    "    lambda txt: analyzer.major_label(txt)[1]     # keep numeric only\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2       -1.0\n",
       "3       -1.0\n",
       "4        0.0\n",
       "        ... \n",
       "12219   -1.0\n",
       "12220    NaN\n",
       "12221    0.0\n",
       "12222   -1.0\n",
       "12223   -1.0\n",
       "Name: annotator_sentiment, Length: 12224, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"annotator_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>gpt_labels_v1</th>\n",
       "      <th>language</th>\n",
       "      <th>sentiment_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12218</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12222</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12223</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>mixed</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11090 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content  annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...                  0.0   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.                  0.0   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...                 -1.0   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...                 -1.0   \n",
       "4                                            Добрий день                  0.0   \n",
       "...                                                  ...                  ...   \n",
       "12218  У меня три окна и двери выбило , даже и не дум...                 -1.0   \n",
       "12219  Краще \"повинна бути зручнішою, ніж Uber чи Boo...                 -1.0   \n",
       "12221  Питання, цей сертифікат можна вже використовув...                  0.0   \n",
       "12222  На Вугледарському напрямку загинув Рома Іванен...                 -1.0   \n",
       "12223  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...                 -1.0   \n",
       "\n",
       "       gpt_labels_v1 language  sentiment_pred  \n",
       "0                0.0       ua             0.0  \n",
       "1               -1.0       ua             0.0  \n",
       "2               -1.0       ru            -1.0  \n",
       "3               -1.0       ru             0.0  \n",
       "4                0.0       ua             1.0  \n",
       "...              ...      ...             ...  \n",
       "12218           -1.0       ru            -1.0  \n",
       "12219            1.0       ua             1.0  \n",
       "12221            0.0       ua            -1.0  \n",
       "12222           -1.0       ua            -1.0  \n",
       "12223           -1.0    mixed            -1.0  \n",
       "\n",
       "[11090 rows x 5 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(subset=[\"annotator_sentiment\", 'gpt_labels_v1'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.639%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (−1)       0.51      0.69      0.59      4301\n",
      "  neutral (0)       0.59      0.24      0.34      4502\n",
      "positive (+1)       0.42      0.64      0.51      2287\n",
      "\n",
      "     accuracy                           0.50     11090\n",
      "    macro avg       0.51      0.52      0.48     11090\n",
      " weighted avg       0.52      0.50      0.47     11090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"sentiment_pred\"])     #  [oai_citation:6‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?utm_source=chatgpt.com)\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(                                              #  [oai_citation:7‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?utm_source=chatgpt.com)\n",
    "      df[\"annotator_sentiment\"], df[\"sentiment_pred\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy gpt: 55.059%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (−1)       0.63      0.67      0.65      5416\n",
      "  neutral (0)       0.38      0.26      0.30      2668\n",
      "positive (+1)       0.51      0.59      0.55      3006\n",
      "\n",
      "     accuracy                           0.55     11090\n",
      "    macro avg       0.51      0.51      0.50     11090\n",
      " weighted avg       0.54      0.55      0.54     11090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(df[\"gpt_labels_v1\"], df[\"sentiment_pred\"])     #  [oai_citation:6‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?utm_source=chatgpt.com)\n",
    "print(f\"Accuracy gpt: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(                                              #  [oai_citation:7‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?utm_source=chatgpt.com)\n",
    "      df[\"gpt_labels_v1\"], df[\"sentiment_pred\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Positive', 'score': 0.5239871740341187},\n",
       "  {'label': 'Very Positive', 'score': 0.40226301550865173},\n",
       "  {'label': 'Neutral', 'score': 0.041605394333601}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzer._pipe(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.20 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.0.0)\n",
      "Collecting accelerate>=0.29\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers==4.45.2) (1.23.5)\n",
      "Requirement already satisfied: xxhash in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets>=2.20) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets>=2.20) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets>=2.20) (2024.12.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets>=2.20) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets>=2.20) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets>=2.20) (2.2.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from accelerate>=0.29) (2.5.1)\n",
      "Requirement already satisfied: psutil in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from accelerate>=0.29) (6.1.1)\n",
      "Requirement already satisfied: scipy in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.4.1)\n",
      "Requirement already satisfied: networkx in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.29) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.29) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.29) (3.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.29) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets>=2.20) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets>=2.20) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets>=2.20) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (2.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (0.2.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (1.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (1.18.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (24.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20) (6.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.29) (3.0.2)\n",
      "Installing collected packages: bitsandbytes, accelerate, transformers, peft, evaluate\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openwillis-speech 1.0.4 requires transformers==4.46.3, but you have transformers 4.45.2 which is incompatible.\n",
      "openwillis-transcribe 1.0.4 requires transformers==4.46.3, but you have transformers 4.45.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.8.1 bitsandbytes-0.42.0 evaluate-0.4.5 peft-0.16.0 transformers-4.45.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install \"transformers==4.45.2\" \"datasets>=2.20\" \"accelerate>=0.29\" \\\n",
    "            \"evaluate\" \"peft\" bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 1163/1163 [00:00<00:00, 10017.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 3-class order fixed to match the pre-trained head\n",
    "label_list = [\"negative\", \"neutral\", \"positive\"]\n",
    "id2label   = {i: l for i, l in enumerate(label_list)}     # int ➜ str\n",
    "label2id   = {l: i for i, l in enumerate(label_list)}     # str ➜ int\n",
    "\n",
    "def encode_label(example):\n",
    "    example[\"labels\"] = label2id[example[\"annotator_sentiment\"]]   # str ➜ int\n",
    "    return example\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "DATASET    = \"YShynkarov/COSMUS\"\n",
    "\n",
    "# 1) load & drop the extra 'mixed' rows\n",
    "ds = load_dataset(DATASET, split=\"train\")\n",
    "ds = ds.filter(lambda x: x[\"annotator_sentiment\"] != \"mixed\") \n",
    "\n",
    "# 2) honour author's original split column if present\n",
    "if \"df_set\" in ds.column_names:                        \n",
    "    splits = DatasetDict({\n",
    "        k: ds.filter(lambda x, k=k: x[\"df_set\"] == k)\n",
    "        for k in (\"train\", \"validation\", \"test\")\n",
    "    })\n",
    "else:                                                                 # fallback 80-10-10\n",
    "    tmp       = ds.train_test_split(test_size=0.2, seed=42)\n",
    "    splits    = DatasetDict({\n",
    "        \"train\": tmp[\"train\"],\n",
    "        \"validation\": tmp[\"test\"].train_test_split(\n",
    "            test_size=0.5, seed=42)[\"train\"],\n",
    "        \"test\": tmp[\"test\"].train_test_split(\n",
    "            test_size=0.5, seed=42)[\"test\"],\n",
    "    })\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(batch[\"document_content\"],\n",
    "               truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# 3) ***encode labels FIRST, then tokenize***\n",
    "splits_tok = splits.map(encode_label)\n",
    "splits_tok = splits_tok.map(\n",
    "    tokenize, batched=True,\n",
    "    remove_columns=[c for c in ds.column_names\n",
    "                    if c not in {\"document_content\", \"annotator_sentiment\"}])\n",
    "\n",
    "# Column is already called \"labels\" after encode_label; no rename needed\n",
    "data_collator = DataCollatorWithPadding(tok, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "import evaluate, numpy as np\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc  = evaluate.load(\"accuracy\").compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1   = evaluate.load(\"f1\").compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./xlmr-cosmus-1epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",               # ← matches eval to satisfy load_best_model_at_end\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=splits_tok[\"train\"],\n",
    "    eval_dataset=splits_tok[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 500/581 [11:12<01:50,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6493, 'grad_norm': 13.629276275634766, 'learning_rate': 2.7882960413080896e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 581/581 [13:01<00:00,  1.36s/it]\n",
      "Downloading builder script: 4.20kB [00:00, 3.56MB/s]\n",
      "\n",
      "Downloading builder script: 6.79kB [00:00, 3.40MB/s]\n",
      "                                                 \n",
      "100%|██████████| 581/581 [13:41<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5479645133018494, 'eval_accuracy': 0.7553832902670112, 'eval_f1': 0.7486548653768322, 'eval_runtime': 33.9328, 'eval_samples_per_second': 34.215, 'eval_steps_per_second': 4.303, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 581/581 [13:46<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 826.9257, 'train_samples_per_second': 11.237, 'train_steps_per_second': 0.703, 'train_loss': 0.6397317567913955, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=581, training_loss=0.6397317567913955, metrics={'train_runtime': 826.9257, 'train_samples_per_second': 11.237, 'train_steps_per_second': 0.703, 'total_flos': 1222424938764288.0, 'train_loss': 0.6397317567913955, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:29<00:00,  4.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5555512309074402,\n",
       " 'eval_accuracy': 0.766981943250215,\n",
       " 'eval_f1': 0.7538136862410986,\n",
       " 'eval_runtime': 29.9605,\n",
       " 'eval_samples_per_second': 38.818,\n",
       " 'eval_steps_per_second': 4.873,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(splits_tok[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./xlmr-cosmus-1epoch/tokenizer_config.json',\n",
       " './xlmr-cosmus-1epoch/special_tokens_map.json',\n",
       " './xlmr-cosmus-1epoch/sentencepiece.bpe.model',\n",
       " './xlmr-cosmus-1epoch/added_tokens.json',\n",
       " './xlmr-cosmus-1epoch/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./xlmr-cosmus-1epoch\")      # model, config & tokenizer\n",
    "tok.save_pretrained(\"./xlmr-cosmus-1epoch\")     # make sure tokenizer is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.7447900772094727}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"./xlmr-cosmus-1epoch\",            # or \"./xlmr-cosmus-lora-merged\"\n",
    "    tokenizer=\"./xlmr-cosmus-1epoch\",\n",
    "    device=0                               # GPU; omit for CPU\n",
    ")\n",
    "print(pipe(\"Це просто неймовірно!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.26.3)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Installing collected packages: hf-xet, huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.26.3\n",
      "    Uninstalling huggingface-hub-0.26.3:\n",
      "      Successfully uninstalled huggingface-hub-0.26.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openwillis-speech 1.0.4 requires huggingface-hub==0.26.3, but you have huggingface-hub 0.33.4 which is incompatible.\n",
      "openwillis-speech 1.0.4 requires transformers==4.46.3, but you have transformers 4.45.2 which is incompatible.\n",
      "openwillis-transcribe 1.0.4 requires huggingface-hub==0.26.3, but you have huggingface-hub 0.33.4 which is incompatible.\n",
      "openwillis-transcribe 1.0.4 requires transformers==4.46.3, but you have transformers 4.45.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed hf-xet-1.1.5 huggingface_hub-0.33.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub   # if not installed                 # paste hf_xxx when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llama_wr` has been saved to /Users/pelmeshek1706/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/pelmeshek1706/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llama_wr`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=hf_<your_token_here>  # paste your token here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi; api = HfApi();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 2.  Sentiment model wrapper with major_label()                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Multilingual twitter-XLM-RoBERTa sentiment wrapper.\n",
    "    Provides polarity_scores() *and* major_label().\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        mdl = \"Pelmeshek/xlmr-cosmus-sentiment\"#\"./xlmr-cosmus-1epoch\"#\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"       # 3-way (neg/neu/pos)  [oai_citation:2‡huggingface.co](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment?utm_source=chatgpt.com)\n",
    "        self._pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=mdl,\n",
    "            tokenizer=mdl,\n",
    "            truncation=True,        # <- key line: clamp to 514 tokens\n",
    "            padding=True,           # keeps batch shapes consistent\n",
    "            max_length=512,         # optional but explicit\n",
    "            top_k=3\n",
    "        )\n",
    "\n",
    "    # ---------- already supplied ----------\n",
    "    def polarity_scores_based(self, text: str):\n",
    "        res   = self._pipe(text)              # list[list[dict(label,score)]]\n",
    "        scores = {d[\"label\"]: d[\"score\"] for d in res[0]}\n",
    "        compound = abs(scores.get(\"positive\", 0.0) - scores.get(\"negative\", 0.0))\n",
    "        return {\"neg\": scores.get(\"negative\", 0.0),\n",
    "                \"neu\": scores.get(\"neutral\",  0.0),\n",
    "                \"pos\": scores.get(\"positive\", 0.0),\n",
    "                \"compound\": compound}\n",
    "\n",
    "    # ---------- new method ----------\n",
    "    def major_label_based(self, text: str):\n",
    "        \"\"\"\n",
    "        Returns (text_label, int_label) where int_label ∈ {−1,0,1}.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sc   = self.polarity_scores(text)\n",
    "            best = max((\"neg\", \"neu\", \"pos\"), key=sc.get)              # pick highest  [oai_citation:5‡stackoverflow.com](https://stackoverflow.com/questions/70916877/cant-seem-to-iterate-over-a-column-to-assign-sentiment-values?utm_source=chatgpt.com)\n",
    "            text_label = {\"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"}[best]\n",
    "            return text_label, {\"negative\": -1.0, \"neutral\": 0.0, \"positive\": 1.0}[text_label]\n",
    "        except Exception as e: \n",
    "            print(\"Error in major_label() for text:\", text)\n",
    "            print(\"Exception:\", e)\n",
    "\n",
    "    def polarity_scores(self, text: str):\n",
    "        \"\"\"\n",
    "        Вызывает self._pipe(text), получает 5 меток:\n",
    "          Very Positive, Positive, Neutral, Very Negative, Negative\n",
    "        Группирует их в три категории:\n",
    "          - pos (Very Positive + Positive)\n",
    "          - neu (Neutral)\n",
    "          - neg (Very Negative + Negative)\n",
    "        Возвращает словарь с тремя суммарными скорорами и compound = |pos - neg|.\n",
    "        \"\"\"\n",
    "        # получаем список словарей [{'label':..., 'score':...}, ...]\n",
    "        raw = self._pipe(text)[0]\n",
    "        # print(\"raw:\", raw) \n",
    "        # отображение исходных меток на группы\n",
    "        label_map = {\n",
    "            \"Very Positive\": \"pos\",\n",
    "            \"Positive\":      \"pos\",\n",
    "            \"Neutral\":       \"neu\",\n",
    "            \"Very Negative\": \"neg\",\n",
    "            \"Negative\":      \"neg\"\n",
    "        }\n",
    "\n",
    "        # инициализируем суммы\n",
    "        agg = {\"pos\": 0.0, \"neu\": 0.0, \"neg\": 0.0}\n",
    "\n",
    "        # суммируем по группам\n",
    "        for entry in raw:\n",
    "            grp = label_map.get(entry[\"label\"])\n",
    "            if grp:\n",
    "                agg[grp] += entry[\"score\"]\n",
    "\n",
    "        # вычисляем compound\n",
    "        compound = abs(agg[\"pos\"] - agg[\"neg\"])\n",
    "\n",
    "        return {\n",
    "            \"neg\":      agg[\"neg\"],\n",
    "            \"neu\":      agg[\"neu\"],\n",
    "            \"pos\":      agg[\"pos\"],\n",
    "            \"compound\": compound\n",
    "        }\n",
    "\n",
    "    def major_label(self, text: str):\n",
    "        \"\"\"\n",
    "        Без изменений: берёт из polarity_scores наибольшую из трёх категорий\n",
    "        и возвращает (текстовую метку, числовую метку).\n",
    "        \"\"\"\n",
    "        sc = self.polarity_scores(text)\n",
    "        best = max((\"neg\", \"neu\", \"pos\"), key=lambda k: sc[k])\n",
    "        text_label = {\"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"}[best]\n",
    "        int_label  = {\"negative\": -1.0, \"neutral\": 0.0, \"positive\": 1.0}[text_label]\n",
    "        return text_label, int_label\n",
    "\n",
    "analyzer_new = SentimentAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', -1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"#UA - Полонне LIFE https://t.me  |05/10/23|09:34CET| - - - - - - - Запустили послугу єВідновлення в застосунку Дія — подавайте заяву, щоб отримати кошти на ремонт пошкодженого житла. Світовий банк виділяє кошти, щоб українці змогли якомога скоріше відновити власні домівки, пошкоджені через повномасштабне вторгнення. На першому етапі послуги єВідновлення допомогу отримають люди, чиї будинки потребують косметичного ремонту. Наприклад, ваше житло придатне для життя, але треба замінити вікна, двері, відремонтувати дах тощо. Пріоритет мають учасники бойових дій, мобілізовані, сім’ї загиблих військових, люди з інвалідністю І та ІІ груп і багатодітні родини. Максимальна сума на відновлення одного об’єкта — 200 тисяч гривень. Далі масштабуємо програму. Щоб допомогу отримали люди, будинки яких були повністю знищені внаслідок російської агресії. Послуга в Дії працює за принципом, схожим до єПідтримки. Подаєте заяву на виплату за кілька кліків та відкриваєте спеціальну картку єВідновлення в одному з 5 банків-партнерів. Саме на цю картку потім надійдуть кошти, які витратите на будматеріали й ремонтні послуги. Суму компенсації визначить спеціальна комісія, яку скерує місцева влада. Комісія зафіксує ступінь руйнувань та зробить висновки. Рішення комісії отримаєте в Дії. Як подати заяву, які критерії, де відкрити картку та інше — зібрали для вас на зручному сайті. Послугу розробили разом з Міністерством розвитку громад, територій та інфраструктури за підтримки проєкту USAID / UK aid «Прозорість та підзвітність у державному управлінні та послугах/ TAPAS» та Світового банку. Полонне LIFE #ПроГоловне Запустили послугу єВідновлення в застосунку Дія — подавайте заяву, щоб отримати кошти на ремонт пошкодженого житла.  Світовий банк виділяє кошти, щоб українці змогли якомога скоріше відновити власні домівки, пошкоджені через повномасштабне вторгнення.   На першому етапі послуги єВідновлення допомогу отримають люди, чиї будинки потребують косметичного ремонту. Наприклад, ваше житло придатне для життя, але треба замінити вікна, двері, відремонтувати дах тощо. Пріоритет мають учасники бойових дій, мобілізовані, сім’ї загиблих військових, люди з інвалідністю І та ІІ груп і багатодітні родини. Максимальна сума на відновлення одного об’єкта — 200 тисяч гривень.   Далі масштабуємо програму. Щоб допомогу отримали люди, будинки яких були повністю знищені внаслідок російської агресії.   Послуга в Дії працює за принципом, схожим до єПідтримки. Подаєте заяву на виплату за кілька кліків та відкриваєте спеціальну картку єВідновлення в одному з 5 банків-партнерів. Саме на цю картку потім надійдуть кошти, які витратите на будматеріали й ремонтні послуги.   Суму компенсації визначить спеціальна комісія, яку скерує місцева влада. Комісія зафіксує ступінь руйнувань та зробить висновки. Рішення комісії отримаєте в Дії.   Як подати заяву, які критерії, де відкрити картку та інше — зібрали для вас на зручному сайті.  Послугу розробили разом з Міністерством розвитку громад, територій та інфраструктури за підтримки проєкту USAID / UK aid «Прозорість та підзвітність у державному управлінні та послугах/ TAPAS» та Світового банку. https://t.me\"\n",
    "analyzer_new.major_label_based(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m982.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: stack_data in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: decorator in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: exceptiongroup in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Installing collected packages: widgetsnbextension, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade ipywidgets         # or conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm[notebook] in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: ipywidgets in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (8.1.7)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: decorator in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade \"tqdm[notebook]\" ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipywidgets\u001b[39m\u001b[38;5;124m\"\u001b[39m, ipywidgets\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[1;32m      3\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebook/Lab\u001b[39m\u001b[38;5;124m\"\u001b[39m, notebook\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[1;32m      4\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m, tqdm\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[1;32m      5\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, platform\u001b[38;5;241m.\u001b[39mpython_version())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook'"
     ]
    }
   ],
   "source": [
    "import ipywidgets, notebook, sys, tqdm, platform\n",
    "print(\"ipywidgets\", ipywidgets.__version__,\n",
    "      \"notebook/Lab\", notebook.__version__,\n",
    "      \"tqdm\", tqdm.__version__,\n",
    "      \"python\", platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5923128f164852addb15224626b982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring texts:   0%|          | 0/12224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 3.  Inference → sentiment_pred column                                #\n",
    "# -------------------------------------------------------------------- #\n",
    "# from tqdm import tqdm      # или просто `from tqdm import tqdm`\n",
    "\n",
    "# tqdm.pandas()\n",
    "# df[\"sentiment_pred_1epoch\"] = df[\"document_content\"].apply(\n",
    "#     lambda txt: analyzer_new.major_label_based(txt)[1]     # keep numeric only\n",
    "# )\n",
    "from tqdm.notebook import tqdm          # or  from tqdm.auto import tqdm  (works everywhere)\n",
    "tqdm.pandas(desc=\"Scoring texts\")       # optional description\n",
    "\n",
    "df[\"sentiment_pred_1epoch\"] = (\n",
    "    df[\"document_content\"]\n",
    "      .progress_apply(lambda txt: analyzer_new.major_label_based(txt)[1])  # ⬅ use progress_apply\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>gpt_labels_v1</th>\n",
       "      <th>language</th>\n",
       "      <th>sentiment_pred_1epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12218</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12222</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12223</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>mixed</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content  annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...                  0.0   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.                  0.0   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...                 -1.0   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...                 -1.0   \n",
       "4                                            Добрий день                  0.0   \n",
       "...                                                  ...                  ...   \n",
       "12218  У меня три окна и двери выбило , даже и не дум...                 -1.0   \n",
       "12219  Краще \"повинна бути зручнішою, ніж Uber чи Boo...                 -1.0   \n",
       "12221  Питання, цей сертифікат можна вже використовув...                  0.0   \n",
       "12222  На Вугледарському напрямку загинув Рома Іванен...                 -1.0   \n",
       "12223  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...                 -1.0   \n",
       "\n",
       "       gpt_labels_v1 language  sentiment_pred_1epoch  \n",
       "0                0.0       ua                   -1.0  \n",
       "1               -1.0       ua                   -1.0  \n",
       "2               -1.0       ru                   -1.0  \n",
       "3               -1.0       ru                   -1.0  \n",
       "4                0.0       ua                   -1.0  \n",
       "...              ...      ...                    ...  \n",
       "12218           -1.0       ru                   -1.0  \n",
       "12219            1.0       ua                   -1.0  \n",
       "12221            0.0       ua                   -1.0  \n",
       "12222           -1.0       ua                   -1.0  \n",
       "12223           -1.0    mixed                   -1.0  \n",
       "\n",
       "[11616 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(subset=[\"annotator_sentiment\", 'sentiment_pred_1epoch'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.093%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (−1)       0.39      1.00      0.56      4541\n",
      "  neutral (0)       0.00      0.00      0.00      4702\n",
      "positive (+1)       0.00      0.00      0.00      2373\n",
      "\n",
      "     accuracy                           0.39     11616\n",
      "    macro avg       0.13      0.33      0.19     11616\n",
      " weighted avg       0.15      0.39      0.22     11616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"])     #  [oai_citation:6‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?utm_source=chatgpt.com)\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(                                              #  [oai_citation:7‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?utm_source=chatgpt.com)\n",
    "      df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.093%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (−1)       0.39      1.00      0.56      4541\n",
      "  neutral (0)       0.00      0.00      0.00      4702\n",
      "positive (+1)       0.00      0.00      0.00      2373\n",
      "\n",
      "     accuracy                           0.39     11616\n",
      "    macro avg       0.13      0.33      0.19     11616\n",
      " weighted avg       0.15      0.39      0.22     11616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/pelmeshek1706/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"])     #  [oai_citation:6‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?utm_source=chatgpt.com)\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(                                              #  [oai_citation:7‡scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?utm_source=chatgpt.com)\n",
    "      df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
