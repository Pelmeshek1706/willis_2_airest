## Introduction

Within the AIREST project, the sentiment analysis capabilities of NLP algorithms were verified using a corpus of Telegram user messages (COSMUS dataset). The evaluation includes three comparable sentiment analysis models to determine the most reliable option for further usage.

## COSMUS Dataset Overview

- **COSMUS** is a sentiment analysis dataset composed of Telegram messages in Ukrainian and Russian languages .
- Data is stored in Parquet format, containing approximately ~12,200 entries .
- Each entry consists of message content (`document_content`), sentiment labels (`annotator_sentiment`), and additional metadata fields: `response_id`, `document_id`, `user_id`, `response_timestamp`, `annotation_date`, `username`, `unique_document_id`, `language_wc`, `document_length`, `gpt_labels_v1`, `language_gpt`, `language_manual`, `language`, `stratification_label`, and `df_set` .
- Sentiment labels are manually annotated with values: "negative", "neutral", "positive", "mixed" .
- Dataset License: MIT .
- Associated tags include: `sentiment`, `social networks`, `Telegram` .
- Objective: analyzing sentiment polarity of social media posts .
- Accessible through Hugging Face Datasets and integrated with Pandas .
- Single available split: `train` on the entire dataset .
- Primary languages: Ukrainian and Russian .

## Section 1.1 â€” Sentiment Analysis

The main model evaluated is **YShynkarov/ukr-roberta-cosmus-sentiment**, which is a fine-tuned variant of `ukr-roberta` adapted to Ukrainian and Russian texts. Additionally, two other models were tested for comparison:

- **cardiffnlp/twitter-xlm-roberta-base-sentiment**  
- **tabularisai/multilingual-sentiment-analysis**  

### Comparative Results Table

| Model                                                         | Accuracy | Negative (P / R / F1) | Neutral (P / R / F1) | Positive (P / R / F1) | Macro Avg (P / R / F1) | Weighted Avg (P / R / F1) |
|:--------------------------------------------------------------|---------:|----------------------:|---------------------:|----------------------:|-----------------------:|--------------------------:|
| `YShynkarov/ukr-roberta-cosmus-sentiment`                     |   76.80% |    0.90 / 0.66 / 0.76 |   0.71 / 0.87 / 0.78 |    0.73 / 0.77 / 0.75 |     0.78 / 0.77 / 0.76 |             0.79 / 0.77 / 0.77 |
| `cardiffnlp/twitter-xlm-roberta-base-sentiment`               |   67.12% |    0.75 / 0.60 / 0.67 |   0.60 / 0.81 / 0.69 |    0.79 / 0.52 / 0.63 |     0.71 / 0.65 / 0.66 |             0.70 / 0.67 / 0.67 |
| `tabularisai/multilingual-sentiment-analysis`                 |   49.56% |    0.52 / 0.69 / 0.59 |   0.58 / 0.24 / 0.34 |    0.42 / 0.64 / 0.50 |     0.50 / 0.52 / 0.48 |             0.52 / 0.50 / 0.47 |

### Evaluation Methodology

1. **Data loading and filtering**: Dataset loaded using Hugging Face Datasets; records labeled as "mixed" were removed.
2. **Label encoding**: Textual labels ("negative", "neutral", "positive") were mapped to numerical values (-1, 0, +1).
3. **Model initialization**: Pipelines for sentiment analysis were set up for each model using identical tokenization and inference parameters.
4. **Predictions**: The sentiment category for each text entry was predicted using the `major_label()` method.
5. **Metric computation**: Accuracy, precision, recall, and F1 scores were calculated for negative, neutral, and positive classes using sklearn metrics.
6. **Result comparison**: Performance metrics were summarized in the comparative results table.

## Evaluation Script

The full evaluation script is provided in **cosmus_eval_major_label.py**, containing steps for:

- Dataset loading and preprocessing
- Model setup and inference
- Calculation and reporting of evaluation metrics