{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4b6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366a863f2ccc407193e6074eba905b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cosmus_eval_major_label.py\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------------------------------------------------- #\n",
    "# 1.  Load the COSMUS dataset (Telegram RU/UA posts)                   #\n",
    "# -------------------------------------------------------------------- #\n",
    "ds = load_dataset(\"YShynkarov/COSMUS\", split=\"train\") \n",
    "ds = ds.filter(lambda x: x[\"annotator_sentiment\"] != \"mixed\")\n",
    "df = ds.to_pandas()[[\"document_content\", \"annotator_sentiment\", \"language\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0f620aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11611</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11613</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11614</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11615</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>negative</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...             neutral   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.             neutral   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...            negative   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...            negative   \n",
       "4                                            Добрий день             neutral   \n",
       "...                                                  ...                 ...   \n",
       "11611  У меня три окна и двери выбило , даже и не дум...            negative   \n",
       "11612  Краще \"повинна бути зручнішою, ніж Uber чи Boo...            negative   \n",
       "11613  Питання, цей сертифікат можна вже використовув...             neutral   \n",
       "11614  На Вугледарському напрямку загинув Рома Іванен...            negative   \n",
       "11615  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...            negative   \n",
       "\n",
       "      language  \n",
       "0           ua  \n",
       "1           ua  \n",
       "2           ru  \n",
       "3           ru  \n",
       "4           ua  \n",
       "...        ...  \n",
       "11611       ru  \n",
       "11612       ua  \n",
       "11613       ua  \n",
       "11614       ua  \n",
       "11615    mixed  \n",
       "\n",
       "[11616 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9859b898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Бажаю удачі тим, хто цього потребує.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['document_content'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a72af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я розумію. Але ви хоч уявляєте, скільки часу на це піде? І не буде там великої суми, бо рахуватимуть тільки вартість \"коробки\". Опис майна \"до\" ніхто не робив.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1987db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "428d1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gold labels → integers\n",
    "label2id = {\"negative\": -1, \"neutral\": 0, \"positive\": 1}\n",
    "df[\"annotator_sentiment\"] = df[\"annotator_sentiment\"].map(label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4cf550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12218</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12222</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12223</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content  annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...                  0.0   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.                  0.0   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...                 -1.0   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...                 -1.0   \n",
       "4                                            Добрий день                  0.0   \n",
       "...                                                  ...                  ...   \n",
       "12218  У меня три окна и двери выбило , даже и не дум...                 -1.0   \n",
       "12219  Краще \"повинна бути зручнішою, ніж Uber чи Boo...                 -1.0   \n",
       "12221  Питання, цей сертифікат можна вже використовув...                  0.0   \n",
       "12222  На Вугледарському напрямку загинув Рома Іванен...                 -1.0   \n",
       "12223  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...                 -1.0   \n",
       "\n",
       "      language  \n",
       "0           ua  \n",
       "1           ua  \n",
       "2           ru  \n",
       "3           ru  \n",
       "4           ua  \n",
       "...        ...  \n",
       "12218       ru  \n",
       "12219       ua  \n",
       "12221       ua  \n",
       "12222       ua  \n",
       "12223    mixed  \n",
       "\n",
       "[11616 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 2.  Sentiment model wrapper with major_label()                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Multilingual twitter-XLM-RoBERTa sentiment wrapper.\n",
    "    Provides polarity_scores() *and* major_label().\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        mdl = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"       # 3-way (neg/neu/pos)\n",
    "        self._pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=mdl, tokenizer=mdl,\n",
    "            top_k=None                                              # returns all three scores  \n",
    "        )\n",
    "\n",
    "    # ---------- already supplied ----------\n",
    "    def polarity_scores(self, text: str):\n",
    "        res   = self._pipe(text)              # list[list[dict(label,score)]]\n",
    "        scores = {d[\"label\"]: d[\"score\"] for d in res[0]}\n",
    "        # compound = abs(scores.get(\"positive\", 0) - scores.get(\"negative\", 0))\n",
    "        return {\"neg\": scores.get(\"negative\", 0),\n",
    "                \"neu\": scores.get(\"neutral\",  0),\n",
    "                \"pos\": scores.get(\"positive\", 0),\n",
    "                # \"compound\": compound,\n",
    "                }\n",
    "\n",
    "    # ---------- new method ----------\n",
    "    def major_label(self, text: str):\n",
    "        \"\"\"\n",
    "        Returns (text_label, int_label) where int_label ∈ {−1,0,1}.\n",
    "        \"\"\"\n",
    "        sc   = self.polarity_scores(text)\n",
    "        best = max((\"neg\", \"neu\", \"pos\"), key=sc.get)           \n",
    "        text_label = {\"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"}[best]\n",
    "        return text_label, {\"negative\": -1, \"neutral\": 0, \"positive\": 1}[text_label]\n",
    "\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mixed': 0.011259634047746658, 'negative': 0.0018209181725978851, 'neutral': 0.006787061225622892, 'positive': 0.9801324009895325}\n",
      "('positive', 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek1706/Desktop/projects/phd_311_venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "class UkrSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Ukrainian sentiment analysis model based on YShynkarov/ukr-roberta-cosmus-sentiment.\n",
    "    Provides polarity_scores() and major_label().\n",
    "    \"\"\"\n",
    "    map_labels = {\n",
    "                'LABEL_0': 'mixed',\n",
    "                'LABEL_1': 'negative',\n",
    "                'LABEL_2': 'neutral',\n",
    "                'LABEL_3': 'positive',\n",
    "            }\n",
    "    int_label_map = {\n",
    "        \"negative\": -1.0,\n",
    "        \"neutral\": 0.0,\n",
    "        \"positive\": 1.0,\n",
    "        \"mixed\": 0\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        repo_id = \"YShynkarov/ukr-roberta-cosmus-sentiment\"\n",
    "        safetensor = hf_hub_download(repo_id=repo_id,\n",
    "                                     filename=\"ukrroberta_cosmus_sentiment.safetensors\")\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(\"youscan/ukr-roberta-base\", num_labels=4)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"youscan/ukr-roberta-base\")\n",
    "\n",
    "        model = RobertaForSequenceClassification(config)\n",
    "        state_dict = load_file(safetensor)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        self._pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=-1,               \n",
    "            return_all_scores=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    def polarity_scores(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Returns dict:\n",
    "          {\n",
    "            \"negative\": float_score,\n",
    "            \"neutral\":  float_score,\n",
    "            \"positive\": float_score,\n",
    "            \"mixed\":    float_score\n",
    "          }\n",
    "        \"\"\"\n",
    "        # pipeline returns list[list[{\"label\":..., \"score\":...}, ...]]\n",
    "        results = self._pipe(text)\n",
    "        scores = {\n",
    "            self.map_labels[item[\"label\"]]: item[\"score\"]\n",
    "            for item in results[0]\n",
    "        }\n",
    "        return scores\n",
    "\n",
    "    def major_label(self, text: str) -> tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Returns (text_label, int_label),\n",
    "        where int_label ∈ {-1,0,1,2} for negative, neutral, positive, mixed.\n",
    "        \"\"\"\n",
    "        scores = self.polarity_scores(text)\n",
    "        best = max(scores, key=scores.get)\n",
    "        return best, self.int_label_map[best]\n",
    "\n",
    "analyzer = UkrSentimentAnalyzer()\n",
    "print(analyzer.polarity_scores(\"Привіт! Все просто чудово\"))\n",
    "# → {'negative': 0.01, 'neutral': 0.05, 'positive': 0.90, 'mixed': 0.04}\n",
    "\n",
    "print(analyzer.major_label(\"Привіт! Все просто чудово\"))\n",
    "# → ('positive', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0ee8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11616/11616 [12:04<00:00, 16.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 3.  Inference → sentiment_pred column                                #\n",
    "# -------------------------------------------------------------------- #\n",
    "from tqdm import tqdm      # или просто `from tqdm import tqdm`\n",
    "tqdm.pandas()\n",
    "\n",
    "df[\"sentiment_pred_1epoch\"] = df[\"document_content\"].progress_apply(\n",
    "    lambda txt: analyzer.major_label(txt)[1]     # keep numeric only\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17cd3661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>language</th>\n",
       "      <th>sentiment_pred_1epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11611</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11613</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11614</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11615</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>-1</td>\n",
       "      <td>mixed</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content  annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...                    0   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.                    0   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...                   -1   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...                   -1   \n",
       "4                                            Добрий день                    0   \n",
       "...                                                  ...                  ...   \n",
       "11611  У меня три окна и двери выбило , даже и не дум...                   -1   \n",
       "11612  Краще \"повинна бути зручнішою, ніж Uber чи Boo...                   -1   \n",
       "11613  Питання, цей сертифікат можна вже використовув...                    0   \n",
       "11614  На Вугледарському напрямку загинув Рома Іванен...                   -1   \n",
       "11615  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...                   -1   \n",
       "\n",
       "      language  sentiment_pred_1epoch  \n",
       "0           ua                    1.0  \n",
       "1           ua                    0.0  \n",
       "2           ru                   -1.0  \n",
       "3           ru                   -1.0  \n",
       "4           ua                    1.0  \n",
       "...        ...                    ...  \n",
       "11611       ru                   -1.0  \n",
       "11612       ua                    1.0  \n",
       "11613       ua                    0.0  \n",
       "11614       ua                    1.0  \n",
       "11615    mixed                   -1.0  \n",
       "\n",
       "[11616 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(subset=[\"annotator_sentiment\", 'sentiment_pred_1epoch'])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e6758db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_pred_1epoch\n",
       " 0.0    5764\n",
       "-1.0    3345\n",
       " 1.0    2507\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment_pred_1epoch\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.799%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (−1)       0.90      0.66      0.76      4541\n",
      "  neutral (0)       0.71      0.87      0.78      4702\n",
      "positive (+1)       0.73      0.77      0.75      2373\n",
      "\n",
      "     accuracy                           0.77     11616\n",
      "    macro avg       0.78      0.77      0.76     11616\n",
      " weighted avg       0.79      0.77      0.77     11616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"])\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(\n",
    "      df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a971ee9",
   "metadata": {},
   "source": [
    "# Ukr -> Eng -> Sentiment by Vader (default sentiment in Openwillis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uk2en_like_space.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Yehor/kulyk-uk-en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "REVISION = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, revision=REVISION)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch_dtype,\n",
    "    revision=REVISION,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def translate_like_space(text: str) -> str:\n",
    "    prompt = \"Translate the text to English:\\n\" + text \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,              # greedy\n",
    "            repetition_penalty=1.05,     \n",
    "        )\n",
    "\n",
    "    gen = output[:, input_ids.shape[1]:]\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca568d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uk = \"Над Україною збито ракету та 7 із 8 «Шахедів»\"\n",
    "print(translate_like_space(uk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['translated_text'] = df['document_content'].progress_apply(translate_like_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602261d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_label_by_max(text: str) -> int:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    top = max(('neg', 'neu', 'pos'), key=lambda k: scores[k])\n",
    "    return {'neg': -1.0, 'neu': 0.0, 'pos': 1.0}[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['vader_analysis'] = df['translated_text'].apply(vader_label_by_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_analysis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annotator_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e58967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"vader_analysis\"])\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(\n",
    "      df[\"annotator_sentiment\"], df[\"vader_analysis\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f0774",
   "metadata": {},
   "source": [
    "# Num Syllables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy pyphen praat-parselmouth pingouin pandas numpy tqdm jsonlines pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --fuzzy \"https://drive.google.com/file/d/1j9d91QqE7_WnOnmEmidtOG55tpmxQUeJ/view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchaudio import info as audiofile_info\n",
    "\n",
    "tqdm.pandas(desc=\"Audio data processing\")\n",
    "\n",
    "with jsonlines.open(\"/content/labels.jsonl\", 'r') as reader:\n",
    "    for line in reader:\n",
    "        labels = line\n",
    "\n",
    "all_audio_files = glob(\"toronto_*/*.wav\", recursive=True)\n",
    "\n",
    "toronto_dataset = pd.DataFrame({\n",
    "    \"path\": all_audio_files\n",
    "})\n",
    "\n",
    "toronto_dataset[\"transcript\"] = toronto_dataset[\"path\"].progress_apply(\n",
    "    lambda x: labels[\"dataset/\" + \"/\".join(x.split(\"/\")[-2:])]\n",
    ")\n",
    "\n",
    "toronto_dataset[\"transcript_len\"] = toronto_dataset[\"transcript\"].progress_apply(len)\n",
    "\n",
    "def get_audio_dur_sec(path):\n",
    "    file_info = audiofile_info(path)\n",
    "    return file_info.num_frames / file_info.sample_rate\n",
    "\n",
    "toronto_dataset[\"audio_dur_sec\"] = toronto_dataset[\"path\"].progress_apply(get_audio_dur_sec)\n",
    "import re\n",
    "\n",
    "def extract_numbers(path):\n",
    "    nums = re.findall(r\"\\d+\", path)\n",
    "    return int(nums[0]), int(nums[-1])\n",
    "\n",
    "toronto_dataset = toronto_dataset.sort_values(by=\"path\", key=lambda col: col.map(extract_numbers)).reset_index(drop=True)\n",
    "toronto_dataset.to_csv(\"meta_toronto.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b276f4c",
   "metadata": {},
   "source": [
    "Ukrainian syllable counting & evaluation pipeline\n",
    "- syll_spacy         : spaCy-uk component (rule-based over Pyphen + patches)\n",
    "- syll_pyphen        : Pyphen hyphenation baseline\n",
    "- syll_nltk          : NLTK SyllableTokenizer with custom sonority hierarchy\n",
    "- syll_praat_like    : \"pure Python\" Parselmouth (intensity peaks + voicing)\n",
    "- syll_praat_original: original Praat script SyllableNucleiv3.praat via Parselmouth\n",
    "- Metrics            : MAE vs references + ICC (absolute agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, io, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- Text libs ----------------\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "import pyphen\n",
    "\n",
    "# ---------------- Audio (Praat) ------------\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "\n",
    "# ---------------- Stats --------------------\n",
    "import pingouin as pg\n",
    "\n",
    "# ---------------- NLTK ---------------------\n",
    "import nltk\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "AUDIO_ROOT = \"/content\"\n",
    "PRAAT_SCRIPT_PATH = \"SyllableNucleiv3.praat\"\n",
    "\n",
    "MIN_PITCH_HZ = 75.0\n",
    "MIN_SYLLABLE_SEP_SEC = 0.10\n",
    "PEAK_PROMINENCE_DB = 2.0\n",
    "INTENSITY_STEP = 0.01\n",
    "VOWELS_UK = set(\"аеєиіїоуюяАЕЄИІЇОУЮЯ\")\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    audio_root: Optional[str] = None\n",
    "    praat_script: Optional[str] = None\n",
    "\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def _to_text(x) -> str:\n",
    "    return x if isinstance(x, str) else \"\"\n",
    "\n",
    "def resolve_audio_path(p: str, audio_root: Optional[str]) -> str:\n",
    "    return p if os.path.isabs(p) else os.path.join(audio_root or \"\", p)\n",
    "\n",
    "def compute_spm(num_syll: float, dur_sec: float) -> float:\n",
    "    if not dur_sec or dur_sec <= 0:\n",
    "        return np.nan\n",
    "    return num_syll / (dur_sec / 60.0)\n",
    "\n",
    "# =========================\n",
    "# 1) Pyphen-only (текст)\n",
    "# =========================\n",
    "_dic = pyphen.Pyphen(lang=\"uk_UA\")\n",
    "_word_re = re.compile(r\"[А-ЩЬЮЯІЇЄҐа-щьюяіїєґʼ'’-]+\", re.U)\n",
    "\n",
    "def count_syll_pyphen(text: str) -> int:\n",
    "    text = _to_text(text)\n",
    "    if not text:\n",
    "        return 0\n",
    "    s = 0\n",
    "    for w in _word_re.findall(text):\n",
    "        w = w.replace(\"’\", \"'\").replace(\"ʼ\", \"'\")\n",
    "        for p in w.split(\"-\"):\n",
    "            ins = _dic.inserted(p)\n",
    "            s += (ins.count(\"-\") + 1) if ins else 1\n",
    "    return s\n",
    "\n",
    "# =========================\n",
    "# 2) spaCy-uk component\n",
    "# =========================\n",
    "def _syllables_word_uk(word: str) -> int:\n",
    "    w = word.replace(\"’\", \"'\").replace(\"ʼ\", \"'\")\n",
    "    if not any(ch in VOWELS_UK for ch in w):\n",
    "        return 1 if re.search(r\"[рРлЛ]\", w) else 1\n",
    "    total = 0\n",
    "    for p in w.split(\"-\"):\n",
    "        ins = _dic.inserted(p)\n",
    "        cnt = (ins.count(\"-\") + 1) if ins else 1\n",
    "        if re.search(r\"(йо|ЙО|ьо|ЬО)\", p):\n",
    "            cnt = max(1, cnt)\n",
    "        total += cnt\n",
    "    return total\n",
    "\n",
    "if not Token.has_extension(\"num_syllables\"):\n",
    "    Token.set_extension(\"num_syllables\", default=0)\n",
    "\n",
    "@Language.component(\"uk_syllable_counter\")\n",
    "def uk_syllable_counter(doc):\n",
    "    for t in doc:\n",
    "        if t.is_alpha or _word_re.fullmatch(t.text):\n",
    "            t._.num_syllables = _syllables_word_uk(t.text)\n",
    "        else:\n",
    "            t._.num_syllables = 0\n",
    "    return doc\n",
    "\n",
    "def _build_nlp_uk():\n",
    "    try:\n",
    "        nlp = spacy.load(\"uk_core_news_sm\")\n",
    "    except Exception:\n",
    "        nlp = spacy.blank(\"uk\")\n",
    "    if \"uk_syllable_counter\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"uk_syllable_counter\", last=True)\n",
    "    return nlp\n",
    "\n",
    "nlp_uk = _build_nlp_uk()\n",
    "\n",
    "def count_syll_spacy(text: str) -> int:\n",
    "    text = _to_text(text)\n",
    "    if not text:\n",
    "        return 0\n",
    "    return sum(t._.num_syllables for t in nlp_uk(text))\n",
    "\n",
    "# =========================\n",
    "# 3) NLTK SSP (укр.)\n",
    "# =========================\n",
    "UKR_SONORITY = [\n",
    "    \"аеєиіїоуюя\",   # vowels\n",
    "    \"йв\",           # glides / approximants\n",
    "    \"рл\",           # liquids\n",
    "    \"мн\",           # nasals\n",
    "    \"жзшщсхгф\",     # fricatives (г ≈ [ɦ])\n",
    "    \"бпдткґчц\"      # stops/affricates\n",
    "]\n",
    "SSP_UK = SyllableTokenizer(lang=\"uk\", sonority_hierarchy=UKR_SONORITY)\n",
    "_APOS_DASH_MAP = str.maketrans({\"’\": \"'\", \"ʼ\": \"'\", \"–\": \"-\", \"—\": \"-\"})\n",
    "WORD_RE_UK = re.compile(\n",
    "    r\"[А-ЩЬЮЯІЇЄҐа-щьюяіїєґ]+(?:'[А-ЩЬЮЯІЇЄҐа-щьюяіїєґ]+)?(?:-[А-ЩЬЮЯІЇЄҐа-щьюяіїєґ]+)*\"\n",
    ")\n",
    "\n",
    "def count_syll_nltk(text: str) -> int:\n",
    "    text = _to_text(text)\n",
    "    if not text:\n",
    "        return 0\n",
    "    text = text.translate(_APOS_DASH_MAP)\n",
    "    tokens = WORD_RE_UK.findall(text)\n",
    "    total = 0\n",
    "    for w in tokens:\n",
    "        for part in w.split(\"-\"):\n",
    "            p = part.replace(\"'\", \"\")\n",
    "            if p:\n",
    "                total += len(SSP_UK.tokenize(p))\n",
    "    return total\n",
    "\n",
    "# =========================\n",
    "# 4A) «Praat-like» nuclei using Parselmouth (clear Python)\n",
    "# =========================\n",
    "def count_syllables_praat_like(\n",
    "    audio_path: str,\n",
    "    min_pitch_hz: float = MIN_PITCH_HZ,\n",
    "    time_step_sec: float = INTENSITY_STEP,\n",
    "    min_separation_sec: float = MIN_SYLLABLE_SEP_SEC,\n",
    "    prominence_db: float = PEAK_PROMINENCE_DB\n",
    ") -> int:\n",
    "    snd = parselmouth.Sound(audio_path)\n",
    "    intensity = snd.to_intensity(minimum_pitch=min_pitch_hz, time_step=time_step_sec)\n",
    "    pitch = snd.to_pitch(time_step=time_step_sec, pitch_floor=min_pitch_hz)\n",
    "\n",
    "    times = intensity.xs()\n",
    "    vals = np.asarray(intensity.values).flatten()\n",
    "    n = len(vals)\n",
    "    if n < 3:\n",
    "        return 0\n",
    "\n",
    "    window = 3\n",
    "    cand = []\n",
    "    for i in range(window, n - window):\n",
    "        v = vals[i]\n",
    "        if vals[i-1] < v > vals[i+1]:\n",
    "            local_min = np.min(vals[i - window:i + window + 1])\n",
    "            if (v - local_min) >= prominence_db:\n",
    "                t = times[i]\n",
    "                f0 = pitch.get_value_at_time(t)\n",
    "                if f0 and not math.isnan(f0):\n",
    "                    cand.append((t, v))\n",
    "\n",
    "    if not cand:\n",
    "        return 0\n",
    "\n",
    "    cand.sort()\n",
    "    kept = []\n",
    "    for t, vv in cand:\n",
    "        if not kept or (t - kept[-1][0]) >= min_separation_sec:\n",
    "            kept.append((t, vv))\n",
    "        else:\n",
    "            if vv > kept[-1][1]:\n",
    "                kept[-1] = (t, vv)\n",
    "    return len(kept)\n",
    "\n",
    "# =========================\n",
    "# 4B) Original script Syllable Nuclei v3 (Praat)\n",
    "# =========================\n",
    "def count_syllables_praat_original(\n",
    "    audio_path: str,\n",
    "    praat_script_path: str,\n",
    "    *,\n",
    "    detect_filled_pauses: bool = False,\n",
    "    language: str = \"English\",\n",
    "    silence_db: float = -25.0,\n",
    "    min_dip_db: float = 2.0,\n",
    "    min_pause_s: float = 0.4,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Call SyllableNucleiv3.praat and return syllables amount (nsyll).\n",
    "    Requires v3-script file. Return Table and read nsyll column.\n",
    "    \"\"\"\n",
    "    sound = parselmouth.Sound(audio_path)\n",
    "    res = parselmouth.praat.run_file(\n",
    "        sound, praat_script_path,\n",
    "        '', 'None',\n",
    "        float(silence_db),\n",
    "        float(min_dip_db),\n",
    "        float(min_pause_s),\n",
    "        bool(detect_filled_pauses),\n",
    "        str(language),\n",
    "        1.0,                 # Filled_Pause_threshold (по умолчанию)\n",
    "        'Table', 'OverWriteData', False\n",
    "    )\n",
    "    table = res[-1] if isinstance(res, (list, tuple)) else res\n",
    "\n",
    "    # Method 1: TSV -> pandas, strip() on column names\n",
    "    try:\n",
    "        tsv = call(table, \"List\", False)\n",
    "        df = pd.read_csv(io.StringIO(tsv), sep=\"\\t\")\n",
    "        df.columns = df.columns.str.strip()\n",
    "        if \"nsyll\" in df.columns:\n",
    "            return int(df.loc[0, \"nsyll\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Method 2: get the index of the column whose name after strip().lower() == \"nsyll\"\n",
    "    ncol = call(table, \"Get number of columns\")\n",
    "    target_idx = None\n",
    "    for i in range(1, ncol + 1):\n",
    "        lbl = call(table, \"Get column label\", i)\n",
    "        if str(lbl).strip().lower() == \"nsyll\":\n",
    "            target_idx = i\n",
    "            break\n",
    "    if target_idx is None:\n",
    "        # fallback: sometimes there's a \"voicedcount\"\n",
    "        for i in range(1, ncol + 1):\n",
    "            lbl = call(table, \"Get column label\", i)\n",
    "            if str(lbl).strip().lower() == \"voicedcount\":\n",
    "                target_idx = i\n",
    "                break\n",
    "    if target_idx is None:\n",
    "        labels = [call(table, \"Get column label\", i) for i in range(1, ncol + 1)]\n",
    "        raise KeyError(f\"nsyll column not found. Columns: {labels}\")\n",
    "\n",
    "    val = call(table, \"Get value\", 1, target_idx)\n",
    "    return int(round(float(val)))\n",
    "\n",
    "# =========================\n",
    "# 5) Basic Runner\n",
    "# =========================\n",
    "def run_all(df: pd.DataFrame, paths: Paths) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Waiting in df: path, transcript, audio_dur_sec\n",
    "    Adding:\n",
    "      syll_pyphen, syll_spacy, syll_nltk,\n",
    "      syll_praat_like, syll_praat_original,\n",
    "      spm_spacy\n",
    "    \"\"\"\n",
    "    req = {\"path\", \"transcript\", \"audio_dur_sec\"}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"DataFrame lacks required columns: {missing}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"transcript\"] = df[\"transcript\"].apply(_to_text)\n",
    "    df[\"_abs_path\"] = df[\"path\"].apply(lambda p: resolve_audio_path(p, paths.audio_root))\n",
    "\n",
    "    # text methods\n",
    "    tqdm.pandas(desc=\"syll_pyphen\")\n",
    "    df[\"syll_pyphen\"] = df[\"transcript\"].progress_apply(count_syll_pyphen)\n",
    "\n",
    "    tqdm.pandas(desc=\"syll_spacy\")\n",
    "    df[\"syll_spacy\"] = df[\"transcript\"].progress_apply(count_syll_spacy)\n",
    "\n",
    "    tqdm.pandas(desc=\"syll_nltk\")\n",
    "    df[\"syll_nltk\"] = df[\"transcript\"].progress_apply(count_syll_nltk)\n",
    "\n",
    "    # audio methods\n",
    "    tqdm.pandas(desc=\"syll_praat_like\")\n",
    "    df[\"syll_praat_like\"] = df[\"_abs_path\"].progress_apply(\n",
    "        lambda p: np.nan if not os.path.exists(p) else count_syllables_praat_like(p)\n",
    "    )\n",
    "\n",
    "    tqdm.pandas(desc=\"syll_praat_original\")\n",
    "    def _safe_praat_orig(pth: str) -> float:\n",
    "        try:\n",
    "            if not os.path.exists(pth):\n",
    "                return np.nan\n",
    "            return float(count_syllables_praat_original(\n",
    "                pth, paths.praat_script or PRAAT_SCRIPT_PATH,\n",
    "                detect_filled_pauses=False, language=\"English\"\n",
    "            ))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    df[\"syll_praat_original\"] = df[\"_abs_path\"].progress_apply(_safe_praat_orig)\n",
    "\n",
    "    # SPM for the main text method (spaCy)\n",
    "    df[\"spm_spacy\"] = df.apply(lambda r: compute_spm(r[\"syll_spacy\"], r[\"audio_dur_sec\"]), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 6) Metrics\n",
    "# =========================\n",
    "def evaluate(df: pd.DataFrame) -> dict:\n",
    "    out = {}\n",
    "    # MAE between methods\n",
    "    pairs = [\n",
    "        (\"MAE_spaCy_vs_PraatLike\",  \"syll_spacy\", \"syll_praat_like\"),\n",
    "        (\"MAE_spaCy_vs_PraatOrig\",  \"syll_spacy\", \"syll_praat_original\"),\n",
    "        (\"MAE_spaCy_vs_Pyphen\",     \"syll_spacy\", \"syll_pyphen\"),\n",
    "        (\"MAE_spaCy_vs_NLTK\",       \"syll_spacy\", \"syll_nltk\"),\n",
    "        (\"MAE_NLTK_vs_PraatOrig\",   \"syll_nltk\",  \"syll_praat_original\"),\n",
    "        (\"MAE_Pyphen_vs_PraatOrig\", \"syll_pyphen\",\"syll_praat_original\"),\n",
    "        (\"MAE_PraatLike_vs_PraatOrig\",\"syll_praat_like\",\"syll_praat_original\"),\n",
    "    ]\n",
    "    for name, a, b in pairs:\n",
    "        out[name] = float(np.nanmean(np.abs(df[a] - df[b])))\n",
    "\n",
    "    # ICC on 5 \"raters\"\n",
    "    long = df[[\"path\",\"syll_spacy\",\"syll_pyphen\",\"syll_nltk\",\"syll_praat_like\",\"syll_praat_original\"]].melt(\n",
    "        id_vars=\"path\", var_name=\"rater\", value_name=\"score\"\n",
    "    ).dropna()\n",
    "    icc_table = pg.intraclass_corr(data=long, targets=\"path\", raters=\"rater\", ratings=\"score\")\n",
    "    # ICC2 ~ two-way random, absolute agreement, single rater\n",
    "    icc2 = icc_table.loc[icc_table[\"Type\"] == \"ICC2\", \"ICC\"].values[0]\n",
    "    out[\"ICC2_(absolute_agreement)\"] = float(icc2)\n",
    "    out[\"ICC_table\"] = icc_table\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"meta_toronto.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255040cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_all(df, Paths(audio_root=AUDIO_ROOT, praat_script=PRAAT_SCRIPT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b99f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print({k: v for k, v in metrics.items() if k != \"ICC_table\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4eef2b",
   "metadata": {},
   "source": [
    "### Find errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = results.copy()\n",
    "\n",
    "# 1) Basec errors\n",
    "df[\"err_abs_spaCy_praat\"] = (df[\"syll_spacy\"] - df[\"syll_praat_original\"]).abs()\n",
    "df[\"err_abs_pyphen_praat\"] = (df[\"syll_pyphen\"] - df[\"syll_praat_original\"]).abs()\n",
    "df[\"err_abs_nltk_praat\"] = (df[\"syll_nltk\"] - df[\"syll_praat_original\"]).abs()\n",
    "\n",
    "# 2) Normalization\n",
    "df[\"err_rel_spaCy_praat\"] = df[\"err_abs_spaCy_praat\"] / df[\"syll_praat_original\"].clip(lower=1)\n",
    "\n",
    "# 3) SPM for audio\n",
    "df[\"spm_praat\"] = df[\"syll_praat_original\"] / (df[\"audio_dur_sec\"] / 60.0)\n",
    "\n",
    "# 4) ΔSPM\n",
    "df[\"d_spm\"] = (df[\"spm_spacy\"] - df[\"spm_praat\"]).abs()\n",
    "\n",
    "# 5) Tail by absolute error (top 100)\n",
    "tail_abs = df.sort_values(\"err_abs_spaCy_praat\", ascending=False).head(100)\n",
    "\n",
    "# 6) Tail by ΔSPM (top 100)\n",
    "tail_spm = df.sort_values(\"d_spm\", ascending=False).head(100)\n",
    "\n",
    "cols = [\"path\", \"transcript\", \"audio_dur_sec\",\n",
    "        \"syll_spacy\", \"syll_praat_original\", \"err_abs_spaCy_praat\", \"spm_spacy\", \"spm_praat\", \"d_spm\"]\n",
    "tail_abs[cols].to_csv(\"tail_abs_top100.csv\", index=False)\n",
    "tail_spm[cols].to_csv(\"tail_spm_top100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e93453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_abs.transcript.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pingouin import intraclass_corr\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "df = results.copy()\n",
    "\n",
    "# ---- utils ----\n",
    "def pair_report(a, b, name_a, name_b):\n",
    "    s1, s2 = df[a], df[b]\n",
    "    mask = ~(s1.isna() | s2.isna())\n",
    "    s1, s2 = s1[mask], s2[mask]\n",
    "    mae  = float(np.mean(np.abs(s1 - s2)))\n",
    "    bias = float(np.mean(s1 - s2))\n",
    "    r,  _ = pearsonr(s1, s2)\n",
    "    rho,_ = spearmanr(s1, s2)\n",
    "    # Bland–Altman\n",
    "    m = (s1 + s2) / 2\n",
    "    d = s1 - s2\n",
    "    md = float(np.mean(d))\n",
    "    sd = float(np.std(d, ddof=1))\n",
    "    loa = (md - 1.96*sd, md + 1.96*sd)\n",
    "    return {\n",
    "        \"pair\": f\"{name_a} vs {name_b}\",\n",
    "        \"n\": int(mask.sum()),\n",
    "        \"MAE\": mae, \"Bias\": bias,\n",
    "        \"Pearson_r\": float(r), \"Spearman_rho\": float(rho),\n",
    "        \"BA_mean_diff\": md, \"BA_LOA_low\": loa[0], \"BA_LOA_high\": loa[1],\n",
    "    }\n",
    "\n",
    "# ---- Audio inside ----\n",
    "audio_rep = pair_report(\"syll_praat_like\", \"syll_praat_original\", \"PraatLike\", \"PraatOriginal\")\n",
    "\n",
    "# ---- Text-inside (all pairs) ----\n",
    "text_pairs = [\n",
    "    (\"syll_spacy\",\"syll_pyphen\",\"spaCy\",\"Pyphen\"),\n",
    "    (\"syll_spacy\",\"syll_nltk\",\"spaCy\",\"NLTK\"),\n",
    "    (\"syll_pyphen\",\"syll_nltk\",\"Pyphen\",\"NLTK\"),\n",
    "]\n",
    "text_rep = [pair_report(*p) for p in text_pairs]\n",
    "\n",
    "# ---- (optional) ICC within classes ----\n",
    "def icc_for(cols, label):\n",
    "    long = df[[\"path\"] + cols].melt(id_vars=\"path\", var_name=\"rater\", value_name=\"score\").dropna()\n",
    "    icct = intraclass_corr(long, targets=\"path\", raters=\"rater\", ratings=\"score\")\n",
    "    icc2 = float(icct.loc[icct[\"Type\"]==\"ICC2\",\"ICC\"].iloc[0])\n",
    "    return {\"group\": label, \"ICC2\": icc2}\n",
    "\n",
    "icc_audio = icc_for([\"syll_praat_like\",\"syll_praat_original\"], \"audio\")\n",
    "icc_text  = icc_for([\"syll_spacy\",\"syll_pyphen\",\"syll_nltk\"], \"text\")\n",
    "\n",
    "print(\"AUDIO inside-class:\", audio_rep, icc_audio)\n",
    "print(\"TEXT inside-class:\", *text_rep, icc_text, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
