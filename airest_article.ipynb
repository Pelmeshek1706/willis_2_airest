{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4b6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366a863f2ccc407193e6074eba905b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cosmus_eval_major_label.py\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------------------------------------------------- #\n",
    "# 1.  Load the COSMUS dataset (Telegram RU/UA posts)                   #\n",
    "# -------------------------------------------------------------------- #\n",
    "ds = load_dataset(\"YShynkarov/COSMUS\", split=\"train\") \n",
    "ds = ds.filter(lambda x: x[\"annotator_sentiment\"] != \"mixed\")\n",
    "df = ds.to_pandas()[[\"document_content\", \"annotator_sentiment\", \"language\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0f620aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11611</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11613</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11614</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11615</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>negative</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...             neutral   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.             neutral   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...            negative   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...            negative   \n",
       "4                                            Добрий день             neutral   \n",
       "...                                                  ...                 ...   \n",
       "11611  У меня три окна и двери выбило , даже и не дум...            negative   \n",
       "11612  Краще \"повинна бути зручнішою, ніж Uber чи Boo...            negative   \n",
       "11613  Питання, цей сертифікат можна вже використовув...             neutral   \n",
       "11614  На Вугледарському напрямку загинув Рома Іванен...            negative   \n",
       "11615  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...            negative   \n",
       "\n",
       "      language  \n",
       "0           ua  \n",
       "1           ua  \n",
       "2           ru  \n",
       "3           ru  \n",
       "4           ua  \n",
       "...        ...  \n",
       "11611       ru  \n",
       "11612       ua  \n",
       "11613       ua  \n",
       "11614       ua  \n",
       "11615    mixed  \n",
       "\n",
       "[11616 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9859b898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Бажаю удачі тим, хто цього потребує.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['document_content'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a72af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я розумію. Але ви хоч уявляєте, скільки часу на це піде? І не буде там великої суми, бо рахуватимуть тільки вартість \"коробки\". Опис майна \"до\" ніхто не робив.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1987db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "428d1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gold labels → integers\n",
    "label2id = {\"negative\": -1, \"neutral\": 0, \"positive\": 1}\n",
    "df[\"annotator_sentiment\"] = df[\"annotator_sentiment\"].map(label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4cf550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12218</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12222</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12223</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content  annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...                  0.0   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.                  0.0   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...                 -1.0   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...                 -1.0   \n",
       "4                                            Добрий день                  0.0   \n",
       "...                                                  ...                  ...   \n",
       "12218  У меня три окна и двери выбило , даже и не дум...                 -1.0   \n",
       "12219  Краще \"повинна бути зручнішою, ніж Uber чи Boo...                 -1.0   \n",
       "12221  Питання, цей сертифікат можна вже використовув...                  0.0   \n",
       "12222  На Вугледарському напрямку загинув Рома Іванен...                 -1.0   \n",
       "12223  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...                 -1.0   \n",
       "\n",
       "      language  \n",
       "0           ua  \n",
       "1           ua  \n",
       "2           ru  \n",
       "3           ru  \n",
       "4           ua  \n",
       "...        ...  \n",
       "12218       ru  \n",
       "12219       ua  \n",
       "12221       ua  \n",
       "12222       ua  \n",
       "12223    mixed  \n",
       "\n",
       "[11616 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 2.  Sentiment model wrapper with major_label()                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Multilingual twitter-XLM-RoBERTa sentiment wrapper.\n",
    "    Provides polarity_scores() *and* major_label().\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        mdl = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"       # 3-way (neg/neu/pos)\n",
    "        self._pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=mdl, tokenizer=mdl,\n",
    "            top_k=None                                              # returns all three scores  \n",
    "        )\n",
    "\n",
    "    # ---------- already supplied ----------\n",
    "    def polarity_scores(self, text: str):\n",
    "        res   = self._pipe(text)              # list[list[dict(label,score)]]\n",
    "        scores = {d[\"label\"]: d[\"score\"] for d in res[0]}\n",
    "        # compound = abs(scores.get(\"positive\", 0) - scores.get(\"negative\", 0))\n",
    "        return {\"neg\": scores.get(\"negative\", 0),\n",
    "                \"neu\": scores.get(\"neutral\",  0),\n",
    "                \"pos\": scores.get(\"positive\", 0),\n",
    "                # \"compound\": compound,\n",
    "                }\n",
    "\n",
    "    # ---------- new method ----------\n",
    "    def major_label(self, text: str):\n",
    "        \"\"\"\n",
    "        Returns (text_label, int_label) where int_label ∈ {−1,0,1}.\n",
    "        \"\"\"\n",
    "        sc   = self.polarity_scores(text)\n",
    "        best = max((\"neg\", \"neu\", \"pos\"), key=sc.get)           \n",
    "        text_label = {\"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"}[best]\n",
    "        return text_label, {\"negative\": -1, \"neutral\": 0, \"positive\": 1}[text_label]\n",
    "\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mixed': 0.011259634047746658, 'negative': 0.0018209181725978851, 'neutral': 0.006787061225622892, 'positive': 0.9801324009895325}\n",
      "('positive', 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pelmeshek1706/Desktop/projects/phd_311_venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "class UkrSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Ukrainian sentiment analysis model based on YShynkarov/ukr-roberta-cosmus-sentiment.\n",
    "    Provides polarity_scores() and major_label().\n",
    "    \"\"\"\n",
    "    map_labels = {\n",
    "                'LABEL_0': 'mixed',\n",
    "                'LABEL_1': 'negative',\n",
    "                'LABEL_2': 'neutral',\n",
    "                'LABEL_3': 'positive',\n",
    "            }\n",
    "    int_label_map = {\n",
    "        \"negative\": -1.0,\n",
    "        \"neutral\": 0.0,\n",
    "        \"positive\": 1.0,\n",
    "        \"mixed\": 0\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        repo_id = \"YShynkarov/ukr-roberta-cosmus-sentiment\"\n",
    "        safetensor = hf_hub_download(repo_id=repo_id,\n",
    "                                     filename=\"ukrroberta_cosmus_sentiment.safetensors\")\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(\"youscan/ukr-roberta-base\", num_labels=4)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"youscan/ukr-roberta-base\")\n",
    "\n",
    "        model = RobertaForSequenceClassification(config)\n",
    "        state_dict = load_file(safetensor)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        self._pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=-1,               \n",
    "            return_all_scores=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    def polarity_scores(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Returns dict:\n",
    "          {\n",
    "            \"negative\": float_score,\n",
    "            \"neutral\":  float_score,\n",
    "            \"positive\": float_score,\n",
    "            \"mixed\":    float_score\n",
    "          }\n",
    "        \"\"\"\n",
    "        # pipeline returns list[list[{\"label\":..., \"score\":...}, ...]]\n",
    "        results = self._pipe(text)\n",
    "        scores = {\n",
    "            self.map_labels[item[\"label\"]]: item[\"score\"]\n",
    "            for item in results[0]\n",
    "        }\n",
    "        return scores\n",
    "\n",
    "    def major_label(self, text: str) -> tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Returns (text_label, int_label),\n",
    "        where int_label ∈ {-1,0,1,2} for negative, neutral, positive, mixed.\n",
    "        \"\"\"\n",
    "        scores = self.polarity_scores(text)\n",
    "        best = max(scores, key=scores.get)\n",
    "        return best, self.int_label_map[best]\n",
    "\n",
    "analyzer = UkrSentimentAnalyzer()\n",
    "print(analyzer.polarity_scores(\"Привіт! Все просто чудово\"))\n",
    "# → {'negative': 0.01, 'neutral': 0.05, 'positive': 0.90, 'mixed': 0.04}\n",
    "\n",
    "print(analyzer.major_label(\"Привіт! Все просто чудово\"))\n",
    "# → ('positive', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0ee8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11616/11616 [12:04<00:00, 16.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 3.  Inference → sentiment_pred column                                #\n",
    "# -------------------------------------------------------------------- #\n",
    "from tqdm import tqdm      # или просто `from tqdm import tqdm`\n",
    "tqdm.pandas()\n",
    "\n",
    "df[\"sentiment_pred_1epoch\"] = df[\"document_content\"].progress_apply(\n",
    "    lambda txt: analyzer.major_label(txt)[1]     # keep numeric only\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17cd3661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_content</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>language</th>\n",
       "      <th>sentiment_pred_1epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡️Українська делегація відправилася на перемов...</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вибухи на Одещині, попередньо — ППО.</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>А что делать тем ,кто лишился своего жилья ,по...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Тогда учись быстро бегать. Для меня вопрос сло...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Добрий день</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11611</th>\n",
       "      <td>У меня три окна и двери выбило , даже и не дум...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ru</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>Краще \"повинна бути зручнішою, ніж Uber чи Boo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11613</th>\n",
       "      <td>Питання, цей сертифікат можна вже використовув...</td>\n",
       "      <td>0</td>\n",
       "      <td>ua</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11614</th>\n",
       "      <td>На Вугледарському напрямку загинув Рома Іванен...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ua</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11615</th>\n",
       "      <td>*_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...</td>\n",
       "      <td>-1</td>\n",
       "      <td>mixed</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        document_content  annotator_sentiment  \\\n",
       "0      ⚡️Українська делегація відправилася на перемов...                    0   \n",
       "1                   Вибухи на Одещині, попередньо — ППО.                    0   \n",
       "2      А что делать тем ,кто лишился своего жилья ,по...                   -1   \n",
       "3      Тогда учись быстро бегать. Для меня вопрос сло...                   -1   \n",
       "4                                            Добрий день                    0   \n",
       "...                                                  ...                  ...   \n",
       "11611  У меня три окна и двери выбило , даже и не дум...                   -1   \n",
       "11612  Краще \"повинна бути зручнішою, ніж Uber чи Boo...                   -1   \n",
       "11613  Питання, цей сертифікат можна вже використовув...                    0   \n",
       "11614  На Вугледарському напрямку загинув Рома Іванен...                   -1   \n",
       "11615  *_Управление «УКРАИНЫ» и «РФ» захвачено иудеям...                   -1   \n",
       "\n",
       "      language  sentiment_pred_1epoch  \n",
       "0           ua                    1.0  \n",
       "1           ua                    0.0  \n",
       "2           ru                   -1.0  \n",
       "3           ru                   -1.0  \n",
       "4           ua                    1.0  \n",
       "...        ...                    ...  \n",
       "11611       ru                   -1.0  \n",
       "11612       ua                    1.0  \n",
       "11613       ua                    0.0  \n",
       "11614       ua                    1.0  \n",
       "11615    mixed                   -1.0  \n",
       "\n",
       "[11616 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(subset=[\"annotator_sentiment\", 'sentiment_pred_1epoch'])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e6758db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_pred_1epoch\n",
       " 0.0    5764\n",
       "-1.0    3345\n",
       " 1.0    2507\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment_pred_1epoch\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.799%\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (−1)       0.90      0.66      0.76      4541\n",
      "  neutral (0)       0.71      0.87      0.78      4702\n",
      "positive (+1)       0.73      0.77      0.75      2373\n",
      "\n",
      "     accuracy                           0.77     11616\n",
      "    macro avg       0.78      0.77      0.76     11616\n",
      " weighted avg       0.79      0.77      0.77     11616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"])\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(\n",
    "      df[\"annotator_sentiment\"], df[\"sentiment_pred_1epoch\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a971ee9",
   "metadata": {},
   "source": [
    "# Ukr -> Eng -> Sentiment by Vader (default sentiment in Openwillis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uk2en_like_space.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Yehor/kulyk-uk-en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "REVISION = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, revision=REVISION)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch_dtype,\n",
    "    revision=REVISION,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def translate_like_space(text: str) -> str:\n",
    "    prompt = \"Translate the text to English:\\n\" + text \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,              # greedy\n",
    "            repetition_penalty=1.05,     \n",
    "        )\n",
    "\n",
    "    gen = output[:, input_ids.shape[1]:]\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca568d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uk = \"Над Україною збито ракету та 7 із 8 «Шахедів»\"\n",
    "print(translate_like_space(uk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['translated_text'] = df['document_content'].progress_apply(translate_like_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602261d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_label_by_max(text: str) -> int:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    top = max(('neg', 'neu', 'pos'), key=lambda k: scores[k])\n",
    "    return {'neg': -1.0, 'neu': 0.0, 'pos': 1.0}[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['vader_analysis'] = df['translated_text'].apply(vader_label_by_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_analysis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annotator_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e58967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------- #\n",
    "# 4.  Evaluation                                                       #\n",
    "# -------------------------------------------------------------------- #\n",
    "acc = accuracy_score(df[\"annotator_sentiment\"], df[\"vader_analysis\"])\n",
    "print(f\"Accuracy: {acc:.3%}\")\n",
    "\n",
    "print(classification_report(\n",
    "      df[\"annotator_sentiment\"], df[\"vader_analysis\"],\n",
    "      target_names=[\"negative (−1)\", \"neutral (0)\", \"positive (+1)\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f0774",
   "metadata": {},
   "source": [
    "# Num Syllables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy pyphen praat-parselmouth pingouin pandas numpy tqdm jsonlines pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --fuzzy \"https://drive.google.com/file/d/1j9d91QqE7_WnOnmEmidtOG55tpmxQUeJ/view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchaudio import info as audiofile_info\n",
    "\n",
    "tqdm.pandas(desc=\"Audio data processing\")\n",
    "\n",
    "with jsonlines.open(\"/content/labels.jsonl\", 'r') as reader:\n",
    "    for line in reader:\n",
    "        labels = line\n",
    "\n",
    "all_audio_files = glob(\"toronto_*/*.wav\", recursive=True)\n",
    "\n",
    "toronto_dataset = pd.DataFrame({\n",
    "    \"path\": all_audio_files\n",
    "})\n",
    "\n",
    "toronto_dataset[\"transcript\"] = toronto_dataset[\"path\"].progress_apply(\n",
    "    lambda x: labels[\"dataset/\" + \"/\".join(x.split(\"/\")[-2:])]\n",
    ")\n",
    "\n",
    "toronto_dataset[\"transcript_len\"] = toronto_dataset[\"transcript\"].progress_apply(len)\n",
    "\n",
    "def get_audio_dur_sec(path):\n",
    "    file_info = audiofile_info(path)\n",
    "    return file_info.num_frames / file_info.sample_rate\n",
    "\n",
    "toronto_dataset[\"audio_dur_sec\"] = toronto_dataset[\"path\"].progress_apply(get_audio_dur_sec)\n",
    "import re\n",
    "\n",
    "def extract_numbers(path):\n",
    "    nums = re.findall(r\"\\d+\", path)\n",
    "    return int(nums[0]), int(nums[-1])\n",
    "\n",
    "toronto_dataset = toronto_dataset.sort_values(by=\"path\", key=lambda col: col.map(extract_numbers)).reset_index(drop=True)\n",
    "toronto_dataset.to_csv(\"meta_toronto.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b276f4c",
   "metadata": {},
   "source": [
    "Ukrainian syllable counting & evaluation pipeline\n",
    "- syll_spacy         : spaCy-uk component (rule-based over Pyphen + patches)\n",
    "- syll_pyphen        : Pyphen hyphenation baseline\n",
    "- syll_nltk          : NLTK SyllableTokenizer with custom sonority hierarchy\n",
    "- syll_praat_like    : \"pure Python\" Parselmouth (intensity peaks + voicing)\n",
    "- syll_praat_original: original Praat script SyllableNucleiv3.praat via Parselmouth\n",
    "- Metrics            : MAE vs references + ICC (absolute agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, io, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- Text libs ----------------\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "import pyphen\n",
    "\n",
    "# ---------------- Audio (Praat) ------------\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "\n",
    "# ---------------- Stats --------------------\n",
    "import pingouin as pg\n",
    "\n",
    "# ---------------- NLTK ---------------------\n",
    "import nltk\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "AUDIO_ROOT = \"/content\"\n",
    "PRAAT_SCRIPT_PATH = \"SyllableNucleiv3.praat\"\n",
    "\n",
    "MIN_PITCH_HZ = 75.0\n",
    "MIN_SYLLABLE_SEP_SEC = 0.10\n",
    "PEAK_PROMINENCE_DB = 2.0\n",
    "INTENSITY_STEP = 0.01\n",
    "VOWELS_UK = set(\"аеєиіїоуюяАЕЄИІЇОУЮЯ\")\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    audio_root: Optional[str] = None\n",
    "    praat_script: Optional[str] = None\n",
    "\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def _to_text(x) -> str:\n",
    "    return x if isinstance(x, str) else \"\"\n",
    "\n",
    "def resolve_audio_path(p: str, audio_root: Optional[str]) -> str:\n",
    "    return p if os.path.isabs(p) else os.path.join(audio_root or \"\", p)\n",
    "\n",
    "def compute_spm(num_syll: float, dur_sec: float) -> float:\n",
    "    if not dur_sec or dur_sec <= 0:\n",
    "        return np.nan\n",
    "    return num_syll / (dur_sec / 60.0)\n",
    "\n",
    "# =========================\n",
    "# 1) Pyphen-only (текст)\n",
    "# =========================\n",
    "_dic = pyphen.Pyphen(lang=\"uk_UA\")\n",
    "_word_re = re.compile(r\"[А-ЩЬЮЯІЇЄҐа-щьюяіїєґʼ'’-]+\", re.U)\n",
    "\n",
    "def count_syll_pyphen(text: str) -> int:\n",
    "    text = _to_text(text)\n",
    "    if not text:\n",
    "        return 0\n",
    "    s = 0\n",
    "    for w in _word_re.findall(text):\n",
    "        w = w.replace(\"’\", \"'\").replace(\"ʼ\", \"'\")\n",
    "        for p in w.split(\"-\"):\n",
    "            ins = _dic.inserted(p)\n",
    "            s += (ins.count(\"-\") + 1) if ins else 1\n",
    "    return s\n",
    "\n",
    "# =========================\n",
    "# 2) spaCy-uk component\n",
    "# =========================\n",
    "def _syllables_word_uk(word: str) -> int:\n",
    "    w = word.replace(\"’\", \"'\").replace(\"ʼ\", \"'\")\n",
    "    if not any(ch in VOWELS_UK for ch in w):\n",
    "        return 1 if re.search(r\"[рРлЛ]\", w) else 1\n",
    "    total = 0\n",
    "    for p in w.split(\"-\"):\n",
    "        ins = _dic.inserted(p)\n",
    "        cnt = (ins.count(\"-\") + 1) if ins else 1\n",
    "        if re.search(r\"(йо|ЙО|ьо|ЬО)\", p):\n",
    "            cnt = max(1, cnt)\n",
    "        total += cnt\n",
    "    return total\n",
    "\n",
    "if not Token.has_extension(\"num_syllables\"):\n",
    "    Token.set_extension(\"num_syllables\", default=0)\n",
    "\n",
    "@Language.component(\"uk_syllable_counter\")\n",
    "def uk_syllable_counter(doc):\n",
    "    for t in doc:\n",
    "        if t.is_alpha or _word_re.fullmatch(t.text):\n",
    "            t._.num_syllables = _syllables_word_uk(t.text)\n",
    "        else:\n",
    "            t._.num_syllables = 0\n",
    "    return doc\n",
    "\n",
    "def _build_nlp_uk():\n",
    "    try:\n",
    "        nlp = spacy.load(\"uk_core_news_sm\")\n",
    "    except Exception:\n",
    "        nlp = spacy.blank(\"uk\")\n",
    "    if \"uk_syllable_counter\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"uk_syllable_counter\", last=True)\n",
    "    return nlp\n",
    "\n",
    "nlp_uk = _build_nlp_uk()\n",
    "\n",
    "def count_syll_spacy(text: str) -> int:\n",
    "    text = _to_text(text)\n",
    "    if not text:\n",
    "        return 0\n",
    "    return sum(t._.num_syllables for t in nlp_uk(text))\n",
    "\n",
    "# =========================\n",
    "# 3) NLTK SSP (укр.)\n",
    "# =========================\n",
    "UKR_SONORITY = [\n",
    "    \"аеєиіїоуюя\",   # vowels\n",
    "    \"йв\",           # glides / approximants\n",
    "    \"рл\",           # liquids\n",
    "    \"мн\",           # nasals\n",
    "    \"жзшщсхгф\",     # fricatives (г ≈ [ɦ])\n",
    "    \"бпдткґчц\"      # stops/affricates\n",
    "]\n",
    "SSP_UK = SyllableTokenizer(lang=\"uk\", sonority_hierarchy=UKR_SONORITY)\n",
    "_APOS_DASH_MAP = str.maketrans({\"’\": \"'\", \"ʼ\": \"'\", \"–\": \"-\", \"—\": \"-\"})\n",
    "WORD_RE_UK = re.compile(\n",
    "    r\"[А-ЩЬЮЯІЇЄҐа-щьюяіїєґ]+(?:'[А-ЩЬЮЯІЇЄҐа-щьюяіїєґ]+)?(?:-[А-ЩЬЮЯІЇЄҐа-щьюяіїєґ]+)*\"\n",
    ")\n",
    "\n",
    "def count_syll_nltk(text: str) -> int:\n",
    "    text = _to_text(text)\n",
    "    if not text:\n",
    "        return 0\n",
    "    text = text.translate(_APOS_DASH_MAP)\n",
    "    tokens = WORD_RE_UK.findall(text)\n",
    "    total = 0\n",
    "    for w in tokens:\n",
    "        for part in w.split(\"-\"):\n",
    "            p = part.replace(\"'\", \"\")\n",
    "            if p:\n",
    "                total += len(SSP_UK.tokenize(p))\n",
    "    return total\n",
    "\n",
    "# =========================\n",
    "# 4A) «Praat-like» nuclei using Parselmouth (clear Python)\n",
    "# =========================\n",
    "def count_syllables_praat_like(\n",
    "    audio_path: str,\n",
    "    min_pitch_hz: float = MIN_PITCH_HZ,\n",
    "    time_step_sec: float = INTENSITY_STEP,\n",
    "    min_separation_sec: float = MIN_SYLLABLE_SEP_SEC,\n",
    "    prominence_db: float = PEAK_PROMINENCE_DB\n",
    ") -> int:\n",
    "    snd = parselmouth.Sound(audio_path)\n",
    "    intensity = snd.to_intensity(minimum_pitch=min_pitch_hz, time_step=time_step_sec)\n",
    "    pitch = snd.to_pitch(time_step=time_step_sec, pitch_floor=min_pitch_hz)\n",
    "\n",
    "    times = intensity.xs()\n",
    "    vals = np.asarray(intensity.values).flatten()\n",
    "    n = len(vals)\n",
    "    if n < 3:\n",
    "        return 0\n",
    "\n",
    "    window = 3\n",
    "    cand = []\n",
    "    for i in range(window, n - window):\n",
    "        v = vals[i]\n",
    "        if vals[i-1] < v > vals[i+1]:\n",
    "            local_min = np.min(vals[i - window:i + window + 1])\n",
    "            if (v - local_min) >= prominence_db:\n",
    "                t = times[i]\n",
    "                f0 = pitch.get_value_at_time(t)\n",
    "                if f0 and not math.isnan(f0):\n",
    "                    cand.append((t, v))\n",
    "\n",
    "    if not cand:\n",
    "        return 0\n",
    "\n",
    "    cand.sort()\n",
    "    kept = []\n",
    "    for t, vv in cand:\n",
    "        if not kept or (t - kept[-1][0]) >= min_separation_sec:\n",
    "            kept.append((t, vv))\n",
    "        else:\n",
    "            if vv > kept[-1][1]:\n",
    "                kept[-1] = (t, vv)\n",
    "    return len(kept)\n",
    "\n",
    "# =========================\n",
    "# 4B) Original script Syllable Nuclei v3 (Praat)\n",
    "# =========================\n",
    "def count_syllables_praat_original(\n",
    "    audio_path: str,\n",
    "    praat_script_path: str,\n",
    "    *,\n",
    "    detect_filled_pauses: bool = False,\n",
    "    language: str = \"English\",\n",
    "    silence_db: float = -25.0,\n",
    "    min_dip_db: float = 2.0,\n",
    "    min_pause_s: float = 0.4,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Call SyllableNucleiv3.praat and return syllables amount (nsyll).\n",
    "    Requires v3-script file. Return Table and read nsyll column.\n",
    "    \"\"\"\n",
    "    sound = parselmouth.Sound(audio_path)\n",
    "    res = parselmouth.praat.run_file(\n",
    "        sound, praat_script_path,\n",
    "        '', 'None',\n",
    "        float(silence_db),\n",
    "        float(min_dip_db),\n",
    "        float(min_pause_s),\n",
    "        bool(detect_filled_pauses),\n",
    "        str(language),\n",
    "        1.0,                 # Filled_Pause_threshold (по умолчанию)\n",
    "        'Table', 'OverWriteData', False\n",
    "    )\n",
    "    table = res[-1] if isinstance(res, (list, tuple)) else res\n",
    "\n",
    "    # Method 1: TSV -> pandas, strip() on column names\n",
    "    try:\n",
    "        tsv = call(table, \"List\", False)\n",
    "        df = pd.read_csv(io.StringIO(tsv), sep=\"\\t\")\n",
    "        df.columns = df.columns.str.strip()\n",
    "        if \"nsyll\" in df.columns:\n",
    "            return int(df.loc[0, \"nsyll\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Method 2: get the index of the column whose name after strip().lower() == \"nsyll\"\n",
    "    ncol = call(table, \"Get number of columns\")\n",
    "    target_idx = None\n",
    "    for i in range(1, ncol + 1):\n",
    "        lbl = call(table, \"Get column label\", i)\n",
    "        if str(lbl).strip().lower() == \"nsyll\":\n",
    "            target_idx = i\n",
    "            break\n",
    "    if target_idx is None:\n",
    "        # fallback: sometimes there's a \"voicedcount\"\n",
    "        for i in range(1, ncol + 1):\n",
    "            lbl = call(table, \"Get column label\", i)\n",
    "            if str(lbl).strip().lower() == \"voicedcount\":\n",
    "                target_idx = i\n",
    "                break\n",
    "    if target_idx is None:\n",
    "        labels = [call(table, \"Get column label\", i) for i in range(1, ncol + 1)]\n",
    "        raise KeyError(f\"nsyll column not found. Columns: {labels}\")\n",
    "\n",
    "    val = call(table, \"Get value\", 1, target_idx)\n",
    "    return int(round(float(val)))\n",
    "\n",
    "# =========================\n",
    "# 5) Basic Runner\n",
    "# =========================\n",
    "def run_all(df: pd.DataFrame, paths: Paths) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Waiting in df: path, transcript, audio_dur_sec\n",
    "    Adding:\n",
    "      syll_pyphen, syll_spacy, syll_nltk,\n",
    "      syll_praat_like, syll_praat_original,\n",
    "      spm_spacy\n",
    "    \"\"\"\n",
    "    req = {\"path\", \"transcript\", \"audio_dur_sec\"}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"DataFrame lacks required columns: {missing}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"transcript\"] = df[\"transcript\"].apply(_to_text)\n",
    "    df[\"_abs_path\"] = df[\"path\"].apply(lambda p: resolve_audio_path(p, paths.audio_root))\n",
    "\n",
    "    # text methods\n",
    "    tqdm.pandas(desc=\"syll_pyphen\")\n",
    "    df[\"syll_pyphen\"] = df[\"transcript\"].progress_apply(count_syll_pyphen)\n",
    "\n",
    "    tqdm.pandas(desc=\"syll_spacy\")\n",
    "    df[\"syll_spacy\"] = df[\"transcript\"].progress_apply(count_syll_spacy)\n",
    "\n",
    "    tqdm.pandas(desc=\"syll_nltk\")\n",
    "    df[\"syll_nltk\"] = df[\"transcript\"].progress_apply(count_syll_nltk)\n",
    "\n",
    "    # audio methods\n",
    "    tqdm.pandas(desc=\"syll_praat_like\")\n",
    "    df[\"syll_praat_like\"] = df[\"_abs_path\"].progress_apply(\n",
    "        lambda p: np.nan if not os.path.exists(p) else count_syllables_praat_like(p)\n",
    "    )\n",
    "\n",
    "    tqdm.pandas(desc=\"syll_praat_original\")\n",
    "    def _safe_praat_orig(pth: str) -> float:\n",
    "        try:\n",
    "            if not os.path.exists(pth):\n",
    "                return np.nan\n",
    "            return float(count_syllables_praat_original(\n",
    "                pth, paths.praat_script or PRAAT_SCRIPT_PATH,\n",
    "                detect_filled_pauses=False, language=\"English\"\n",
    "            ))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    df[\"syll_praat_original\"] = df[\"_abs_path\"].progress_apply(_safe_praat_orig)\n",
    "\n",
    "    # SPM for the main text method (spaCy)\n",
    "    df[\"spm_spacy\"] = df.apply(lambda r: compute_spm(r[\"syll_spacy\"], r[\"audio_dur_sec\"]), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 6) Metrics\n",
    "# =========================\n",
    "def evaluate(df: pd.DataFrame) -> dict:\n",
    "    out = {}\n",
    "    # MAE between methods\n",
    "    pairs = [\n",
    "        (\"MAE_spaCy_vs_PraatLike\",  \"syll_spacy\", \"syll_praat_like\"),\n",
    "        (\"MAE_spaCy_vs_PraatOrig\",  \"syll_spacy\", \"syll_praat_original\"),\n",
    "        (\"MAE_spaCy_vs_Pyphen\",     \"syll_spacy\", \"syll_pyphen\"),\n",
    "        (\"MAE_spaCy_vs_NLTK\",       \"syll_spacy\", \"syll_nltk\"),\n",
    "        (\"MAE_NLTK_vs_PraatOrig\",   \"syll_nltk\",  \"syll_praat_original\"),\n",
    "        (\"MAE_Pyphen_vs_PraatOrig\", \"syll_pyphen\",\"syll_praat_original\"),\n",
    "        (\"MAE_PraatLike_vs_PraatOrig\",\"syll_praat_like\",\"syll_praat_original\"),\n",
    "    ]\n",
    "    for name, a, b in pairs:\n",
    "        out[name] = float(np.nanmean(np.abs(df[a] - df[b])))\n",
    "\n",
    "    # ICC on 5 \"raters\"\n",
    "    long = df[[\"path\",\"syll_spacy\",\"syll_pyphen\",\"syll_nltk\",\"syll_praat_like\",\"syll_praat_original\"]].melt(\n",
    "        id_vars=\"path\", var_name=\"rater\", value_name=\"score\"\n",
    "    ).dropna()\n",
    "    icc_table = pg.intraclass_corr(data=long, targets=\"path\", raters=\"rater\", ratings=\"score\")\n",
    "    # ICC2 ~ two-way random, absolute agreement, single rater\n",
    "    icc2 = icc_table.loc[icc_table[\"Type\"] == \"ICC2\", \"ICC\"].values[0]\n",
    "    out[\"ICC2_(absolute_agreement)\"] = float(icc2)\n",
    "    out[\"ICC_table\"] = icc_table\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"meta_toronto.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255040cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_all(df, Paths(audio_root=AUDIO_ROOT, praat_script=PRAAT_SCRIPT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b99f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print({k: v for k, v in metrics.items() if k != \"ICC_table\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4eef2b",
   "metadata": {},
   "source": [
    "### Find errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = results.copy()\n",
    "\n",
    "# 1) Basec errors\n",
    "df[\"err_abs_spaCy_praat\"] = (df[\"syll_spacy\"] - df[\"syll_praat_original\"]).abs()\n",
    "df[\"err_abs_pyphen_praat\"] = (df[\"syll_pyphen\"] - df[\"syll_praat_original\"]).abs()\n",
    "df[\"err_abs_nltk_praat\"] = (df[\"syll_nltk\"] - df[\"syll_praat_original\"]).abs()\n",
    "\n",
    "# 2) Normalization\n",
    "df[\"err_rel_spaCy_praat\"] = df[\"err_abs_spaCy_praat\"] / df[\"syll_praat_original\"].clip(lower=1)\n",
    "\n",
    "# 3) SPM for audio\n",
    "df[\"spm_praat\"] = df[\"syll_praat_original\"] / (df[\"audio_dur_sec\"] / 60.0)\n",
    "\n",
    "# 4) ΔSPM\n",
    "df[\"d_spm\"] = (df[\"spm_spacy\"] - df[\"spm_praat\"]).abs()\n",
    "\n",
    "# 5) Tail by absolute error (top 100)\n",
    "tail_abs = df.sort_values(\"err_abs_spaCy_praat\", ascending=False).head(100)\n",
    "\n",
    "# 6) Tail by ΔSPM (top 100)\n",
    "tail_spm = df.sort_values(\"d_spm\", ascending=False).head(100)\n",
    "\n",
    "cols = [\"path\", \"transcript\", \"audio_dur_sec\",\n",
    "        \"syll_spacy\", \"syll_praat_original\", \"err_abs_spaCy_praat\", \"spm_spacy\", \"spm_praat\", \"d_spm\"]\n",
    "tail_abs[cols].to_csv(\"tail_abs_top100.csv\", index=False)\n",
    "tail_spm[cols].to_csv(\"tail_spm_top100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e93453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_abs.transcript.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pingouin import intraclass_corr\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "df = results.copy()\n",
    "\n",
    "# ---- utils ----\n",
    "def pair_report(a, b, name_a, name_b):\n",
    "    s1, s2 = df[a], df[b]\n",
    "    mask = ~(s1.isna() | s2.isna())\n",
    "    s1, s2 = s1[mask], s2[mask]\n",
    "    mae  = float(np.mean(np.abs(s1 - s2)))\n",
    "    bias = float(np.mean(s1 - s2))\n",
    "    r,  _ = pearsonr(s1, s2)\n",
    "    rho,_ = spearmanr(s1, s2)\n",
    "    # Bland–Altman\n",
    "    m = (s1 + s2) / 2\n",
    "    d = s1 - s2\n",
    "    md = float(np.mean(d))\n",
    "    sd = float(np.std(d, ddof=1))\n",
    "    loa = (md - 1.96*sd, md + 1.96*sd)\n",
    "    return {\n",
    "        \"pair\": f\"{name_a} vs {name_b}\",\n",
    "        \"n\": int(mask.sum()),\n",
    "        \"MAE\": mae, \"Bias\": bias,\n",
    "        \"Pearson_r\": float(r), \"Spearman_rho\": float(rho),\n",
    "        \"BA_mean_diff\": md, \"BA_LOA_low\": loa[0], \"BA_LOA_high\": loa[1],\n",
    "    }\n",
    "\n",
    "# ---- Audio inside ----\n",
    "audio_rep = pair_report(\"syll_praat_like\", \"syll_praat_original\", \"PraatLike\", \"PraatOriginal\")\n",
    "\n",
    "# ---- Text-inside (all pairs) ----\n",
    "text_pairs = [\n",
    "    (\"syll_spacy\",\"syll_pyphen\",\"spaCy\",\"Pyphen\"),\n",
    "    (\"syll_spacy\",\"syll_nltk\",\"spaCy\",\"NLTK\"),\n",
    "    (\"syll_pyphen\",\"syll_nltk\",\"Pyphen\",\"NLTK\"),\n",
    "]\n",
    "text_rep = [pair_report(*p) for p in text_pairs]\n",
    "\n",
    "# ---- (optional) ICC within classes ----\n",
    "def icc_for(cols, label):\n",
    "    long = df[[\"path\"] + cols].melt(id_vars=\"path\", var_name=\"rater\", value_name=\"score\").dropna()\n",
    "    icct = intraclass_corr(long, targets=\"path\", raters=\"rater\", ratings=\"score\")\n",
    "    icc2 = float(icct.loc[icct[\"Type\"]==\"ICC2\",\"ICC\"].iloc[0])\n",
    "    return {\"group\": label, \"ICC2\": icc2}\n",
    "\n",
    "icc_audio = icc_for([\"syll_praat_like\",\"syll_praat_original\"], \"audio\")\n",
    "icc_text  = icc_for([\"syll_spacy\",\"syll_pyphen\",\"syll_nltk\"], \"text\")\n",
    "\n",
    "print(\"AUDIO inside-class:\", audio_rep, icc_audio)\n",
    "print(\"TEXT inside-class:\", *text_rep, icc_text, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4076b",
   "metadata": {},
   "source": [
    "# translate DAIZWOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Yehor/kulyk-en-uk\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "REVISION = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, revision=REVISION)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch_dtype,\n",
    "    revision=REVISION,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def translate_like_space(text: str) -> str:\n",
    "    prompt = \"Translate the text to Ukrainian:\\n\" + text\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,              # greedy\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "    gen = output[:, input_ids.shape[1]:]\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/dcapswoz_all_transcripts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99390778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['Text_ukr'] = df['Text'].progress_apply(translate_like_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dcapwoz_all_plus_ukr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688a1b9",
   "metadata": {},
   "source": [
    "# Calculating perplexity, tangeniality, coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377cdcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>File_number</th>\n",
       "      <th>Text_ukr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.1</td>\n",
       "      <td>so I'm going to</td>\n",
       "      <td>0.934210</td>\n",
       "      <td>300</td>\n",
       "      <td>Тому я збираюся</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>interview in Spanish</td>\n",
       "      <td>0.608470</td>\n",
       "      <td>300</td>\n",
       "      <td>Інтерв'ю іспанською мовою</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23.9</td>\n",
       "      <td>24.3</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.690606</td>\n",
       "      <td>300</td>\n",
       "      <td>добре</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62.1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>good</td>\n",
       "      <td>0.951897</td>\n",
       "      <td>300</td>\n",
       "      <td>хороший</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>68.8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>Atlanta Georgia</td>\n",
       "      <td>0.987629</td>\n",
       "      <td>300</td>\n",
       "      <td>Атланта Джорджія</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26078</th>\n",
       "      <td>26078</td>\n",
       "      <td>26078</td>\n",
       "      <td>1171.6</td>\n",
       "      <td>1185.3</td>\n",
       "      <td>what I'm most proud of I can say that that th...</td>\n",
       "      <td>0.927357</td>\n",
       "      <td>718</td>\n",
       "      <td>Що я найбільше пишаюся, можу сказати, що це ро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26079</th>\n",
       "      <td>26079</td>\n",
       "      <td>26079</td>\n",
       "      <td>1186.6</td>\n",
       "      <td>1238.6</td>\n",
       "      <td>I like when my kids now my oldest kid you kno...</td>\n",
       "      <td>0.969410</td>\n",
       "      <td>718</td>\n",
       "      <td>Мені подобається, коли мої діти зараз найстарш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26080</th>\n",
       "      <td>26080</td>\n",
       "      <td>26080</td>\n",
       "      <td>1248.2</td>\n",
       "      <td>1248.9</td>\n",
       "      <td>you're welcome</td>\n",
       "      <td>0.982563</td>\n",
       "      <td>718</td>\n",
       "      <td>Вітаємо</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26081</th>\n",
       "      <td>26081</td>\n",
       "      <td>26081</td>\n",
       "      <td>1252.8</td>\n",
       "      <td>1253.3</td>\n",
       "      <td>goodbye</td>\n",
       "      <td>0.875275</td>\n",
       "      <td>718</td>\n",
       "      <td>Прощання</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26082</th>\n",
       "      <td>26082</td>\n",
       "      <td>26082</td>\n",
       "      <td>1273.2</td>\n",
       "      <td>1273.7</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.972170</td>\n",
       "      <td>718</td>\n",
       "      <td>добре</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26083 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  Start_Time  End_Time  \\\n",
       "0                 0           0        14.3      15.1   \n",
       "1                 1           1        20.3      21.1   \n",
       "2                 2           2        23.9      24.3   \n",
       "3                 3           3        62.1      62.7   \n",
       "4                 4           4        68.8      69.8   \n",
       "...             ...         ...         ...       ...   \n",
       "26078         26078       26078      1171.6    1185.3   \n",
       "26079         26079       26079      1186.6    1238.6   \n",
       "26080         26080       26080      1248.2    1248.9   \n",
       "26081         26081       26081      1252.8    1253.3   \n",
       "26082         26082       26082      1273.2    1273.7   \n",
       "\n",
       "                                                    Text  Confidence  \\\n",
       "0                                        so I'm going to    0.934210   \n",
       "1                                   interview in Spanish    0.608470   \n",
       "2                                                   okay    0.690606   \n",
       "3                                                   good    0.951897   \n",
       "4                                        Atlanta Georgia    0.987629   \n",
       "...                                                  ...         ...   \n",
       "26078   what I'm most proud of I can say that that th...    0.927357   \n",
       "26079   I like when my kids now my oldest kid you kno...    0.969410   \n",
       "26080                                     you're welcome    0.982563   \n",
       "26081                                            goodbye    0.875275   \n",
       "26082                                               okay    0.972170   \n",
       "\n",
       "       File_number                                           Text_ukr  \n",
       "0              300                                    Тому я збираюся  \n",
       "1              300                          Інтерв'ю іспанською мовою  \n",
       "2              300                                              добре  \n",
       "3              300                                            хороший  \n",
       "4              300                                   Атланта Джорджія  \n",
       "...            ...                                                ...  \n",
       "26078          718  Що я найбільше пишаюся, можу сказати, що це ро...  \n",
       "26079          718  Мені подобається, коли мої діти зараз найстарш...  \n",
       "26080          718                                            Вітаємо  \n",
       "26081          718                                           Прощання  \n",
       "26082          718                                              добре  \n",
       "\n",
       "[26083 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_woz = pd.read_csv(\"/Users/pelmeshek1706/Desktop/projects/airest_notebooks/data/dcapwoz_all_plus_ukr.csv\")\n",
    "df_woz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\n",
    "    \"PYTORCH_CUDA_ALLOC_CONF\",\n",
    "    \"expandable_segments:True,max_split_size_mb:128\"\n",
    ")  # enables expandable segments and limits large block fragmentation\n",
    "# See official docs on expandable_segments. Must be set **BEFORE** importing torch.  #  [oai_citation:1‡PyTorch Docs](https://docs.pytorch.org/docs/stable/notes/cuda.html?utm_source=chatgpt.com)\n",
    "\n",
    "# ============================================================\n",
    "# DiscourseMetrics — version with CPU embedding and memory-friendly SDPA\n",
    "# ============================================================\n",
    "\n",
    "import re, string, math\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class DiscourseMetrics:\n",
    "    def __init__(\n",
    "        self,\n",
    "        language: str = \"en\",\n",
    "        device: Optional[str] = None,      # device for LM (usually \"cuda\")\n",
    "        emb_device: str = \"cpu\",           # embedder ONLY on CPU, as requested\n",
    "        max_len_sent: int = 256,\n",
    "        emb_model_id: str = \"google/embeddinggemma-300m\",\n",
    "        ppl_model_id: str = \"google/gemma-3-270m\",\n",
    "        ppl_max_tokens: int = 2048,\n",
    "        dtype_auto: bool = True,\n",
    "        use_tf32: bool = True,\n",
    "        use_flash_attn2: bool = False,     # defaults to SDPA; enable FA2 if available\n",
    "        compile_model: bool = False,\n",
    "        emb_batch_size: int = 128,         # ↓ defaults lowered for memory spikes\n",
    "        ppl_windows_bs: int = 256          # ↓ defaults lowered for memory spikes\n",
    "    ):\n",
    "        self.language = language\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.emb_device = emb_device\n",
    "        self.max_len_sent = int(max_len_sent)\n",
    "        self.ppl_max_tokens = int(ppl_max_tokens)\n",
    "        self.emb_batch_size = int(emb_batch_size)\n",
    "        self.ppl_windows_bs = int(ppl_windows_bs)\n",
    "\n",
    "        # ---- matrix multiply accelerators (Ampere/Ada/Hopper) ----\n",
    "        if self.device == \"cuda\" and use_tf32:\n",
    "            try:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.set_float32_matmul_precision(\"high\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # === EMBEDDINGS (on CPU!) ===\n",
    "        # SentenceTransformer supports explicit device selection: \"cpu\" / \"cuda:0\" / list of devices.  [oai_citation:2‡SentenceTransformers](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html?utm_source=chatgpt.com)\n",
    "        self.emb = SentenceTransformer(emb_model_id, device=self.emb_device)\n",
    "        self.emb_dim = self.emb.get_sentence_embedding_dimension()\n",
    "\n",
    "        # === Tokenizer ===\n",
    "        self.tok = AutoTokenizer.from_pretrained(ppl_model_id)\n",
    "        if self.tok.pad_token is None and self.tok.eos_token is not None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "        self.tok.padding_side = \"left\"  # left-padding is better for windowed batching\n",
    "\n",
    "        # === dtype ===\n",
    "        if dtype_auto:\n",
    "            dtype = torch.bfloat16 if self.device == \"cuda\" else torch.float32\n",
    "        else:\n",
    "            dtype = torch.float32\n",
    "        self._dtype = dtype\n",
    "\n",
    "        # === Load causal LM with SDPA (or FA2, if available) ===\n",
    "        self.causal = self._load_causal_resilient(\n",
    "            ppl_model_id,\n",
    "            dtype=dtype,\n",
    "            want_flash_attn2=(self.device == \"cuda\" and use_flash_attn2)\n",
    "        ).to(self.device).eval()\n",
    "        self.causal.config.use_cache = False\n",
    "        self.model_max_length = getattr(self.causal.config, \"max_position_embeddings\", 32768)\n",
    "\n",
    "        if compile_model:\n",
    "            try:\n",
    "                self.causal = torch.compile(self.causal, mode=\"reduce-overhead\", fullgraph=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # legacy API\n",
    "        self.measures: Dict[str, object] = {\n",
    "            \"english_langs\": {\"en\", \"english\", \"EN\", \"ENG\"},\n",
    "            \"supported_langs_bert\": {\n",
    "                \"en\", \"english\", \"ru\", \"uk\", \"ua\", \"de\", \"fr\", \"es\", \"it\", \"pt\",\n",
    "                \"nl\", \"pl\", \"sv\", \"tr\", \"ar\", \"zh\", \"ja\", \"ko\"\n",
    "            },\n",
    "            \"words_texts\": \"words_texts\",\n",
    "        }\n",
    "\n",
    "    # --------- resilient loaders (dtype + SDPA/FA2) ---------\n",
    "    @staticmethod\n",
    "    def _from_pretrained_with_dtype(model_id: str, dtype, **kwargs):\n",
    "        try:\n",
    "            return AutoModelForCausalLM.from_pretrained(model_id, dtype=dtype, **kwargs)\n",
    "        except TypeError:\n",
    "            return AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, **kwargs)\n",
    "\n",
    "    def _load_causal_resilient(self, model_id: str, dtype, want_flash_attn2: bool):\n",
    "        attn = \"flash_attention_2\" if want_flash_attn2 else \"sdpa\"\n",
    "        try:\n",
    "            # SDPA — fast native attention in PyTorch/Transformers; FA2 requires fp16/bf16.  [oai_citation:3‡Hugging Face](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one?utm_source=chatgpt.com)\n",
    "            return self._from_pretrained_with_dtype(model_id, dtype=dtype, attn_implementation=attn)\n",
    "        except (ImportError, OSError, RuntimeError) as e:\n",
    "            if \"flash_attn\" in str(e) or \"flash_attn_2_cuda\" in str(e) or \"FlashAttention\" in str(e):\n",
    "                # fallback to SDPA\n",
    "                return self._from_pretrained_with_dtype(model_id, dtype=dtype, attn_implementation=\"sdpa\")\n",
    "            raise\n",
    "        except TypeError as e:\n",
    "            if \"attn_implementation\" in str(e):\n",
    "                return self._from_pretrained_with_dtype(model_id, dtype=dtype)\n",
    "            raise\n",
    "\n",
    "    # ---------------------- utils ----------------------\n",
    "    @staticmethod\n",
    "    def _clean_text(text: str) -> str:\n",
    "        clean = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "        return clean\n",
    "\n",
    "    def _encode_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        if not texts:\n",
    "            return np.zeros((0, self.emb_dim), dtype=np.float32)\n",
    "        vecs = self.emb.encode(\n",
    "            texts,\n",
    "            batch_size=self.emb_batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        return vecs.astype(np.float32)\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        enc = self.tok(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        ids = enc[\"input_ids\"].to(self.device)\n",
    "        attn = enc[\"attention_mask\"].to(self.device)\n",
    "        return ids, attn\n",
    "\n",
    "    def _truncate(self, ids: torch.Tensor, attn: torch.Tensor):\n",
    "        max_len = min(self.model_max_length, self.ppl_max_tokens)\n",
    "        if ids.size(1) > max_len:\n",
    "            ids = ids[:, :max_len]\n",
    "            attn = attn[:, :max_len]\n",
    "        return ids, attn\n",
    "\n",
    "    # ---------------------- PPL (one-pass) ----------------------\n",
    "    @torch.inference_mode()\n",
    "    def _perplexity_full(self, text: str) -> float:\n",
    "        ids, attn = self._tokenize(text)\n",
    "        ids, attn = self._truncate(ids, attn)\n",
    "        if ids.size(1) < 2:\n",
    "            return float(\"nan\")\n",
    "        out = self.causal(input_ids=ids, attention_mask=attn)\n",
    "        logits = out.logits\n",
    "        shift_logits = logits[:, :-1, :]\n",
    "        shift_labels = ids[:, 1:]\n",
    "        shift_mask = attn[:, 1:].to(dtype=shift_logits.dtype)\n",
    "        logp = F.log_softmax(shift_logits, dim=-1)\n",
    "        nll = -logp.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        nll = (nll * shift_mask).sum() / shift_mask.sum().clamp_min(1.0)\n",
    "        return float(torch.exp(nll).item())\n",
    "\n",
    "    # ---------------------- PPL (windowed, batched windows) ----------------------\n",
    "    @torch.inference_mode()\n",
    "    def _batch_next_logprobs(self, batch_ids: torch.Tensor, lens: torch.Tensor, next_ids: torch.Tensor) -> torch.Tensor:\n",
    "        attn_mask = (batch_ids != self.tok.pad_token_id).to(batch_ids.dtype)\n",
    "        out = self.causal(input_ids=batch_ids, attention_mask=attn_mask)\n",
    "        idx = lens - 1\n",
    "        last_logits = out.logits[torch.arange(batch_ids.size(0), device=batch_ids.device), idx]\n",
    "        logp = F.log_softmax(last_logits, dim=-1)\n",
    "        return logp[torch.arange(batch_ids.size(0), device=batch_ids.device), next_ids]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _ppl_k_batched(self, ids: torch.Tensor, k: int) -> float:\n",
    "        ids = ids[:, :min(ids.size(1), self.ppl_max_tokens)]\n",
    "        T = ids.size(1)\n",
    "        if T < 2:\n",
    "            return float(\"nan\")\n",
    "        k_eff = min(k, self.model_max_length - 1)\n",
    "        pad = self.tok.pad_token_id\n",
    "        chunks = []\n",
    "        for start_t in range(1, T, self.ppl_windows_bs):\n",
    "            t_positions = list(range(start_t, min(T, start_t + self.ppl_windows_bs)))\n",
    "            lens, next_ids, seqs = [], [], []\n",
    "            for t in t_positions:\n",
    "                L = min(k_eff, t)\n",
    "                lens.append(L)\n",
    "                next_ids.append(int(ids[0, t].item()))\n",
    "                seqs.append(ids[0, t - L:t])\n",
    "            maxL = max(lens)\n",
    "            batch = torch.full((len(seqs), maxL), pad, dtype=torch.long, device=self.device)\n",
    "            for b, seq in enumerate(seqs):\n",
    "                batch[b, -lens[b]:] = seq\n",
    "            lens_t = torch.tensor(lens, device=self.device)\n",
    "            next_t = torch.tensor(next_ids, device=self.device)\n",
    "            chunks.append(self._batch_next_logprobs(batch, lens_t, next_t))\n",
    "        logps = torch.cat(chunks)\n",
    "        return float(math.exp(-logps.mean().item()))\n",
    "\n",
    "    def calculate_perplexity(self, text: str, windows: List[int] = None, mode: str = \"full\") -> Tuple[float, float, float, float]:\n",
    "        if windows is None:\n",
    "            windows = [256, 2, 5, 7]\n",
    "        ids, attn = self._tokenize(text)\n",
    "        ids, attn = self._truncate(ids, attn)\n",
    "\n",
    "        results = {}\n",
    "        if mode == \"full\":\n",
    "            results[256] = self._perplexity_full(text)\n",
    "            for k in [w for w in windows if w != 256]:\n",
    "                results[k] = self._ppl_k_batched(ids, k)\n",
    "        else:\n",
    "            for k in windows:\n",
    "                results[k] = self._ppl_k_batched(ids, k)\n",
    "\n",
    "        return (results.get(256, float(\"nan\")),\n",
    "                results.get(2, float(\"nan\")),\n",
    "                results.get(5, float(\"nan\")),\n",
    "                results.get(7, float(\"nan\")))\n",
    "\n",
    "    # ---------------------- Tangentiality (batched) ----------------------\n",
    "    @staticmethod\n",
    "    def _split_phrases(text: str) -> List[str]:\n",
    "        parts = re.split(r'(?<=[\\.\\!\\?\\n])\\s+|\\n+', text)\n",
    "        return [p.strip() for p in parts if p and p.strip()]\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_words(text: str) -> List[str]:\n",
    "        words = re.sub(r'\\s+', ' ', text.strip()).split(' ')\n",
    "        return [w for w in words if w]\n",
    "\n",
    "    def compute_turn_tangentiality_list(self, texts: List[str]) -> List[float]:\n",
    "        if not texts:\n",
    "            return []\n",
    "        first_text_phr = self._split_phrases(texts[0])\n",
    "        if len(first_text_phr) >= 2:\n",
    "            pe0 = self._encode_texts(first_text_phr)\n",
    "            s0 = float(np.mean([np.dot(pe0[j-1], pe0[j]) for j in range(1, len(pe0))]))\n",
    "        else:\n",
    "            s0 = float(\"nan\")\n",
    "\n",
    "        last_phr, first_phr = [], []\n",
    "        for i in range(1, len(texts)):\n",
    "            prev_phr = self._split_phrases(texts[i-1])\n",
    "            curr_phr = self._split_phrases(texts[i])\n",
    "            last_phr.append(prev_phr[-1] if prev_phr else \"\")\n",
    "            first_phr.append(curr_phr[0] if curr_phr else \"\")\n",
    "\n",
    "        pair_embs = self._encode_texts(last_phr + first_phr)\n",
    "        A, B = pair_embs[:len(last_phr)], pair_embs[len(last_phr):]\n",
    "        sims = (A * B).sum(axis=1)\n",
    "        return [s0] + sims.tolist()\n",
    "\n",
    "    # ---------------------- Word coherence ----------------------\n",
    "    def get_word_embeddings(self, word_list: List[str], *_, **__) -> np.ndarray:\n",
    "        if len(word_list) == 0:\n",
    "            return np.zeros((0, self.emb_dim), dtype=np.float32)\n",
    "        vecs = self.emb.encode(\n",
    "            word_list,\n",
    "            batch_size=128,              # conservative\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=False,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        return vecs.astype(np.float32)\n",
    "\n",
    "    def get_word_coherence_utterance(self, row: dict, measures: dict = None, **kwargs):\n",
    "        measures = measures or {\"words_texts\": \"words_texts\"}\n",
    "        words_texts = row.get(measures['words_texts'], [])\n",
    "        if len(words_texts) == 0:\n",
    "            return [np.nan]*0, [np.nan]*0, [np.nan]*0, {k: [np.nan]*0 for k in range(2, 11)}\n",
    "\n",
    "        word_embeddings = self.get_word_embeddings(words_texts)\n",
    "        norms = np.linalg.norm(word_embeddings, axis=1, keepdims=True).clip(min=1e-9)\n",
    "        we = word_embeddings / norms\n",
    "        sim = we @ we.T\n",
    "\n",
    "        if len(words_texts) > 1:\n",
    "            word_coh = [np.nan] + [float(sim[j, j-1]) for j in range(1, len(words_texts))]\n",
    "        else:\n",
    "            word_coh = [np.nan]*len(words_texts)\n",
    "\n",
    "        if len(words_texts) > 5:\n",
    "            arr = [float(np.mean(sim[j-2:j+3, j])) for j in range(2, len(words_texts)-2)]\n",
    "            word_coh_5 = [np.nan]*2 + arr + [np.nan]*2\n",
    "        else:\n",
    "            word_coh_5 = [np.nan]*len(words_texts)\n",
    "\n",
    "        if len(words_texts) > 10:\n",
    "            arr = [float(np.mean(sim[j-5:j+6, j])) for j in range(5, len(words_texts)-5)]\n",
    "            word_coh_10 = [np.nan]*5 + arr + [np.nan]*5\n",
    "        else:\n",
    "            word_coh_10 = [np.nan]*len(words_texts)\n",
    "\n",
    "        variability = {}\n",
    "        for k in range(2, 11):\n",
    "            if len(words_texts) > k:\n",
    "                variability[k] = [float(sim[j, j+k]) for j in range(len(words_texts)-k)] + [np.nan]*k\n",
    "            else:\n",
    "                variability[k] = [np.nan]*len(words_texts)\n",
    "\n",
    "        return word_coh, word_coh_5, word_coh_10, variability\n",
    "\n",
    "    # ---------------------- Compatible wrappers ----------------------\n",
    "    def compute_perplexity_metric(self, text: str) -> float:\n",
    "        return float(self._perplexity_full(text))\n",
    "\n",
    "    def compute_tangentiality_metric(self, text: str) -> float:\n",
    "        phrases = self._split_phrases(text)\n",
    "        if len(phrases) >= 2:\n",
    "            pe = self._encode_texts(phrases)\n",
    "            return float(np.mean([np.dot(pe[j-1], pe[j]) for j in range(1, len(pe))]))\n",
    "        return float(\"nan\")\n",
    "\n",
    "    def compute_coherence_metric(self, text: str) -> float:\n",
    "        words = self._split_words(text)\n",
    "        row_like = {self.measures[\"words_texts\"]: words}\n",
    "        word_coh, _, _, _ = self.get_word_coherence_utterance(row=row_like, measures=self.measures)\n",
    "        return float(np.nanmean(word_coh)) if len(word_coh) else float(\"nan\")\n",
    "\n",
    "    def compute_dataset(self, texts: List[str]) -> pd.DataFrame:\n",
    "        tangen_list = self.compute_turn_tangentiality_list(texts)\n",
    "        rows = []\n",
    "        for t, tang in tqdm(zip(texts, tangen_list)):\n",
    "            try:\n",
    "                perplexity = self.compute_perplexity_metric(t)\n",
    "            except Exception:\n",
    "                perplexity = None\n",
    "            try:\n",
    "                coherence = self.compute_coherence_metric(t)\n",
    "            except Exception:\n",
    "                coherence = None\n",
    "            rows.append({\n",
    "                \"text\": t,\n",
    "                \"perplexity\": perplexity,\n",
    "                \"tangeniality\": tang,\n",
    "                \"coherence\": coherence,\n",
    "            })\n",
    "        return pd.DataFrame(rows, columns=[\"text\", \"perplexity\", \"tangeniality\", \"coherence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b06c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DiscourseMetrics(\n",
    "    language=\"uk\",\n",
    "    emb_model_id=\"google/embeddinggemma-300m\",\n",
    "    ppl_model_id=\"google/gemma-3-270m\",\n",
    "    ppl_max_tokens=2048,\n",
    "    device=\"mps\",        \n",
    "    emb_device=\"cpu\",     \n",
    "    use_flash_attn2=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fcac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def build_file_level_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    meter: \"DiscourseMetrics\",\n",
    "    file_col: str = \"File_number\",\n",
    "    text_col: Optional[str] = None,\n",
    "    checkpoint_every: int = 1,\n",
    "    # Where to store subfolders \"5\", \"10\", etc.; by default — current directory\n",
    "    checkpoint_base_dir: str = \"parts\",\n",
    "    # Checkpoint file base name (without extension)\n",
    "    checkpoint_name: str = \"metrics_partial\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Every `checkpoint_every` groups:\n",
    "      - saves an intermediate result to <checkpoint_base_dir>/<iters_done>/\n",
    "      - clears memory (gc + torch.{mps|cuda}.empty_cache())\n",
    "\n",
    "    Returns the final combined DataFrame.\n",
    "    \"\"\"\n",
    "    if file_col not in df.columns:\n",
    "        raise ValueError(f\"Column `{file_col}` not found in df\")\n",
    "\n",
    "    # Auto-detect text column\n",
    "    if text_col is None:\n",
    "        if \"Text\" in df.columns:\n",
    "            text_col = \"Text\"\n",
    "        elif \"text\" in df.columns:\n",
    "            text_col = \"text\"\n",
    "        else:\n",
    "            raise ValueError(\"Text column not found. Provide `text_col` or add 'Text'/'text' to df.\")\n",
    "\n",
    "    # Prepare groups (fix order by file_col values)\n",
    "    # Avoid .groupby(sort=True) on large data — extract and sort unique values first\n",
    "    unique_files = sorted(df[file_col].dropna().unique().tolist())\n",
    "\n",
    "    results = []\n",
    "    iters_done = 0\n",
    "\n",
    "    # Helper: save checkpoint\n",
    "    def _save_checkpoint():\n",
    "        nonlocal results, iters_done\n",
    "        if iters_done == 0:\n",
    "            return\n",
    "        out_df_partial = (\n",
    "            pd.DataFrame(results)\n",
    "            .sort_values(\"file_number\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        ckpt_dir = os.path.join(checkpoint_base_dir, str(iters_done))\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        # Save as both parquet (fast & type-safe) and CSV (just in case)\n",
    "        pq_path = os.path.join(ckpt_dir, f\"{checkpoint_name}.parquet\")\n",
    "        csv_path = os.path.join(ckpt_dir, f\"{checkpoint_name}.csv\")\n",
    "        try:\n",
    "            out_df_partial.to_parquet(pq_path, index=False)\n",
    "        except Exception:\n",
    "            # If pyarrow/fastparquet is missing — at least save as CSV\n",
    "            pass\n",
    "        out_df_partial.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Helper: aggressively free up memory\n",
    "    def _flush_mem(*extra_to_del):\n",
    "        # Delete any provided large local objects\n",
    "        for obj in extra_to_del:\n",
    "            try:\n",
    "                del obj\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Garbage collection\n",
    "        try:\n",
    "            gc.collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Sync and empty cache on MPS/CUDA (if present)\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                try:\n",
    "                    torch.mps.synchronize()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    torch.mps.empty_cache()  # frees unused MPS cache\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    torch.cuda.synchronize()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    # Since PyTorch 2.8, recommended: torch.cuda.memory.empty_cache\n",
    "                    # but classic torch.cuda.empty_cache() is still supported and maps to that\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        from torch.cuda import memory as _cuda_memory\n",
    "                        _cuda_memory.empty_cache()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        except Exception:\n",
    "            # No torch, or unexpected config\n",
    "            pass\n",
    "\n",
    "    # Main loop\n",
    "    for file_number in tqdm(unique_files, total=len(unique_files)):\n",
    "        g = df[df[file_col] == file_number]\n",
    "        g_sorted = g.sort_index()\n",
    "        texts = g_sorted[text_col].astype(str).tolist()\n",
    "\n",
    "        # Compute metrics at the utterance level\n",
    "        per_text = meter.compute_dataset(texts)\n",
    "\n",
    "        out_row = {\n",
    "            \"file_number\": int(file_number),\n",
    "            \"mean_perplexity\": float(np.nanmean(per_text[\"perplexity\"])) if len(per_text) else np.nan,\n",
    "            \"mean_tangeniality\": float(np.nanmean(per_text[\"tangeniality\"])) if len(per_text) else np.nan,\n",
    "            \"mean_coherence\": float(np.nanmean(per_text[\"coherence\"])) if len(per_text) else np.nan,\n",
    "        }\n",
    "        results.append(out_row)\n",
    "        iters_done += 1\n",
    "\n",
    "        # Every N iterations — checkpoint + memory cleanup\n",
    "        if checkpoint_every and (iters_done % checkpoint_every == 0):\n",
    "            _save_checkpoint()\n",
    "            _flush_mem(per_text, texts, g_sorted, g)\n",
    "\n",
    "    # Final DataFrame\n",
    "    out_df = (\n",
    "        pd.DataFrame(results)\n",
    "        .sort_values(\"file_number\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Final checkpoint + memory cleanup, to clear everything after heavy work\n",
    "    _save_checkpoint()\n",
    "    _flush_mem()\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede565",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_level_df = build_file_level_metrics(df_woz, dm, file_col=\"File_number\", text_col=\"Text\")\n",
    "file_level_df.to_csv(\"dcwoz_eng_new_gemma.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8462935",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_level_df = build_file_level_metrics(df_woz, dm, file_col=\"File_number\", text_col=\"Text_ukr\")\n",
    "file_level_df.to_csv(\"dcwoz_ukr_new_gemma.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0e48a",
   "metadata": {},
   "source": [
    "# run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9ab9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>N</th>\n",
       "      <th>Pearson_r</th>\n",
       "      <th>Spearman_rho</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ICC2_1</th>\n",
       "      <th>mean_diff_% (UK_vs_EN)</th>\n",
       "      <th>t_p</th>\n",
       "      <th>wilcoxon_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coherence</td>\n",
       "      <td>275</td>\n",
       "      <td>0.242285</td>\n",
       "      <td>0.202955</td>\n",
       "      <td>2.097768e-02</td>\n",
       "      <td>2.118642e-02</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>-2.219679</td>\n",
       "      <td>5.625587e-236</td>\n",
       "      <td>7.488097e-47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perplexity</td>\n",
       "      <td>275</td>\n",
       "      <td>0.060095</td>\n",
       "      <td>0.262961</td>\n",
       "      <td>9.059692e+07</td>\n",
       "      <td>1.937525e+08</td>\n",
       "      <td>0.058709</td>\n",
       "      <td>398.242392</td>\n",
       "      <td>1.732284e-01</td>\n",
       "      <td>2.671626e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tangeniality</td>\n",
       "      <td>275</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>0.972289</td>\n",
       "      <td>1.273736e-02</td>\n",
       "      <td>1.548681e-02</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>-1.368960</td>\n",
       "      <td>1.118840e-34</td>\n",
       "      <td>1.584426e-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         metric    N  Pearson_r  Spearman_rho           MAE          RMSE  \\\n",
       "0     coherence  275   0.242285      0.202955  2.097768e-02  2.118642e-02   \n",
       "1    perplexity  275   0.060095      0.262961  9.059692e+07  1.937525e+08   \n",
       "2  tangeniality  275   0.973861      0.972289  1.273736e-02  1.548681e-02   \n",
       "\n",
       "     ICC2_1  mean_diff_% (UK_vs_EN)            t_p    wilcoxon_p  \n",
       "0  0.005617               -2.219679  5.625587e-236  7.488097e-47  \n",
       "1  0.058709              398.242392   1.732284e-01  2.671626e-07  \n",
       "2  0.953386               -1.368960   1.118840e-34  1.584426e-28  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the two CSVs, inspect, and run EN-vs-UK evaluation at the session level.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, ttest_rel, wilcoxon\n",
    "\n",
    "ENG_PATH = \"dcwoz_eng_new_gemma.csv\"\n",
    "UKR_PATH = \"dcwoz_ukr_new_gemma.csv\"\n",
    "\n",
    "df_en = pd.read_csv(ENG_PATH)\n",
    "df_uk = pd.read_csv(UKR_PATH)\n",
    "\n",
    "# --- Helpers ---\n",
    "def _dropna_pair(a: np.ndarray, b: np.ndarray):\n",
    "    mask = (~np.isnan(a)) & (~np.isnan(b))\n",
    "    return a[mask], b[mask]\n",
    "\n",
    "def safe_pearson(a, b):\n",
    "    x, y = _dropna_pair(np.asarray(a, float), np.asarray(b, float))\n",
    "    if len(x) < 3:\n",
    "        return np.nan, len(x)\n",
    "    return float(pearsonr(x, y)[0]), len(x)\n",
    "\n",
    "def safe_spearman(a, b):\n",
    "    x, y = _dropna_pair(np.asarray(a, float), np.asarray(b, float))\n",
    "    if len(x) < 3:\n",
    "        return np.nan, len(x)\n",
    "    return float(spearmanr(x, y)[0]), len(x)\n",
    "\n",
    "def mae_rmse(a, b):\n",
    "    x, y = _dropna_pair(np.asarray(a, float), np.asarray(b, float))\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan, 0\n",
    "    mae = float(np.mean(np.abs(x - y)))\n",
    "    rmse = float(np.sqrt(np.mean((x - y) ** 2)))\n",
    "    return mae, rmse, len(x)\n",
    "\n",
    "def rel_mean_diff_percent(a, b):\n",
    "    a = np.asarray(a, float)\n",
    "    b = np.asarray(b, float)\n",
    "    mask = np.abs(a) > 1e-9\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean((b[mask] - a[mask]) / np.abs(a[mask]) * 100.0))\n",
    "\n",
    "def paired_tests(a, b):\n",
    "    x, y = _dropna_pair(np.asarray(a, float), np.asarray(b, float))\n",
    "    if len(x) < 3:\n",
    "        return {\"t_p\": np.nan, \"wilcoxon_p\": np.nan}\n",
    "    try:\n",
    "        t_p = float(ttest_rel(x, y, nan_policy=\"omit\").pvalue)\n",
    "    except Exception:\n",
    "        t_p = np.nan\n",
    "    try:\n",
    "        d = x - y\n",
    "        if np.allclose(d, 0):\n",
    "            w_p = 1.0\n",
    "        else:\n",
    "            w_p = float(wilcoxon(x, y, zero_method=\"wilcox\", alternative=\"two-sided\").pvalue)\n",
    "    except Exception:\n",
    "        w_p = np.nan\n",
    "    return {\"t_p\": t_p, \"wilcoxon_p\": w_p}\n",
    "\n",
    "# ICC(2,1): two-way random, absolute agreement (McGraw & Wong 1996)\n",
    "def icc2_1(wide: pd.DataFrame) -> float:\n",
    "    X = wide.dropna().to_numpy(float)\n",
    "    if X.shape[0] < 3 or X.shape[1] != 2:\n",
    "        return np.nan\n",
    "    n, k = X.shape\n",
    "    mean_target = X.mean(axis=1, keepdims=True)\n",
    "    mean_rater = X.mean(axis=0, keepdims=True)\n",
    "    grand_mean = X.mean()\n",
    "\n",
    "    ss_total = ((X - grand_mean) ** 2).sum()\n",
    "    ss_between_targets = (k * ((mean_target - grand_mean) ** 2)).sum()\n",
    "    ss_between_raters = (n * ((mean_rater - grand_mean) ** 2)).sum()\n",
    "    ss_error = ss_total - ss_between_targets - ss_between_raters\n",
    "\n",
    "    ms_between_targets = ss_between_targets / (n - 1)\n",
    "    ms_between_raters = ss_between_raters / (k - 1)\n",
    "    ms_error = ss_error / ((n - 1) * (k - 1))\n",
    "\n",
    "    icc = (ms_between_targets - ms_error) / (\n",
    "        ms_between_targets + (k - 1) * ms_error + (k * (ms_between_raters - ms_error) / n)\n",
    "    )\n",
    "    return float(icc)\n",
    "\n",
    "# --- Harmonize and merge ---\n",
    "# Try common columns: expect ['file_number','mean_perplexity','mean_tangeniality','mean_coherence']\n",
    "# Normalize column names to lower\n",
    "df_en.columns = [c.lower() for c in df_en.columns]\n",
    "df_uk.columns = [c.lower() for c in df_uk.columns]\n",
    "\n",
    "# Robust rename if needed\n",
    "rename_map = {\n",
    "    \"file_number\": \"file_number\",\n",
    "    \"filenumber\": \"file_number\",\n",
    "    \"file\": \"file_number\",\n",
    "    \"mean_perplexity\": \"mean_perplexity\",\n",
    "    \"mean_tangeniality\": \"mean_tangeniality\",\n",
    "    \"mean_tangentiality\": \"mean_tangeniality\",\n",
    "    \"mean_coherence\": \"mean_coherence\",\n",
    "}\n",
    "df_en = df_en.rename(columns=rename_map)\n",
    "df_uk = df_uk.rename(columns=rename_map)\n",
    "\n",
    "# Keep only expected cols\n",
    "keep = [\"file_number\", \"mean_perplexity\", \"mean_tangeniality\", \"mean_coherence\"]\n",
    "missing_en = [c for c in keep if c not in df_en.columns]\n",
    "missing_uk = [c for c in keep if c not in df_uk.columns]\n",
    "\n",
    "summary = {\n",
    "    \"missing_columns_en\": missing_en,\n",
    "    \"missing_columns_uk\": missing_uk,\n",
    "    \"n_rows_en\": len(df_en),\n",
    "    \"n_rows_uk\": len(df_uk),\n",
    "}\n",
    "\n",
    "# Merge\n",
    "merged = df_en[keep].merge(df_uk[keep], on=\"file_number\", how=\"inner\", suffixes=(\"_en\", \"_uk\"))\n",
    "# display_dataframe_to_user(\"Merged EN-UK session means (head)\", merged.head(30))\n",
    "\n",
    "# --- Evaluate for each metric on session means ---\n",
    "rows = []\n",
    "for metric in [\"perplexity\", \"tangeniality\", \"coherence\"]:\n",
    "    a = pd.to_numeric(merged[f\"mean_{metric}_en\"], errors=\"coerce\").to_numpy(float)\n",
    "    b = pd.to_numeric(merged[f\"mean_{metric}_uk\"], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "    r, n_r = safe_pearson(a, b)\n",
    "    rho, n_rho = safe_spearman(a, b)\n",
    "    mae, rmse, n_e = mae_rmse(a, b)\n",
    "\n",
    "    # ICC on a wide dataframe with two columns\n",
    "    wide = pd.DataFrame({\"EN\": a, \"UK\": b})\n",
    "    icc = icc2_1(wide)\n",
    "\n",
    "    bias_pct = rel_mean_diff_percent(a, b)\n",
    "    tests = paired_tests(a, b)\n",
    "\n",
    "    rows.append({\n",
    "        \"metric\": metric,\n",
    "        \"N\": int(min(n_r, n_rho, n_e)),\n",
    "        \"Pearson_r\": r,\n",
    "        \"Spearman_rho\": rho,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"ICC2_1\": icc,\n",
    "        \"mean_diff_% (UK_vs_EN)\": bias_pct,\n",
    "        \"t_p\": tests[\"t_p\"],\n",
    "        \"wilcoxon_p\": tests[\"wilcoxon_p\"],\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(rows).sort_values(\"metric\").reset_index(drop=True)\n",
    "eval_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab3345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
